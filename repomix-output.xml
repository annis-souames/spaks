This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: infra
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
kube-scheduler-simulator/
  .github/
    PULL_REQUEST_TEMPLATE.md
  keps/
    140-scenario-based-simulation/
      kep.yaml
      README.md
    159-scheduler-simulator-operator/
      kep.yaml
      README.md
    184-scheduler-simulation/
      kep.yaml
      README.md
    README.md
  scenario/
    .devcontainer/
      devcontainer.json
      post-install.sh
    api/
      v1alpha1/
        groupversion_info.go
        scenario_types.go
        zz_generated.deepcopy.go
    cmd/
      main.go
    config/
      crd/
        bases/
          simulation.kube-scheduler-simulator.x-k8s.io_scenarios.yaml
        kustomization.yaml
        kustomizeconfig.yaml
      default/
        cert_metrics_manager_patch.yaml
        kustomization.yaml
        manager_metrics_patch.yaml
        metrics_service.yaml
      manager/
        kustomization.yaml
        manager.yaml
      network-policy/
        allow-metrics-traffic.yaml
        kustomization.yaml
      prometheus/
        kustomization.yaml
        monitor_tls_patch.yaml
        monitor.yaml
      rbac/
        kustomization.yaml
        leader_election_role_binding.yaml
        leader_election_role.yaml
        metrics_auth_role_binding.yaml
        metrics_auth_role.yaml
        metrics_reader_role.yaml
        role_binding.yaml
        role.yaml
        scenario_admin_role.yaml
        scenario_editor_role.yaml
        scenario_viewer_role.yaml
        service_account.yaml
      samples/
        kustomization.yaml
        simulation_v1alpha1_scenario.yaml
    hack/
      boilerplate.go.txt
    internal/
      controller/
        scenario_controller_test.go
        scenario_controller.go
        suite_test.go
    test/
      e2e/
        e2e_suite_test.go
        e2e_test.go
      utils/
        utils.go
    .dockerignore
    .gitignore
    .golangci.yml
    Dockerfile
    Makefile
    PROJECT
    README.md
  simulator/
    cmd/
      scheduler/
        Dockerfile
        kubeconfig.yaml
        scheduler.go
        scheduler.yaml
      simulator/
        Dockerfile
        simulator.go
    config/
      v1alpha1/
        defaults.go
        doc.go
        register.go
        types.go
        zz_generated.conversion.go
        zz_generated.deepcopy.go
        zz_generated.defaults.go
      config_test.go
      config.go
    docs/
      api-samples/
        v1/
          export.md
          import.md
          README.md
      sample/
        debuggable-scheduler/
          kubeconfig.yaml
          main.go
          scheduler.yaml
        drl/
          plugin.go
        nodenumber/
          plugin.go
        plugin-extender/
          docker-compose.yaml
          extender.go
      api.md
      debuggable-scheduler.md
      environment-variables.md
      extender.md
      external-scheduler.md
      how-it-works.md
      import-cluster-resources.md
      integrate-your-scheduler.md
      kubeconfig.yaml
      plugin-extender.md
      running-simulator.md
      simulator-server-config.md
    errors/
      errors.go
    hack/
      boilerplate/
        boilerplate.go.txt
      etcd.sh
      start_simulator.sh
      tools.go
      update-generated-conversions.sh
      update-generated-deep-copies.sh
      update-generated-defaulters.sh
    oneshotimporter/
      importer_test.go
      importer.go
    pkg/
      debuggablescheduler/
        command.go
        debuggable_scheduler.go
        server.go
      externalscheduler/
        command.go
        external_scheduler.go
      README.md
    reset/
      reset.go
    resourceapplier/
      resource.go
      resourceapplier_test.go
      resourceapplier.go
    resourcewatcher/
      mock_resourcewatcher/
        lister.go
        streamWriter.go
        watchInterface.go
      streamwriter/
        mock_streamwriter/
          responseStream.go
        streamwriter_test.go
        streamwriter.go
      eventproxy_test.go
      eventproxy.go
      mock_eventproxy_test.go
      resourcewatcher_test.go
      resourcewatcher.go
    scheduler/
      config/
        config.go
        plugin_test.go
        plugin.go
        wasm_test.go
        wasm.go
      extender/
        annotation/
          annotation.go
        mock_extender/
          extender.go
          resultstore.go
        resultstore/
          resultstore_test.go
          resultstore.go
        extender_test.go
        extender.go
        service_test.go
        service.go
      plugin/
        annotation/
          annotation.go
        mock/
          framework.go
          wrappedplugin.go
        resultstore/
          store_test.go
          store.go
        plugins_test.go
        plugins.go
        wrappedplugin_test.go
        wrappedplugin.go
      storereflector/
        mock_storereflector/
          resultstore.go
        annotation.go
        storereflector_test.go
        storereflector.go
      scheduler_test.go
      scheduler.go
    server/
      di/
        di.go
        interface.go
      handler/
        extender.go
        reset.go
        schedulerconfig.go
        snapshot.go
        watcher.go
      server.go
    snapshot/
      mock_snapshot/
        scheduler.go
      snapshot_test.go
      snapshot.go
      utils_test.go
      utils.go
    syncer/
      syncer_test.go
      syncer.go
    util/
      decoder_test.go
      decoder.go
      retry.go
      semaphored_errgroup.go
    .dockerignore
    .golangci.yml
    config.yaml
    kubeconfig.yaml
    Makefile
  tools/
    tools.go
  web/
    api/
      v1/
        export.ts
        namespace.ts
        node.ts
        pod.ts
        priorityclass.ts
        pv.ts
        pvc.ts
        reset.ts
        schedulerconfiguration.ts
        storageclass.ts
        types.ts
        watcher.ts
      APIProvider.vue
      APIProviderKeys.ts
    assets/
      README.md
      variables.scss
    components/
      ResourceBar/
        BarHeader.vue
        DefinitionTree.vue
        DeleteButton.vue
        ResourceBar.vue
        SchedulingResults.vue
        YamlEditor.vue
      ResourceViews/
        DataTables/
          DataTable.vue
          NamespaceDataTable.vue
          NodeDataTable.vue
          PodDataTable.vue
          PriorityClassDataTable.vue
          PVCDataTable.vue
          PVDataTable.vue
          ResourcesDataTable.vue
          StorageClassDataTable.vue
        Lists/
          NamespaceList.vue
          NodeList.vue
          PodList.vue
          PriorityClassList.vue
          PVCList.vue
          PVList.vue
          ResourcesList.vue
          StorageClassList.vue
          UnscheduledPodList.vue
        ResourcesViewPanel.vue
      StoreKey/
        NamespaceStoreKey.ts
        NodeStoreKey.ts
        PodStoreKey.ts
        PriorityClassStoreKey.ts
        PVCStoreKey.ts
        PVStoreKey.ts
        SchedulerConfigurationStoreKey.ts
        SnackBarStoreKey.ts
        StorageClassStoreKey.ts
      StoreProvider/
        NamespaceStoreProvider.vue
        NodeStoreProvider.vue
        PodStoreProvider.vue
        PriorityClassStoreProvider.vue
        PVCStoreProvider.vue
        PVStoreProvider.vue
        SchedulerConfigurationStoreProvider.vue
        SnackbarStoreProvider.vue
        StorageClassStoreProvider.vue
      TopBar/
        ExportButton.vue
        ImportButton.vue
        ResetButton.vue
        SchedulerConfigurationEditButton.vue
        TopBar.vue
      README.md
      ResourceAddButton.vue
      ResourceWatcher.vue
      Snackbar.vue
    docs/
      environment-variables.md
    layouts/
      default.vue
      error.vue
      README.md
    middleware/
      README.md
    pages/
      index.vue
      README.md
    plugins/
      ApiRuntimeConfigPlugin.ts
    static/
      kubernetes-logo_with_border.svg
      kubernetes-logo.svg
      node.svg
      pod.svg
      pv.svg
      pvc.svg
      README.md
      sc.svg
      sched.svg
    store/
      helpers/
        storeHelper.ts
      namespace.ts
      node.ts
      pod.ts
      priorityclass.ts
      pv.ts
      pvc.ts
      schedulerconfiguration.ts
      snackbar.ts
      storageclass.ts
    types/
      api/
        v1.ts
      resources.ts
    .dockerignore
    .eslintrc.js
    .gitignore
    .prettierignore
    .stylelintignore
    .stylelintrc.json
    Dockerfile
    jest.config.js
    nuxt.config.js
    package.json
    README.md
    tsconfig.eslint.json
    tsconfig.json
  .gitignore
  cloudbuild.yaml
  code-of-conduct.md
  compose.yml
  CONTRIBUTING.md
  kwok.yaml
  LICENSE
  Makefile
  OWNERS
  README.md
  RELEASE.md
  SECURITY_CONTACTS
  SECURITY.md
.gitignore
pod-tmp.yml
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="kube-scheduler-simulator/.github/PULL_REQUEST_TEMPLATE.md">
<!--  Thanks for sending a pull request!  Here are some tips for you:

1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide/first-contribution.md#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide
2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR/issue labels, read here:
https://git.k8s.io/community/contributors/devel/sig-release/release.md#issuepr-kind-label
3. Ensure you have added or ran the appropriate tests for your PR: https://git.k8s.io/community/contributors/devel/sig-testing/testing.md
4. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews
5. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests
-->

#### What type of PR is this?

<!--
Add related area tag(s):
/area web
/area simulator
/area scenario

Add one of the following kinds:
/kind bug
/kind cleanup
/kind documentation
/kind feature

Optionally add one or more of the following kinds if applicable:
/kind api-change
/kind deprecation
/kind failing-test
/kind flake
/kind regression
-->

#### What this PR does / why we need it:

#### Which issue(s) this PR fixes:
<!--
*Automatically closes linked issue when PR is merged.
Usage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.
_If PR is about `failing-tests or flakes`, please post the related issues/tests in a comment and do not use `Fixes`_*
-->
Fixes #

#### Special notes for your reviewer:


/label tide/merge-method-squash
</file>

<file path="kube-scheduler-simulator/keps/140-scenario-based-simulation/kep.yaml">
title: Scenario-based simulation
kep-number: 140
authors:
  - "@sanposhiho"
  - "@everpeace"
owning-sig: sig-scheduling
status: provisional
creation-date: 2022-06-06
reviewers:
  - "@196Ikuchil"
approvers:
  - "@196Ikuchil"

stage: alpha

milestone:
  alpha: "v0.1"
</file>

<file path="kube-scheduler-simulator/keps/140-scenario-based-simulation/README.md">
# KEP-140: Scenario-based simulation

## Summary

A new scenario-based simulation feature is introduced to kube-scheduler-simulator by the new `Scenario` CRD.

## Motivation

Nowadays, the scheduler is extendable in multiple ways:
- configure with [KubeSchedulerConfiguration](https://kubernetes.io/docs/reference/scheduling/config/)
- add Plugins of [Scheduling Framework](https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/)
- add [Extenders](https://github.com/kubernetes/enhancements/tree/5320deb4834c05ad9fb491dcd361f952727ece3e/keps/sig-scheduling/1819-scheduler-extender)
- etc...

But unfortunately, not all expansions yield good results.
Those who customize the scheduler need to ensure it is working as expected and doesn't have an unacceptably negative impact on the scheduling result or performance. And usually, evaluating the scheduler is not easy because there are many factors for evaluating the scheduler's ability.

The scenario-based simulation feature will be helpful for those who customize the scheduler to evaluate their scheduler.

## Goals

Users can simulate their controller, including the scheduler, with defined scenarios and evaluate their controller's behavior.
It's mainly designed for the scheduler, but you can use it for other controllers like a cluster autoscaler.

## Non-Goals

See the result of scenario-based simulation from Web UI. (maybe implemented in the future, but out of the scope of this proposal.)

## User Stories 

### Story 1

The company has added many features into the scheduler via some custom plugins,
and they want to make sure that their expansions are working as expected and have not negatively impacted the scheduling results.

#### Solution

They can define appropriate scenarios and analyze the .status.result with the result calculation package.

### Story 2

The users want to see how their customized scheduler behaves in the worst case scenario.

#### Solution

Even when a scenario is running, users can add operations to that scenario. 
So, in this case, they can add operations that are worst case for the scheduler by looking at the simulation results and the resources status at that point.

## Proposal

### Implementation design details

#### The current simulator and proposal

We initially designed the simulator with a strong emphasis on Web UI. 
Then, thanks to so many contributions from everyone, we've expanded the simulator to be able to be used by other clients like kubectl, client-go, etc.

Now that simulators are no longer just for WebUI, we need to think about how we can design scenario-based simulations to be easy to use by other clients as well.

Therefore, this kep proposes to define the scenario **as CRD**. All clients, including web UI, can use the scenario-based simulation feature by creating the Scenario resource.

#### Scenario CRD

The Scenario is a non-namespaced resource. 
This CRD will be applied to kube-apiserver started in kube-scheduler-simulator.

The following diagram depicts the high-level relationship among entities in `Scenario` CRD.

![Scenario CRD diagram](images/crd-diagram.png)

We may need to change etcd request-size limitation by --max-request-bytes since the scenario resource may be more significant than other standard resources.
https://etcd.io/docs/v3.4/dev-guide/limit/#request-size-limit

```go
// ScenarioSpec defines the desired state of Scenario
type ScenarioSpec struct {
	// Operations field has all operations for a scenario.
	// Also, you can add a new operation while the scenario runs.
	//
	// +patchMergeKey=ID
	// +patchStrategy=merge
	Operations []*ScenarioOperation `json:"operations"`

	// Controllers have the configuration for controllers working with simulation.
	Controllers *Controllers `json:"controllers"`
}

// See [# SimulationControllers and PreSimulationControllers](#simulationcontrollers-and-presimulationcontrollers).
type Controllers struct {
	// PreSimulationControllers is a list of controllers that should be run before SimulationControllers.
	// They will run in parallel.
	// 
	// It's an optional field. 
	// All controllers registered in the simulator will be enabled automatically. (except controllers set in Simulate.)
	// So, you need to configure it only when you want to disable some controllers enabled by default.
	//
	// +optional
	PreSimulationControllers	*ControllerSet `json:"presimulationControllers`
	// SimulationControllers is a list of controllers that are the target of this simulation.
	// These are run one by one in the same order specified in Enabled field.
	// 
	// It's a required field; no controllers will be enabled automatically.
	SimulationControllers  *ControllerSet `json:"simulationControllers"`
}

type ControllerSet struct {
	// Enabled specifies controllers that should be enabled.
	// +listType=atomic
	Enabled  []Controller `json:"enabled"`
	// Disabled specifies controllers that should be disabled.
	// When all controllers need to be disabled, an array containing only one "*" should be provided.
	// +listType=map
	// +listMapKey=name
	Disabled []Controller `json:"disabled"`
}

type Controller struct {
	Name string
}

type ScenarioOperation struct {
	// ID for this operation. Normally, the system sets this field for you.
	ID string `json:"id"`
	// MajorStep indicates when the operation should be done.
	MajorStep int32 `json:"step"`

	// One of the following four fields must be specified.
	// If more than one is set or all are empty, the operation is invalid, and the scenario will fail.

	// Create is the operation to create a new resource.
	//
	// +optional
	Create *CreateOperation `json:"createOperation,omitempty"`
	// Patch is the operation to patch a resource.
	//
	// +optional
	Patch *PatchOperation `json:"patchOperation,omitempty"`
	// Delete indicates the operation to delete a resource.
	//
	// +optional
	Delete *DeleteOperation `json:"deleteOperation,omitempty"`
	// Done indicates the operation to mark the scenario as Succeeded.
	// When finish the step DoneOperation belongs, this Scenario changes its status to Succeeded.
	//
	// +optional
	Done *DoneOperation `json:"doneOperation,omitempty"`
}

type CreateOperation struct {
	// Object is the Object to be created.
	Object *unstructured.Unstructured `json:"object"`

	// +optional
	CreateOptions metav1.CreateOptions `json:"createOptions,omitempty"`
}

type PatchOperation struct {
	TypeMeta   metav1.TypeMeta   `json:"typeMeta"`
	ObjectMeta metav1.ObjectMeta `json:"objectMeta"`
	// Patch is the patch for target.
	Patch string `json:"patch"`
	// PatchType
	PatchType types.PatchType

	// +optional
	PatchOptions metav1.PatchOptions `json:"patchOptions,omitempty"`
}

type DeleteOperation struct {
	TypeMeta   metav1.TypeMeta   `json:"typeMeta"`
	ObjectMeta metav1.ObjectMeta `json:"objectMeta"`

	// +optional
	DeleteOptions metav1.DeleteOptions `json:"deleteOptions,omitempty"`
}

type DoneOperation struct{}

// See [# The concept "ScenarioStep"](#the-concept-scenariostep).
// ScenarioStep is the time represented by a set of numbers, MajorStep and MinorStep,
// which are like hours and minutes in clocks in the real world.
// ScenarioStep.Major is moved to the next ScenarioStep.Major when the SimulationController can no longer do anything with the current cluster state.
// Scenario.Minor is moved to the next Scenario.Minor when any resources operations(create/edit/delete) happens.
type ScenarioStep struct {
	Major int32 `json:"major"`
	Minor int32 `json:"minor"`
}

// ScenarioStatus defines the observed state of Scenario
type ScenarioStatus struct {
	// The phase is a simple, high-level summary of where the Scenario is in its lifecycle.
	//
	// +optional
	Phase ScenarioPhase `json:"phase,omitempty"`
	// A human-readable message indicating details about why the scenario is in this phase.
	//
	// +optional
	Message *string `json:"message,omitempty"`
	// StepStatus has the status related to step.
	//
	StepStatus ScenarioStepStatus `json:"stepStatus"`
	// ScenarioResult has the result of the simulation.
	// Just before Step advances, this result is updated based on all occurrences at that step.
	//
	// +optional
	ScenarioResult ScenarioResult `json:"scenarioResult,omitempty"`
}

type ScenarioStepStatus struct {
	// Step indicates the current ScenarioStep.
	//
	// +optional
	Step ScenarioStep `json:"step,omitempty"`
	// Phase indicates the current phase in a single step.
	// 
	// +optional
	Phase StepPhase `json:"phase,omitempty"`
	// RunningSimulationController indicates one of the SimulationControllers that is currently running/paused/completed.
	RunningSimulationController string `json:"runningSimulationController"`
}

type StepPhase string

const (
	// Operating means controller is currently operating operation defined for the step.
	Operating StepPhase = "Operating"
	// OperatingCompleted means the PreSimulationControllers have finished operating operation defined for the step.
	OperatingCompleted StepPhase = "OperatingCompleted"
	// ControllerRunning means the SimulationController is working.
	ControllerRunning StepPhase = "ControllerRunning"
	// ControllerPaused means the SimulationController is paused(or will be paused).
	ControllerPaused	StepPhase = "ControllerPaused"
	// ControllerCompleted means the current running SimulationController no longer do anything with the current cluster state.
	ControllerCompleted StepPhase = "ControllerCompleted"
	// StepCompleted means the controller is preparing to move to the next step.
	StepCompleted StepPhase = "Finished"
)

type ScenarioPhase string

const (
	// ScenarioPending phase indicates the scenario isn't started yet.
	// e.g., waiting for another scenario to finish running.
	ScenarioPending ScenarioPhase = "Pending"
	// ScenarioRunning phase indicates the scenario is running.
	ScenarioRunning ScenarioPhase = "Running"
	// ScenarioPaused phase indicates all ScenarioSpec.Operations
	// has been finished but not marked as done by ScenarioDone ScenarioOperations.
	ScenarioPaused ScenarioPhase = "Paused"
	// ScenarioSucceeded phase describes Scenario is fully completed
	// by ScenarioDone ScenarioOperations. User
	// can’t add any ScenarioOperations once
	// Scenario reached this phase.
	ScenarioSucceeded ScenarioPhase = "Succeeded"
	// ScenarioFailed phase indicates something wrong happened while running the scenario.
	// For example:
	// - the controller cannot create a resource for some reason.
	// - users change the scheduler configuration via simulator API.
	ScenarioFailed  ScenarioPhase = "Failed"
	ScenarioUnknown ScenarioPhase = "Unknown"
)

type ScenarioResult struct {
	// SimulatorVersion represents the version of the simulator that runs this scenario.
	SimulatorVersion string `json:"simulatorVersion"`
	// Timeline is a map of operations keyed with MajorStep(string).
	// This may have many of the same operations as .spec.operations but has additional PodScheduled and Delete operations for Pods
	// to represent a Pod is scheduled or preempted by the scheduler.
	//
	// +patchMergeKey=ID
	// +patchStrategy=merge
	Timeline map[string][]ScenarioTimelineEvent `json:"timeline"`
}

type ScenarioTimelineEvent struct {
	// The ID will be the same as spec.ScenarioOperations.ID if it is from the defined operation.
	// Otherwise, it'll be newly generated.
	ID string
	// Step indicates the ScenarioStep at which the operation has been done.
	Step	ScenarioStep `json:"step"`

	// Only one of the following fields must be non-empty.

	// Create is the result of ScenarioSpec.Operations.CreateOperation.
	Create *CreateOperationResult `json:"create"`
	// Patch is the result of ScenarioSpec.Operations.PatchOperation.
	Patch *PatchOperationResult `json:"patch"`
	// Delete is the result of ScenarioSpec.Operations.DeleteOperation.
	Delete *DeleteOperationResult `json:"delete"`
	// Done is the result of ScenarioSpec.Operations.DoneOperation.
	Done *DoneOperationResult `json:"done"`
}

type CreateOperationResult struct {
	// Operation is the operation that was done.
	Operation CreateOperation `json:"operation"`
	// Result is the resource after the creation.
	Result unstructured.Unstructured `json:"result"`
}

type PatchOperationResult struct {
	// Operation is the operation that was done.
	Operation PatchOperation `json:"operation"`
	// Result is the resource after the patch.
	Result unstructured.Unstructured `json:"result"`
}

type DeleteOperationResult struct {
	// Operation is the operation that was done.
	Operation DeleteOperation `json:"operation"`
}

type DoneOperationResult struct {
	// Operation is the operation that was done.
	Operation DoneOperation `json:"operation"`
}

// Scenario is the Schema for the scenario API
type Scenario struct {
	metav1.TypeMeta   `json:",inline"`
	metav1.ObjectMeta `json:"metadata,omitempty"`

	Spec   ScenarioSpec   `json:"spec,omitempty"`
	Status ScenarioStatus `json:"status,omitempty"`
}
```

#### The detailed goal of the scenario

- Users can define Scenario and see how the SimulationControllers work with a scenario.
- Users can analyze and evaluate the controller with .status.Result.Timeline.
	- All operations that happened while running the scenario will be recorded in Timeline and users can analyze Timeline by using our functions.
- The result from the same Scenario won't be much changed run by run.
	- Hopefully, if the Scenario and controllers are the same, the result should be similar. 

#### The required configuration for users

- add the comment directive to all SimulationController's codes and modify the code by our code generator.
  - The simulator has the scheduler internally by default, and this scheduler has already been set up.
  - See [# How to stop the SimulationControllers loop](#how-to-stop-the-simulationcontrollers-loop).
- define the functions for controllers to expect when controllers start to work and when controllers finish working.
  - The simulator has some controllers internally by default, and they have already been set up.
  - See [#How the simulator knows when a cluster state gets converged by the controller?](#how-the-simulator-knows-when-a-cluster-state-gets-converged-by-the-controller).

#### Kubernetes controller

Let's talk about the controllers in Kubernetes first.

> In Kubernetes, controllers are control loops that watch the state of your cluster, then make or request changes where needed. Each controller tries to move the current cluster state closer to the desired state.
https://kubernetes.io/docs/concepts/architecture/controller/

The Kubernetes controller has the desired state.
For example, the ReplicaSet controller. It's working on managing the number of Pods to satisfy all replicaset's `.spec.replicas`.

Some controllers in this world refer to something other than k8s resources like external metrics and real time to determine their ideal state. 
But, they cannot be used in our Scenario. In other words, **controllers that refer only to resources on k8s can be used in Scenario.**
This is because the scenario controller cannot manage something other than k8s resources, and unexpected operations based on them may happen during the Scenario, which we want to avoid.

The following controllers cannot be used in Scenario, for example:
- controllers that use external metrics.
  - e.g., the cluster autoscaler that scales Nodes based on external metrics.
- controllers that do something based on real time.
  - e.g., the cronjob controller that creates Job based on real time.

All controllers that refer only to k8s resources won't do anything if there is no change on k8s resources state.
In other words, all controllers may do something only when someone performs a k8s resource operation.

#### How the simulator knows when a cluster state gets converged by the controller?

In the simulator, users define the `ControllerWaiter` for each controller so that we can expect when a controller finishes working.

```go
// ControllerWaiter is used to know when a cluster state gets converged by the controller.
type ControllerWaiter interface {
	// Name returns the controller's name.
    Name() string
	// WaitConditionFunc returns wait.ConditionFunc that detects when the controller cannot do anything in this cluster state.
    WaitConditionFunc(ctx context.Context) (wait.ConditionFunc, error)
}
```

For example, the ReplicaSet controller. `WaitConditionFunc` returns the `wait.ConditionFunc` that detects 
all replicaset satisfy .spec.replicas == .status.fullyLabeledReplicas (the ideal state for the ReplicaSet controller) or .status.conditions has ReplicaFailure true (cannot do anymore to move the cluster state closer to the ideal state).

Talk about why we need it in the later section.

#### SimulationControllers and PreSimulationControllers

We have two types of controllers; SimulationControllers and PreSimulationControllers.

SimulationControllers are the target of this simulation,
and PreSimulationControllers are the other controllers that are needed to simulate.

Let's say you want to simulate the scheduler and you are going to use ReplicaSet in the simulation. 
In that case, you will set scheduler as SimulationController and ReplicaSet controller as PreSimulationControllers.

#### The concept "ScenarioStep"

The Scenario has the concept "ScenarioStep" to represent the _simulated_ time used during scenario running.
It is not directly related to the real-time that you see in your wall-clock, but is a variable maintained by the scenario controller.

ScenarioStep is represented by a set of numbers, "MajorStep" and "MinorStep", which are like hours and minutes in clocks in the real world.
(Precisely, MajorStep and MinorStep don't have the range limitations that hours and minutes have)

So, ScenarioStep.Major and ScenarioStep.Minor are similar. The difference is who performs the operations. 
- ScenarioStep.Major is incremented just before the scenario controller will perform the next resource operations defined in .spec.Operations.
	- The scenario controller increments it when all SimulationController can no longer do anything with the current cluster state.
- ScenarioStep.Minor is incremented just before the SimulationController will perform resource operations.

#### What happens in a single MajorStep.

The following diagram shows all what happens at a single ScenarioStep:

![diagram](images/step.png)

And if a Scenario has two SimulationControllers, the SimulationController will be run one by one in the same order specified in .spec.controllers.SimulationControllers.enabled field.

![diagram2](images/step-two-simulated-controllers.png)

Let's go into the details.

##### 1. .spec.operations for the ScenarioStep is operated

ScenarioStep: {X, 0}
StepPhase: Operating

First, the scenario controller operate all .spec.operations for the MajorStep X.

##### 2. wait PreSimulationControllers to handle resource changes

ScenarioStep: {X, 0}
StepPhase: Operating

In [# Kubernetes controller](#Kubernetes-controller) section, we describe it:
> All controllers that refer only to k8s resources won't do anything if there is no change on k8s resources state.
In other words, all controllers may do something only when someone performs a k8s resource operation.

At (1), .spec.operations is performed and PreSimulationControllers may perform some operations to move the current cluster state closer to the desired state.

And **the SimulationControllers need to stop working while the PreSimulationControllers work.** (Actually, the SimulationController has been stopped since (1))
In .status.Result.Timeline, PreSimulationControllers actions are treated as if they were instantaneous.

So, why..? This is because we need to ensure that the simulation results do not vary significantly from run to run. 

For example:
The user has the CRD named "NodeSet". It's like ReplicaSet, but literally, it creates a defined number of Nodes.
Let's say the user wants to simulate the scheduler and defines the operation in the Scenario that creates NodeSet to create 1000 Nodes. Since it is strictly impossible to create 1000 Nodes simultaneously, the scheduler will schedule pending Pods to the Nodes created first. 
And depending on how fast the NodeSet controller creates the 1000 Node, the simulation results will change.
The scheduler needs to be stopped scheduling until the NodeSet controller creates 1000 Nodes so that the speed of the NodeSet controller doesn't affect the simulation result.

##### 3. PreSimulationControllers finish to work 

ScenarioStep: {X, 0}
StepPhase: OperatingCompleted

See [#How the simulator knows when a cluster state gets converged by the controller?](#how-the-simulator-knows-when-a-cluster-state-gets-converged-by-the-controller).

We use `WaitConditionFunc` to wait for all controllers to finish their work to deal with (1).

##### 4. the SimulationController starts

ScenarioStep: {X, 0}
StepPhase: ControllerRunning

The SimulationControllers have been stopped until now.

Here, start one of the SimulationControllers. 
SimulatedControllers is run in the same order specified in .spec.Controllers.SimulationControllers.Enabled field.

##### 5. the SimulationController does operation(s) for k8s resources

ScenarioStep: {X, 0} -> {X, 1}
StepPhase: ControllerRunning -> ControllerPaused

The SimulationController may perform some operations for k8s resources.
In admission webhook, we change the StepPhase to ControllerPaused and increment MinorStep. (Then, the requests are accepted.)

##### 6. the SimulationController stops working

ScenarioStep: {X, 1}
StepPhase: ControllerPaused

In [# Kubernetes controller](#Kubernetes-controller) section, we see the controllers are working in **loops**.

> In Kubernetes, controllers are control loops that watch the state of your cluster, then make or request changes where needed. Each controller tries to move the current cluster state closer to the desired state.
https://kubernetes.io/docs/concepts/architecture/controller/

The SimulationController always checks ScenarioStep at the end of its loop. And if StepPhase is ControllerPaused, the controller stops working. 

StepPhase was changed to ControllerPaused at (5), and the SimulationController should be stopped at the end of that loop.

See also [# How to stop the SimulationControllers loop](#how-to-stop-the-simulationcontrollers-loop).

##### 7. wait PreSimulationControllers to handle resource changes

ScenarioStep: {X, 1}
StepPhase: ControllerPaused

The PreSimulationControllers may work when the SimulationControllers perform some operations on k8s resources. 

So, every time the working SimulationController performs operations, we need to wait for the PreSimulationControllers to handle resource changes like (2).

It will repeat from (3) to (7) until the SimulationController can no longer do anything with the current cluster state.

##### 8. the SimulationController can no longer do anything with the current cluster state

ScenarioStep: {X, Y} 
StepPhase: ControllerRunning -> ControllerCompleted

We use `WaitConditionFunc` of the SimulationController to detect this (like at (3)).
The SimulationController stops working again like at (6).

After ControllerCompleted, the next SimulationController is started. 
And it will repeat from (4) to (8). 

##### 9. all the SimulationController can no longer do anything with the current cluster state

ScenarioStep: {X, Z}
StepPhase: ControllerCompleted -> StepCompleted

Yey! The simulation in the single MajorStep is finished!

Increment ScenarioMajorStep and reset ScenarioMinorStep to 0.

#### How to stop the SimulationControllers loop

In [# Kubernetes controller](#Kubernetes-controller) section, we see the controllers are working in **loops**.

> In Kubernetes, controllers are control loops that watch the state of your cluster, then make or request changes where needed. Each controller tries to move the current cluster state closer to the desired state.
https://kubernetes.io/docs/concepts/architecture/controller/

It's required to add the function `CheckScenarioStepPhase()` to all SimulationControllers, that stops the SimulationControllers loop when a running Scenario wants.

The following is an example for super simplified implementation of controllers.
`CheckScenarioStepPhase()` needs to be added at the end of loop.

```go
// Run runs HogeController.
func (h *HogeController) Run() {
	for {
		runOnce()
	}
}

// runOnce checks the cluster state and move the current cluster state closer to the desired state.
func (h *HogeController) runOnce() {
	defer scenario.CheckScenarioStepPhase("hoge-controller", config) // config: *(k8s.io/client-go/rest).Config to be used to connect kube-apiserver in the simulator.
}
```

The function `scenario.CheckScenarioStepPhase()` is always executed at the end of `runOnce`, and checks the running Scenario's .status.stepStatus.phase and .status.stepStatus.runningSimulationController. 
And when .status.stepStatus.phase is ControllerPaused and .status.stepStatus.runningSimulationController is "hoge-controller", it is blocked at that point until .status.stepStatus.phase becomes OperatingCompleted.

See the diagram in [the previous section](#the-concept-scenariostep), when the hoge-controller performs some operation for k8s resources, the admission webhook will change .status.stepStatus.phase to ControllerPaused. Then `scenario.CheckScenarioStepPhase` will be executed at the end of `runOnce` and the hoge-controller will stop looping.

Note: Even if multiple k8s resource operations happen in a single `runOnce`, the controller will stop at the end of `runOnce`.

#### The result calculation packages

ScenarioResult only has the simple data that represent what happens during the scenario.

So, we will provide useful functions and data structures to analyze the result. 

Here is the example ideas:
- the function to aggregate scheduling results from Pod's annotations.
	- the simulator records the scheduling results in Pod's annotation.
- the function to aggregate changes in allocation rate of the entire cluster.
- the function to aggregate changes in resource utilization for each Node.
- the generic iterator function that users can aggregate custom values.
- (Do you have any other idea? Tell us!)

By putting only the minimum simple information in ScenarioResult and providing functions to change it into a user-friendly struct, we can support many data structures without any changes to Scenario API.

#### Adding operations to running Scenario 

It is allowed to add operations while the Scenario is running.

Note that it does not make sense to add past ScenarioStep operations.
And, it is strongly recommended to add operations to running Scenario only after Scenario has reached "Paused" ScenarioPhase because ScenarioStep always continues to move forward until it has reached "Paused" ScenarioPhase.
Otherwise, you may add the past ScenarioStep operations, which are ignored by running Scenario.

#### Configure when to update ScenarioResult

The scenario controller only updates status.scenarioResult in Scenario resource when proceeding to the next ScenarioStep.
This is because kube-apiserver will be so busy if the controller update status.scenarioResult everytime it updated,
especially when the size of Scenario is so big.

> etcd is designed to handle small key value pairs typical for metadata. Larger requests will work, but may increase the latency of other requests. By default, the maximum size of any request is 1.5 MiB. This limit is configurable through --max-request-bytes flag for etcd server.
https://etcd.io/docs/v3.4/dev-guide/limit/#request-size-limit

For example, when using Scenario for accurate benchmark testing, users may want to reduce the request to update Scenario for kube-apiserver as much as possible.

We can add a new configuration environment variable `UPDATE_SCENARIO_RESULTS_STRATEGY` and define some strategies like:
- `UPDATE_SCENARIO_RESULTS_STRATEGY=AtMovingNextStep`: It's default value. update status.scenarioResult in Scenario resource when proceeding to the next ScenarioStep.
- `UPDATE_SCENARIO_RESULTS_STRATEGY=OnPause`: update status.scenarioResult in Scenario resource when the Scenario's phase becomes `Paused`, `Succeeded` or `Failed`.
- `UPDATE_SCENARIO_RESULTS_STRATEGY=OnDone`: update status.scenarioResult in Scenario resource when the Scenario's phase becomes `Succeeded` or `Failed`.

We can discuss it after all the implementation of Scenario is done.

#### The case kube-apiserver has Scenarios when the scenario controller starts to run

When the scenario controller is started and finds the Scenario which phase is "Running", the controller changes the status "Failed" with updating the `.status.message` like "the controller restarted while the Scenario was running".

#### Prohibitions and restrictions

The scenarios run one by one, and multiple scenarios are never run simultaneously. 
That means the scenario controller will run the following Scenario after the current running Scenario becomes "Failed" or "Succeeded".

In addition, the following actions are prohibited during scenario execution. The scenario result will be unstable or invalid if these actions are performed.
- change the scheduler configuration via simulator API.
- create/delete/edit any resources manually.

And all resources created before starting a scenario are deleted at the start of the scenario,
so that they don't affect the simulation results.
</file>

<file path="kube-scheduler-simulator/keps/159-scheduler-simulator-operator/kep.yaml">
title: scheduler simulator operator
kep-number: 159
authors:
  - "@sanposhiho"
  - "@everpeace"
owning-sig: sig-scheduling
status: provisional
creation-date: 2022-06-06
reviewers:
  - "@196Ikuchil"
approvers:
  - "@196Ikuchil"

stage: alpha

milestone:
  alpha: "v0.1"
</file>

<file path="kube-scheduler-simulator/keps/159-scheduler-simulator-operator/README.md">
# KEP-159: Scheduler Simulator Operator

## Summary

A new custom resource to represent a simulator itself and a custom controller to manage it will be introduced.

## Motivation

With this, we can easily create/use the simulator as sandbox for the simulator by creating the Simulator custom resource. 

This feature also makes it easier to handle simulators from other CRD or controllers. 
They can gets the information for the simulator easily by accessint the `Simulator` resource.

### Goals

- Users can define simulators via Simulator resources.
- The controller will manage(create, edit, delete) simulators.

### Non-Goals

Create new `Simulator` from Web UI. (may be implemented in the future, but out of scope of this proposal.)

## Proposal

### Implementation design details

#### Required prior knowledge

The current simulator is built in a single binary: the binary contains kube-apiserver, kube-scheduler and the simulator backend (which provides the web UI specific feature.)

And, the simulator can be configured via environment variables.

#### Simulator CRD

The CRD `Simulator` will be applied to kube-apiserver in user's cluster. (not to simulator's kube-apiserver.) 

It will create the Pod for the simulator in the same namespace.

```go
type Simulator struct {
  metav1.TypeMeta 
  metav1.ObjectMeta // namespaced.

  Spec SimulatorSpec
  Status SimulatorStatus
}

// SimulatorSpec has the spec for the simulator.
// All fields can be updated, but a Pod will be restarted.
//
// Note that all data stored in "etcd" container will be removed if you don't specify Volume in "etcd" container.
// Or, you can use an external etcd with EtcdURL.
type SimulatorSpec struct {
  // KubeAPIServerPort indicates the kube-apiserver’s port which the simulator has internally.
  // (The each simulator has own kube-apiserver internally.)
  // This field's value will be added to a "simulator" container's env as "KUBE_API_PORT".
  KubeAPIServerPort            int 
  // SimulatorServerPort indicates the port for the simulator’s endpoint.
  // This field's value will be added to a "simulator" container's env as "PORT".
  SimulatorServerPort          int 
  // KubeSchedulerConfigurationPath indicates the path to configuration for the scheduler. 
  // The Simulator has the scheduler internally and you can configure it with this configuration.
  // This field's value will be added to a "simulator" container's env as "KUBE_SCHEDULER_CONFIG_PATH".
  // optional
  KubeSchedulerConfigurationPath   string
  // EtcdURL indicates the URL of etcd used in kube-apiserver.
  // If this field is empty, the Pod created by this Simulator resource will have the etcd container.
  // This field's value will be added to a "simulator" container's env as "KUBE_SCHEDULER_SIMULATOR_ETCD_URL".
  // optional
  EtcdURL *string
  // CORSAllowedOriginList indicates the AllowedOriginList for CORS. 
  // This is applied to kube-apiserver and simulator server.
  // This field's value will be added to a "simulator" container's env as "CORS_ALLOWED_ORIGIN_LIST".
  // optional.
  CORSAllowedOriginList        []string
  // Image should be the image of simulator.
  // You can use your customized simulator here, but the simulator container created by this image should follow these things:
  // - can be configured by the same environment variables. 
  // - (will add other requirements if any)
  Image                        string
  // Describes the pod that will be created.
  // This Pod should create the simulator.
  //
  // The system will add the container named "simulator" that: 
  // - run the simulator created by image specified in Image field,
  // - have Ports for kube-apiserver and the simulator endpoint,
  // - have environment variables to configure the simulator.
  // 
  // And if the EtcdURL field is empty, the system will add the container named "etcd" for kube-apiserver.
  // 
  // You can overwrite those container spec by defining containers that have the same name, "simulator" or "etcd".
  // 
  // When PodTemplate has the container(s) named "simulator" or "etcd" and the field is different from the container spec generated by other fields in SimulatorSpec,
  // container(s) definition in PodTemplate takes precedence.
  PodTemplate *corev1.PodTemplateSpec 
}

type SimulatorStatus struct {
  Phase SimulatorPhase

  // A human readable message indicating details about why the simulator is in this phase.
  // optional
  Message *string 

  // KubeAPIServerURL is the url to access the kube-apiserver in the simulator.
  KubeAPIServerURL string
}

type SimulatorPhase string
const (
  // SimulatorPending means the simulator is waiting for the controller to start to creating the Pod for this Simulator resource.
  SimulatorPending SimulatorPhase = "Pending"
  // SimulatorCreating means the simulator is being creating. 
  SimulatorCreating SimulatorPhase = "Creating"
  // SimulatorAvailable means the simulator is available.
  SimulatorAvailable SimulatorPhase = "Available"
)
```

## Alternatives

<!--
What other approaches did you consider, and why did you rule them out? These do
not need to be as detailed as the proposal, but should include enough
information to express the idea and why it was not acceptable.
-->
</file>

<file path="kube-scheduler-simulator/keps/184-scheduler-simulation/kep.yaml">
title: scheduler simulation 
kep-number: 184
authors:
  - "@sanposhiho"
  - "@everpeace"
owning-sig: sig-scheduling
status: provisional
creation-date: 2022-06-06
reviewers:
  - "@196Ikuchil"
approvers:
  - "@196Ikuchil"

stage: alpha

milestone:
  alpha: "v0.1"

see-also:
  - "/keps/140-scenario-based-simulation"
  - "/keps/159-scheduler-simulator-operator"
</file>

<file path="kube-scheduler-simulator/keps/184-scheduler-simulation/README.md">
# KEP-184: Scheduler Simulation

## Summary

A new `SchedulerSimulation` CRD and controller are implemented. 

`SchedulerSimulation` allows users to use `Simulator` for one-shot `Scenario`-based scheduler simulation.
(`Scenario` is the CRD to run scenario-based simulation introduced in KEP-140,
and `Simulator` is the CRD to create simulator as Pod introduced in KEP-167.)

## Motivation

With `Scenario` introduced in KEP-140, we can define scenarios and see how the scheduler behaves.

But, we cannot define "which scheduler will do it" in `Scenario`. 
It can't be helped because only one scheduler is running in a simulator and `Scenario` resource is created in a simulator.

Wouldn't it be nice to be able to run the same scenario with various schedulers and see which scheduler is the best one? Yes. That's one of the motivations. 

### Goals

- In `SchedulerSimulation` resource, users can define the `Scenario` and the `Simulator`'s spec which run that `Scenario`. 
- `SchedulerSimulation` resources (precisely the controller for `SchedulerSimulation`) will create the `Simulator` resource to run a simulator and run the `Scenario` in that simulator.

### Non-Goals

Create `SchedulerSimulation` in Web UI. (may be implemented in the future, but out of scope of this proposal.)

## Proposal

### Implementation design details

#### Required prior knowledge

The `Scenario` is the CRD to run scenario-based simulation introduced in KEP-140.
It is applied to kube-apiserver started in kube-scheduler-simulator, (all simulator has own kube-apiserver internally.)
and a scenario is start to running just after created.

The `Simulator` is the CRD to run the simulator as Pod in the cluster introduced in KEP-167.
It is applied to user's cluster (not to kube-apiserver started in a simulator.)

#### SchedulerSimulation CRD

The CRD `SchedulerSimulation` will be applied to kube-apiserver in user's cluster. (not to simulator's kube-apiserver.) 

It's namespaced resource. 

```go
type SchedulerSimulation struct {
  metav1.TypeMeta 
  metav1.ObjectMeta // namespaced.

  Spec SchedulerSimulationSpec
  Status SchedulerSimulationStatus
}
  
type SchedulerSimulationSpec struct {
  // Describes the simulator that will be created.
  // Cannot be updated.
  SimulatorSpec SimulatorSpec
  // ScenarioTemplateFilePath represents the path for file that has the Scenario resource's definition.
  // The Scenario defined in the file will be run in the simulator.
  //
  // It should be the path for the mounted path in most cases,
  // otherwise, the scenario won't created successfully.
  //
  // Cannot be updated.
  ScenarioTemplateFilePath string 
  // ScenarioResultFilePath represents the file path that the result of scenario will be stored.
  // 
  // It should be the path for the mounted path in most cases,
  // otherwise, the simulation result won't stored to the persistent file.
  //
  // Cannot be updated.
  ScenarioResultFilePath string
  // Volume to be mounted.
  //
  // The container named "scenario-runner" will be created in the simulator Pod.
  // That container will see the scenario defined in ScenarioTemplateFilePath, create the scenario in the simulator, and store the result into ScenarioReusltFilePath.
  // The volume specified here is mounted as mountPath `/mnt` in that container.
  //
  // More info about volume: https://kubernetes.io/docs/concepts/storage/volumes
  //
  // Cannot be updated.
  Volume corev1.Volume 
  // ScenarioRunnerImage is the image for the container named "scenario-runner" that will be created in the simulator Pod.
  // That container will see the scenario defined in ScenarioTemplateFilePath, create the scenario in the simulator, and store the result into ScenarioReusltFilePath.
  //
  // Default value is [TODO: provide the image of "scenario-runner" and write the image name here]
  // Cannot be updated.
  ScenarioRunnerImage corev1.Image
}

type SchedulerSimulationStatus struct {
  Phase SchedulerSimulationPhase

  // Represents time when the simulation controller started processing a job. 
  // It is represented in RFC3339 form and is in UTC.
  // +optional
  StartTime *metav1.Time 

  // Represents time when the simulation was completed. It is not guaranteed to
  // be set in happens-before order across separate operations.
  // It is represented in RFC3339 form and is in UTC.
  // The completion time is only set when the job finishes successfully.
  // +optional
  CompletionTime *metav1.Time 

  // A human readable message indicating details about why the simulation is in this phase.
  // optional
  Message *string 
}

type SchedulerSimulationPhase string
const (
  // SchedulerSimulationFailed means the job has failed its execution.
  SchedulerSimulationFailed SchedulerSimulationPhase = "Failed"
  // SchedulerSimulationCompleted means the simulation has completed its execution.
  SchedulerSimulationCompleted SchedulerSimulationPhase = "Completed"
)
```

#### Scenario runner container

The container named "scenario-runner" will be created in the simulator Pod(= the Pod created by `Simulator` resource). 
That container will fetch the `Scenario` defined in ScenarioTemplateFilePath, create the `Scenario` in the simulator, and store the result into ScenarioResultFilePath.

The volume specified in Volume and VolumeMount fields is mounted in that container.

So, the simulator Pod created for SchedulerSimulation with scenario-runner container will be like:

```yaml
  containers:
  # other containers definition....

  - name: scenario-runner
    image: # spec.ScenarioRunnerImage image here. 
    env:
    - name: SIMULATOR_KUBE_APISERVER_PORT # need to pass kube-apiserver port via env or command argument so that scenario-runner can access kube-apiserver in the simulator.
      value: 9999 # read from spec.SimulatorSpec.KubeAPIServerPort here.
    volumeMounts: # read from spec.VolumeMounts.
  volume: # read from spec.Volume.
```

#### SchedulerSimulation execution flow

0. User creates SchedulerSimulation resource in user's cluster.
1. The SchedulerSimulation controller creates a Simulator resource with "scenario-runner" container.
2. After the Pod gets started, the "scenario-runner" reads ScenarioTemplateFile and creates the Scenario resource in simulator's kube-apiserver.
3. The simulator run the scenario.
4. After the scenario is done, the "scenario-runner" fetch the Scenario's result from simulator's kube-apiserver and store it in ScenarioResultFile.
5. The "scenario-runner" container is terminated.
6. The SchedulerSimulation controller deletes the Simulator resource.
7. The controller changes the status of SchedulerSimulation to "Completed"

---

![diagram](images/execution_flow.png)

#### Why input the scenario and output the result are via file

The etcd has the size limitation against resource and if you are concerned about this, use ScenarioTemplateFile instead.
https://etcd.io/docs/v3.4/dev-guide/limit/#request-size-limit

> etcd is designed to handle small key value pairs typical for metadata. Larger requests will work, but may increase the latency of other requests. By default, the maximum size of any request is 1.5 MiB. This limit is configurable through --max-request-bytes flag for etcd server.

Unlike `Scenario` CRD, which is applied to simulator's kube-apiserver, `SchedulerSimulation` CRD is applied to kube-apiserver in user's cluster,
and we should not have a negative impact on kube-apiserver in user's cluster.

### User Stories 

#### Story 1

The company has added many features into scheduler via some custom plugins, 
and they want to make sure that their implementation changes for the plugins are working as expected and has not negatively impacted the scheduling results in the CI.

##### Solution

They can run the simulation by creating SchedulerSimulation resources in CI.
By analyzes the results from SchedulerSimulation and calculates a score from some point of view, they can see the impact of that implementation change in CI and evaluate their implemantation changes.

## Alternatives

### Including Scenario field in SchedulerSimulationSpec

Like:

```go
type SchedulerSimulationSpec struct {
  // Scenario represents the scenario that will be run in the simulator.
  // Note that using both Scenario and ScenarioTemplateFile makes no sense. (Scenario has priority over ScenarioTemplateFile in that case.) 
  // 
  // The etcd has the size limitation against resource and if you are concerned about this, use ScenarioTemplateFile instead.
  // https://etcd.io/docs/v3.4/dev-guide/limit/#request-size-limit
  Scenario *ScenarioSpec
  // ScenarioTemplateFile represents the path for file that has the Scenario resource's definition.
  // The Scenario defined in the file will be run in the simulator.
  //
  // Note that using both Scenario and ScenarioTemplateFile makes no sense. (Scenario has priority over ScenarioTemplateFile in that case.) 
  ScenarioTemplateFile *string 
  //
  // Other fields....
}
```

Given the difficulty in deciding whether to use Scenario or ScenarioTemplateFile, Scenario field will only confuse users.
</file>

<file path="kube-scheduler-simulator/keps/README.md">
## Internal Kubernetes Enhancement Proposals (KEPs)

This directory contains internal KEP for kube-scheduler-simulator.
The operation is the same as the original KEP for Kubernetes repo, except that it is managed internally in this repository.

> A Kubernetes Enhancement Proposal (KEP) is a way to propose, communicate and coordinate on new efforts for the Kubernetes project. You can read the full details of the project in KEP-0000.
This process is still in a beta state and is mandatory for all enhancements beginning release 1.14.
> https://github.com/kubernetes/enhancements/tree/master/keps

## What is KEP?

Please see https://github.com/kubernetes/enhancements/tree/master/keps.

## How to start to write new KEP?

Follow the process outlined in the KEP template.

https://github.com/kubernetes/enhancements/tree/master/keps/NNNN-kep-template

## What kind of features needs KEP? 

The big change requires KEP. 
Yes, what you want to say is "what is the big change then?", right? 

It's actually case-by-case, but many of the following changes will require KEP, for example:
- Introduce a new CRD.
- Add a new API to a CRD.
- Add a big change to the behavior of the core feature.
- Add a new component on Web UI.

If you are not sure if it is necessary, 
ask [OWNERS](https://github.com/kubernetes-sigs/kube-scheduler-simulator/blob/master/OWNERS).
</file>

<file path="kube-scheduler-simulator/scenario/.devcontainer/devcontainer.json">
{
  "name": "Kubebuilder DevContainer",
  "image": "docker.io/golang:1.23",
  "features": {
    "ghcr.io/devcontainers/features/docker-in-docker:2": {},
    "ghcr.io/devcontainers/features/git:1": {}
  },

  "runArgs": ["--network=host"],

  "customizations": {
    "vscode": {
      "settings": {
        "terminal.integrated.shell.linux": "/bin/bash"
      },
      "extensions": [
        "ms-kubernetes-tools.vscode-kubernetes-tools",
        "ms-azuretools.vscode-docker"
      ]
    }
  },

  "onCreateCommand": "bash .devcontainer/post-install.sh"
}
</file>

<file path="kube-scheduler-simulator/scenario/.devcontainer/post-install.sh">
#!/bin/bash
set -x

curl -Lo ./kind https://kind.sigs.k8s.io/dl/latest/kind-linux-amd64
chmod +x ./kind
mv ./kind /usr/local/bin/kind

curl -L -o kubebuilder https://go.kubebuilder.io/dl/latest/linux/amd64
chmod +x kubebuilder
mv kubebuilder /usr/local/bin/

KUBECTL_VERSION=$(curl -L -s https://dl.k8s.io/release/stable.txt)
curl -LO "https://dl.k8s.io/release/$KUBECTL_VERSION/bin/linux/amd64/kubectl"
chmod +x kubectl
mv kubectl /usr/local/bin/kubectl

docker network create -d=bridge --subnet=172.19.0.0/24 kind

kind version
kubebuilder version
docker --version
go version
kubectl version --client
</file>

<file path="kube-scheduler-simulator/scenario/api/v1alpha1/groupversion_info.go">
/*
Copyright 2022.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

// Package v1alpha1 contains API Schema definitions for the simulation v1alpha1 API group.
// +kubebuilder:object:generate=true
// +groupName=simulation.kube-scheduler-simulator.x-k8s.io
package v1alpha1

import (
	"k8s.io/apimachinery/pkg/runtime/schema"
	"sigs.k8s.io/controller-runtime/pkg/scheme"
)

var (
	// GroupVersion is group version used to register these objects.
	GroupVersion = schema.GroupVersion{Group: "simulation.kube-scheduler-simulator.x-k8s.io", Version: "v1alpha1"}

	// SchemeBuilder is used to add go types to the GroupVersionKind scheme.
	SchemeBuilder = &scheme.Builder{GroupVersion: GroupVersion}

	// AddToScheme adds the types in this group-version to the given scheme.
	AddToScheme = SchemeBuilder.AddToScheme
)
</file>

<file path="kube-scheduler-simulator/scenario/api/v1alpha1/scenario_types.go">
/*
Copyright 2022.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package v1alpha1

import (
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

// EDIT THIS FILE!  THIS IS SCAFFOLDING FOR YOU TO OWN!
// NOTE: json tags are required.  Any new fields you add must have json tags for the fields to be serialized.

// ScenarioSpec defines the desired state of Scenario.
type ScenarioSpec struct {
	// INSERT ADDITIONAL SPEC FIELDS - desired state of cluster
	// Important: Run "make" to regenerate code after modifying this file

	// Foo is an example field of Scenario. Edit scenario_types.go to remove/update
	Foo string `json:"foo,omitempty"`
}

// ScenarioStatus defines the observed state of Scenario.
type ScenarioStatus struct {
	// INSERT ADDITIONAL STATUS FIELD - define observed state of cluster
	// Important: Run "make" to regenerate code after modifying this file
}

// +kubebuilder:object:root=true
// +kubebuilder:subresource:status

// Scenario is the Schema for the scenarios API.
type Scenario struct {
	metav1.TypeMeta   `json:",inline"`
	metav1.ObjectMeta `json:"metadata,omitempty"`

	Spec   ScenarioSpec   `json:"spec,omitempty"`
	Status ScenarioStatus `json:"status,omitempty"`
}

// +kubebuilder:object:root=true

// ScenarioList contains a list of Scenario.
type ScenarioList struct {
	metav1.TypeMeta `json:",inline"`
	metav1.ListMeta `json:"metadata,omitempty"`
	Items           []Scenario `json:"items"`
}

func init() {
	SchemeBuilder.Register(&Scenario{}, &ScenarioList{})
}
</file>

<file path="kube-scheduler-simulator/scenario/api/v1alpha1/zz_generated.deepcopy.go">
//go:build !ignore_autogenerated

/*
Copyright 2022.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

// Code generated by controller-gen. DO NOT EDIT.

package v1alpha1

import (
	runtime "k8s.io/apimachinery/pkg/runtime"
)

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *Scenario) DeepCopyInto(out *Scenario) {
	*out = *in
	out.TypeMeta = in.TypeMeta
	in.ObjectMeta.DeepCopyInto(&out.ObjectMeta)
	out.Spec = in.Spec
	out.Status = in.Status
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new Scenario.
func (in *Scenario) DeepCopy() *Scenario {
	if in == nil {
		return nil
	}
	out := new(Scenario)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyObject is an autogenerated deepcopy function, copying the receiver, creating a new runtime.Object.
func (in *Scenario) DeepCopyObject() runtime.Object {
	if c := in.DeepCopy(); c != nil {
		return c
	}
	return nil
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *ScenarioList) DeepCopyInto(out *ScenarioList) {
	*out = *in
	out.TypeMeta = in.TypeMeta
	in.ListMeta.DeepCopyInto(&out.ListMeta)
	if in.Items != nil {
		in, out := &in.Items, &out.Items
		*out = make([]Scenario, len(*in))
		for i := range *in {
			(*in)[i].DeepCopyInto(&(*out)[i])
		}
	}
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new ScenarioList.
func (in *ScenarioList) DeepCopy() *ScenarioList {
	if in == nil {
		return nil
	}
	out := new(ScenarioList)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyObject is an autogenerated deepcopy function, copying the receiver, creating a new runtime.Object.
func (in *ScenarioList) DeepCopyObject() runtime.Object {
	if c := in.DeepCopy(); c != nil {
		return c
	}
	return nil
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *ScenarioSpec) DeepCopyInto(out *ScenarioSpec) {
	*out = *in
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new ScenarioSpec.
func (in *ScenarioSpec) DeepCopy() *ScenarioSpec {
	if in == nil {
		return nil
	}
	out := new(ScenarioSpec)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *ScenarioStatus) DeepCopyInto(out *ScenarioStatus) {
	*out = *in
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new ScenarioStatus.
func (in *ScenarioStatus) DeepCopy() *ScenarioStatus {
	if in == nil {
		return nil
	}
	out := new(ScenarioStatus)
	in.DeepCopyInto(out)
	return out
}
</file>

<file path="kube-scheduler-simulator/scenario/cmd/main.go">
/*
Copyright 2025.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package main

import (
	"crypto/tls"
	"flag"
	"os"
	"path/filepath"

	"k8s.io/apimachinery/pkg/runtime"
	utilruntime "k8s.io/apimachinery/pkg/util/runtime"
	clientgoscheme "k8s.io/client-go/kubernetes/scheme"
	// Import all Kubernetes client auth plugins (e.g. Azure, GCP, OIDC, etc.)
	// to ensure that exec-entrypoint and run can make use of them.
	_ "k8s.io/client-go/plugin/pkg/client/auth"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/certwatcher"
	"sigs.k8s.io/controller-runtime/pkg/healthz"
	"sigs.k8s.io/controller-runtime/pkg/log/zap"
	"sigs.k8s.io/controller-runtime/pkg/metrics/filters"
	metricsserver "sigs.k8s.io/controller-runtime/pkg/metrics/server"
	"sigs.k8s.io/controller-runtime/pkg/webhook"
	simulationv1alpha1 "sigs.k8s.io/kube-scheduler-simulator/scenario/api/v1alpha1"
	"sigs.k8s.io/kube-scheduler-simulator/scenario/internal/controller"
)

var (
	scheme   = runtime.NewScheme()
	setupLog = ctrl.Log.WithName("setup")
)

func init() {
	utilruntime.Must(clientgoscheme.AddToScheme(scheme))

	utilruntime.Must(simulationv1alpha1.AddToScheme(scheme))
	// +kubebuilder:scaffold:scheme
}

// nolint:gocyclo
func main() {
	var metricsAddr string
	var metricsCertPath, metricsCertName, metricsCertKey string
	var webhookCertPath, webhookCertName, webhookCertKey string
	var enableLeaderElection bool
	var probeAddr string
	var secureMetrics bool
	var enableHTTP2 bool
	var tlsOpts []func(*tls.Config)
	flag.StringVar(&metricsAddr, "metrics-bind-address", "0", "The address the metrics endpoint binds to. "+
		"Use :8443 for HTTPS or :8080 for HTTP, or leave as 0 to disable the metrics service.")
	flag.StringVar(&probeAddr, "health-probe-bind-address", ":8081", "The address the probe endpoint binds to.")
	flag.BoolVar(&enableLeaderElection, "leader-elect", false,
		"Enable leader election for controller manager. "+
			"Enabling this will ensure there is only one active controller manager.")
	flag.BoolVar(&secureMetrics, "metrics-secure", true,
		"If set, the metrics endpoint is served securely via HTTPS. Use --metrics-secure=false to use HTTP instead.")
	flag.StringVar(&webhookCertPath, "webhook-cert-path", "", "The directory that contains the webhook certificate.")
	flag.StringVar(&webhookCertName, "webhook-cert-name", "tls.crt", "The name of the webhook certificate file.")
	flag.StringVar(&webhookCertKey, "webhook-cert-key", "tls.key", "The name of the webhook key file.")
	flag.StringVar(&metricsCertPath, "metrics-cert-path", "",
		"The directory that contains the metrics server certificate.")
	flag.StringVar(&metricsCertName, "metrics-cert-name", "tls.crt", "The name of the metrics server certificate file.")
	flag.StringVar(&metricsCertKey, "metrics-cert-key", "tls.key", "The name of the metrics server key file.")
	flag.BoolVar(&enableHTTP2, "enable-http2", false,
		"If set, HTTP/2 will be enabled for the metrics and webhook servers")
	opts := zap.Options{
		Development: true,
	}
	opts.BindFlags(flag.CommandLine)
	flag.Parse()

	ctrl.SetLogger(zap.New(zap.UseFlagOptions(&opts)))

	// if the enable-http2 flag is false (the default), http/2 should be disabled
	// due to its vulnerabilities. More specifically, disabling http/2 will
	// prevent from being vulnerable to the HTTP/2 Stream Cancellation and
	// Rapid Reset CVEs. For more information see:
	// - https://github.com/advisories/GHSA-qppj-fm5r-hxr3
	// - https://github.com/advisories/GHSA-4374-p667-p6c8
	disableHTTP2 := func(c *tls.Config) {
		setupLog.Info("disabling http/2")
		c.NextProtos = []string{"http/1.1"}
	}

	if !enableHTTP2 {
		tlsOpts = append(tlsOpts, disableHTTP2)
	}

	// Create watchers for metrics and webhooks certificates
	var metricsCertWatcher, webhookCertWatcher *certwatcher.CertWatcher

	// Initial webhook TLS options
	webhookTLSOpts := tlsOpts

	if len(webhookCertPath) > 0 {
		setupLog.Info("Initializing webhook certificate watcher using provided certificates",
			"webhook-cert-path", webhookCertPath, "webhook-cert-name", webhookCertName, "webhook-cert-key", webhookCertKey)

		var err error
		webhookCertWatcher, err = certwatcher.New(
			filepath.Join(webhookCertPath, webhookCertName),
			filepath.Join(webhookCertPath, webhookCertKey),
		)
		if err != nil {
			setupLog.Error(err, "Failed to initialize webhook certificate watcher")
			os.Exit(1)
		}

		webhookTLSOpts = append(webhookTLSOpts, func(config *tls.Config) {
			config.GetCertificate = webhookCertWatcher.GetCertificate
		})
	}

	webhookServer := webhook.NewServer(webhook.Options{
		TLSOpts: webhookTLSOpts,
	})

	// Metrics endpoint is enabled in 'config/default/kustomization.yaml'. The Metrics options configure the server.
	// More info:
	// - https://pkg.go.dev/sigs.k8s.io/controller-runtime@v0.20.0/pkg/metrics/server
	// - https://book.kubebuilder.io/reference/metrics.html
	metricsServerOptions := metricsserver.Options{
		BindAddress:   metricsAddr,
		SecureServing: secureMetrics,
		TLSOpts:       tlsOpts,
	}

	if secureMetrics {
		// FilterProvider is used to protect the metrics endpoint with authn/authz.
		// These configurations ensure that only authorized users and service accounts
		// can access the metrics endpoint. The RBAC are configured in 'config/rbac/kustomization.yaml'. More info:
		// https://pkg.go.dev/sigs.k8s.io/controller-runtime@v0.20.0/pkg/metrics/filters#WithAuthenticationAndAuthorization
		metricsServerOptions.FilterProvider = filters.WithAuthenticationAndAuthorization
	}

	// If the certificate is not specified, controller-runtime will automatically
	// generate self-signed certificates for the metrics server. While convenient for development and testing,
	// this setup is not recommended for production.
	//
	// TODO(user): If you enable certManager, uncomment the following lines:
	// - [METRICS-WITH-CERTS] at config/default/kustomization.yaml to generate and use certificates
	// managed by cert-manager for the metrics server.
	// - [PROMETHEUS-WITH-CERTS] at config/prometheus/kustomization.yaml for TLS certification.
	if len(metricsCertPath) > 0 {
		setupLog.Info("Initializing metrics certificate watcher using provided certificates",
			"metrics-cert-path", metricsCertPath, "metrics-cert-name", metricsCertName, "metrics-cert-key", metricsCertKey)

		var err error
		metricsCertWatcher, err = certwatcher.New(
			filepath.Join(metricsCertPath, metricsCertName),
			filepath.Join(metricsCertPath, metricsCertKey),
		)
		if err != nil {
			setupLog.Error(err, "to initialize metrics certificate watcher", "error", err)
			os.Exit(1)
		}

		metricsServerOptions.TLSOpts = append(metricsServerOptions.TLSOpts, func(config *tls.Config) {
			config.GetCertificate = metricsCertWatcher.GetCertificate
		})
	}

	mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{
		Scheme:                 scheme,
		Metrics:                metricsServerOptions,
		WebhookServer:          webhookServer,
		HealthProbeBindAddress: probeAddr,
		LeaderElection:         enableLeaderElection,
		LeaderElectionID:       "621d549f.kube-scheduler-simulator.x-k8s.io",
		// LeaderElectionReleaseOnCancel defines if the leader should step down voluntarily
		// when the Manager ends. This requires the binary to immediately end when the
		// Manager is stopped, otherwise, this setting is unsafe. Setting this significantly
		// speeds up voluntary leader transitions as the new leader don't have to wait
		// LeaseDuration time first.
		//
		// In the default scaffold provided, the program ends immediately after
		// the manager stops, so would be fine to enable this option. However,
		// if you are doing or is intended to do any operation such as perform cleanups
		// after the manager stops then its usage might be unsafe.
		// LeaderElectionReleaseOnCancel: true,
	})
	if err != nil {
		setupLog.Error(err, "unable to start manager")
		os.Exit(1)
	}

	if err = (&controller.ScenarioReconciler{
		Client: mgr.GetClient(),
		Scheme: mgr.GetScheme(),
	}).SetupWithManager(mgr); err != nil {
		setupLog.Error(err, "unable to create controller", "controller", "Scenario")
		os.Exit(1)
	}
	// +kubebuilder:scaffold:builder

	if metricsCertWatcher != nil {
		setupLog.Info("Adding metrics certificate watcher to manager")
		if err := mgr.Add(metricsCertWatcher); err != nil {
			setupLog.Error(err, "unable to add metrics certificate watcher to manager")
			os.Exit(1)
		}
	}

	if webhookCertWatcher != nil {
		setupLog.Info("Adding webhook certificate watcher to manager")
		if err := mgr.Add(webhookCertWatcher); err != nil {
			setupLog.Error(err, "unable to add webhook certificate watcher to manager")
			os.Exit(1)
		}
	}

	if err := mgr.AddHealthzCheck("healthz", healthz.Ping); err != nil {
		setupLog.Error(err, "unable to set up health check")
		os.Exit(1)
	}
	if err := mgr.AddReadyzCheck("readyz", healthz.Ping); err != nil {
		setupLog.Error(err, "unable to set up ready check")
		os.Exit(1)
	}

	setupLog.Info("starting manager")
	if err := mgr.Start(ctrl.SetupSignalHandler()); err != nil {
		setupLog.Error(err, "problem running manager")
		os.Exit(1)
	}
}
</file>

<file path="kube-scheduler-simulator/scenario/config/crd/bases/simulation.kube-scheduler-simulator.x-k8s.io_scenarios.yaml">
---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  annotations:
    controller-gen.kubebuilder.io/version: v0.17.1
  name: scenarios.simulation.kube-scheduler-simulator.x-k8s.io
spec:
  group: simulation.kube-scheduler-simulator.x-k8s.io
  names:
    kind: Scenario
    listKind: ScenarioList
    plural: scenarios
    singular: scenario
  scope: Namespaced
  versions:
  - name: v1alpha1
    schema:
      openAPIV3Schema:
        description: Scenario is the Schema for the scenarios API.
        properties:
          apiVersion:
            description: |-
              APIVersion defines the versioned schema of this representation of an object.
              Servers should convert recognized schemas to the latest internal value, and
              may reject unrecognized values.
              More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources
            type: string
          kind:
            description: |-
              Kind is a string value representing the REST resource this object represents.
              Servers may infer this from the endpoint the client submits requests to.
              Cannot be updated.
              In CamelCase.
              More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds
            type: string
          metadata:
            type: object
          spec:
            description: ScenarioSpec defines the desired state of Scenario.
            properties:
              foo:
                description: Foo is an example field of Scenario. Edit scenario_types.go
                  to remove/update
                type: string
            type: object
          status:
            description: ScenarioStatus defines the observed state of Scenario.
            type: object
        type: object
    served: true
    storage: true
    subresources:
      status: {}
</file>

<file path="kube-scheduler-simulator/scenario/config/crd/kustomization.yaml">
# This kustomization.yaml is not intended to be run by itself,
# since it depends on service name and namespace that are out of this kustomize package.
# It should be run by config/default
resources:
- bases/simulation.kube-scheduler-simulator.x-k8s.io_scenarios.yaml
# +kubebuilder:scaffold:crdkustomizeresource

patches:
# [WEBHOOK] To enable webhook, uncomment all the sections with [WEBHOOK] prefix.
# patches here are for enabling the conversion webhook for each CRD
# +kubebuilder:scaffold:crdkustomizewebhookpatch

# [WEBHOOK] To enable webhook, uncomment the following section
# the following config is for teaching kustomize how to do kustomization for CRDs.
configurations:
- kustomizeconfig.yaml
</file>

<file path="kube-scheduler-simulator/scenario/config/crd/kustomizeconfig.yaml">
# This file is for teaching kustomize how to substitute name and namespace reference in CRD
nameReference:
- kind: Service
  version: v1
  fieldSpecs:
  - kind: CustomResourceDefinition
    version: v1
    group: apiextensions.k8s.io
    path: spec/conversion/webhook/clientConfig/service/name

namespace:
- kind: CustomResourceDefinition
  version: v1
  group: apiextensions.k8s.io
  path: spec/conversion/webhook/clientConfig/service/namespace
  create: false

varReference:
- path: metadata/annotations
</file>

<file path="kube-scheduler-simulator/scenario/config/default/cert_metrics_manager_patch.yaml">
# This patch adds the args, volumes, and ports to allow the manager to use the metrics-server certs.

# Add the volumeMount for the metrics-server certs
- op: add
  path: /spec/template/spec/containers/0/volumeMounts/-
  value:
    mountPath: /tmp/k8s-metrics-server/metrics-certs
    name: metrics-certs
    readOnly: true

# Add the --metrics-cert-path argument for the metrics server
- op: add
  path: /spec/template/spec/containers/0/args/-
  value: --metrics-cert-path=/tmp/k8s-metrics-server/metrics-certs

# Add the metrics-server certs volume configuration
- op: add
  path: /spec/template/spec/volumes/-
  value:
    name: metrics-certs
    secret:
      secretName: metrics-server-cert
      optional: false
      items:
        - key: ca.crt
          path: ca.crt
        - key: tls.crt
          path: tls.crt
        - key: tls.key
          path: tls.key
</file>

<file path="kube-scheduler-simulator/scenario/config/default/kustomization.yaml">
# Adds namespace to all resources.
namespace: scenario-system

# Value of this field is prepended to the
# names of all resources, e.g. a deployment named
# "wordpress" becomes "alices-wordpress".
# Note that it should also match with the prefix (text before '-') of the namespace
# field above.
namePrefix: scenario-

# Labels to add to all resources and selectors.
#labels:
#- includeSelectors: true
#  pairs:
#    someName: someValue

resources:
- ../crd
- ../rbac
- ../manager
# [WEBHOOK] To enable webhook, uncomment all the sections with [WEBHOOK] prefix including the one in
# crd/kustomization.yaml
#- ../webhook
# [CERTMANAGER] To enable cert-manager, uncomment all sections with 'CERTMANAGER'. 'WEBHOOK' components are required.
#- ../certmanager
# [PROMETHEUS] To enable prometheus monitor, uncomment all sections with 'PROMETHEUS'.
#- ../prometheus
# [METRICS] Expose the controller manager metrics service.
- metrics_service.yaml
# [NETWORK POLICY] Protect the /metrics endpoint and Webhook Server with NetworkPolicy.
# Only Pod(s) running a namespace labeled with 'metrics: enabled' will be able to gather the metrics.
# Only CR(s) which requires webhooks and are applied on namespaces labeled with 'webhooks: enabled' will
# be able to communicate with the Webhook Server.
#- ../network-policy

# Uncomment the patches line if you enable Metrics
patches:
# [METRICS] The following patch will enable the metrics endpoint using HTTPS and the port :8443.
# More info: https://book.kubebuilder.io/reference/metrics
- path: manager_metrics_patch.yaml
  target:
    kind: Deployment

# Uncomment the patches line if you enable Metrics and CertManager
# [METRICS-WITH-CERTS] To enable metrics protected with certManager, uncomment the following line.
# This patch will protect the metrics with certManager self-signed certs.
#- path: cert_metrics_manager_patch.yaml
#  target:
#    kind: Deployment

# [WEBHOOK] To enable webhook, uncomment all the sections with [WEBHOOK] prefix including the one in
# crd/kustomization.yaml
#- path: manager_webhook_patch.yaml
#  target:
#    kind: Deployment

# [CERTMANAGER] To enable cert-manager, uncomment all sections with 'CERTMANAGER' prefix.
# Uncomment the following replacements to add the cert-manager CA injection annotations
#replacements:
# - source: # Uncomment the following block to enable certificates for metrics
#     kind: Service
#     version: v1
#     name: controller-manager-metrics-service
#     fieldPath: metadata.name
#   targets:
#     - select:
#         kind: Certificate
#         group: cert-manager.io
#         version: v1
#         name: metrics-certs
#       fieldPaths:
#         - spec.dnsNames.0
#         - spec.dnsNames.1
#       options:
#         delimiter: '.'
#         index: 0
#         create: true
#
# - source:
#     kind: Service
#     version: v1
#     name: controller-manager-metrics-service
#     fieldPath: metadata.namespace
#   targets:
#     - select:
#         kind: Certificate
#         group: cert-manager.io
#         version: v1
#         name: metrics-certs
#       fieldPaths:
#         - spec.dnsNames.0
#         - spec.dnsNames.1
#       options:
#         delimiter: '.'
#         index: 1
#         create: true
#
# - source: # Uncomment the following block if you have any webhook
#     kind: Service
#     version: v1
#     name: webhook-service
#     fieldPath: .metadata.name # Name of the service
#   targets:
#     - select:
#         kind: Certificate
#         group: cert-manager.io
#         version: v1
#         name: serving-cert
#       fieldPaths:
#         - .spec.dnsNames.0
#         - .spec.dnsNames.1
#       options:
#         delimiter: '.'
#         index: 0
#         create: true
# - source:
#     kind: Service
#     version: v1
#     name: webhook-service
#     fieldPath: .metadata.namespace # Namespace of the service
#   targets:
#     - select:
#         kind: Certificate
#         group: cert-manager.io
#         version: v1
#         name: serving-cert
#       fieldPaths:
#         - .spec.dnsNames.0
#         - .spec.dnsNames.1
#       options:
#         delimiter: '.'
#         index: 1
#         create: true
#
# - source: # Uncomment the following block if you have a ValidatingWebhook (--programmatic-validation)
#     kind: Certificate
#     group: cert-manager.io
#     version: v1
#     name: serving-cert # This name should match the one in certificate.yaml
#     fieldPath: .metadata.namespace # Namespace of the certificate CR
#   targets:
#     - select:
#         kind: ValidatingWebhookConfiguration
#       fieldPaths:
#         - .metadata.annotations.[cert-manager.io/inject-ca-from]
#       options:
#         delimiter: '/'
#         index: 0
#         create: true
# - source:
#     kind: Certificate
#     group: cert-manager.io
#     version: v1
#     name: serving-cert
#     fieldPath: .metadata.name
#   targets:
#     - select:
#         kind: ValidatingWebhookConfiguration
#       fieldPaths:
#         - .metadata.annotations.[cert-manager.io/inject-ca-from]
#       options:
#         delimiter: '/'
#         index: 1
#         create: true
#
# - source: # Uncomment the following block if you have a DefaultingWebhook (--defaulting )
#     kind: Certificate
#     group: cert-manager.io
#     version: v1
#     name: serving-cert
#     fieldPath: .metadata.namespace # Namespace of the certificate CR
#   targets:
#     - select:
#         kind: MutatingWebhookConfiguration
#       fieldPaths:
#         - .metadata.annotations.[cert-manager.io/inject-ca-from]
#       options:
#         delimiter: '/'
#         index: 0
#         create: true
# - source:
#     kind: Certificate
#     group: cert-manager.io
#     version: v1
#     name: serving-cert
#     fieldPath: .metadata.name
#   targets:
#     - select:
#         kind: MutatingWebhookConfiguration
#       fieldPaths:
#         - .metadata.annotations.[cert-manager.io/inject-ca-from]
#       options:
#         delimiter: '/'
#         index: 1
#         create: true
#
# - source: # Uncomment the following block if you have a ConversionWebhook (--conversion)
#     kind: Certificate
#     group: cert-manager.io
#     version: v1
#     name: serving-cert
#     fieldPath: .metadata.namespace # Namespace of the certificate CR
#   targets: # Do not remove or uncomment the following scaffold marker; required to generate code for target CRD.
# +kubebuilder:scaffold:crdkustomizecainjectionns
# - source:
#     kind: Certificate
#     group: cert-manager.io
#     version: v1
#     name: serving-cert
#     fieldPath: .metadata.name
#   targets: # Do not remove or uncomment the following scaffold marker; required to generate code for target CRD.
# +kubebuilder:scaffold:crdkustomizecainjectionname
</file>

<file path="kube-scheduler-simulator/scenario/config/default/manager_metrics_patch.yaml">
# This patch adds the args to allow exposing the metrics endpoint using HTTPS
- op: add
  path: /spec/template/spec/containers/0/args/0
  value: --metrics-bind-address=:8443
</file>

<file path="kube-scheduler-simulator/scenario/config/default/metrics_service.yaml">
apiVersion: v1
kind: Service
metadata:
  labels:
    control-plane: controller-manager
    app.kubernetes.io/name: scenario
    app.kubernetes.io/managed-by: kustomize
  name: controller-manager-metrics-service
  namespace: system
spec:
  ports:
  - name: https
    port: 8443
    protocol: TCP
    targetPort: 8443
  selector:
    control-plane: controller-manager
    app.kubernetes.io/name: scenario
</file>

<file path="kube-scheduler-simulator/scenario/config/manager/kustomization.yaml">
resources:
- manager.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
images:
- name: controller
  newName: controller
  newTag: latest
</file>

<file path="kube-scheduler-simulator/scenario/config/manager/manager.yaml">
apiVersion: v1
kind: Namespace
metadata:
  labels:
    control-plane: controller-manager
    app.kubernetes.io/name: scenario
    app.kubernetes.io/managed-by: kustomize
  name: system
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: controller-manager
  namespace: system
  labels:
    control-plane: controller-manager
    app.kubernetes.io/name: scenario
    app.kubernetes.io/managed-by: kustomize
spec:
  selector:
    matchLabels:
      control-plane: controller-manager
      app.kubernetes.io/name: scenario
  replicas: 1
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: manager
      labels:
        control-plane: controller-manager
        app.kubernetes.io/name: scenario
    spec:
      securityContext:
        runAsNonRoot: true
        # TODO(user): For common cases that do not require escalating privileges
        # it is recommended to ensure that all your Pods/Containers are restrictive.
        # More info: https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted
        # Please uncomment the following code if your project does NOT have to work on old Kubernetes
        # versions < 1.19 or on vendors versions which do NOT support this field by default (i.e. Openshift < 4.11 ).
        # seccompProfile:
        #   type: RuntimeDefault
        seccompProfile:
          type: RuntimeDefault
      containers:
      - command:
        - /manager
        args:
          - --leader-elect
          - --health-probe-bind-address=:8081
        image: controller:latest
        imagePullPolicy: IfNotPresent
        name: manager
        ports: []
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - "ALL"
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8081
          initialDelaySeconds: 15
          periodSeconds: 20
        readinessProbe:
          httpGet:
            path: /readyz
            port: 8081
          initialDelaySeconds: 5
          periodSeconds: 10
        # TODO(user): Configure the resources accordingly based on the project requirements.
        # More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
        resources:
          limits:
            cpu: 500m
            memory: 128Mi
          requests:
            cpu: 10m
            memory: 64Mi
        volumeMounts: []
      volumes: []
      serviceAccountName: controller-manager
      terminationGracePeriodSeconds: 10
</file>

<file path="kube-scheduler-simulator/scenario/config/network-policy/allow-metrics-traffic.yaml">
# This NetworkPolicy allows ingress traffic
# with Pods running on namespaces labeled with 'metrics: enabled'. Only Pods on those
# namespaces are able to gather data from the metrics endpoint.
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  labels:
    app.kubernetes.io/name: scenario
    app.kubernetes.io/managed-by: kustomize
  name: allow-metrics-traffic
  namespace: system
spec:
  podSelector:
    matchLabels:
      control-plane: controller-manager
      app.kubernetes.io/name: scenario
  policyTypes:
    - Ingress
  ingress:
    # This allows ingress traffic from any namespace with the label metrics: enabled
    - from:
      - namespaceSelector:
          matchLabels:
            metrics: enabled  # Only from namespaces with this label
      ports:
        - port: 8443
          protocol: TCP
</file>

<file path="kube-scheduler-simulator/scenario/config/network-policy/kustomization.yaml">
resources:
- allow-metrics-traffic.yaml
</file>

<file path="kube-scheduler-simulator/scenario/config/prometheus/kustomization.yaml">
resources:
- monitor.yaml

# [PROMETHEUS-WITH-CERTS] The following patch configures the ServiceMonitor in ../prometheus
# to securely reference certificates created and managed by cert-manager.
# Additionally, ensure that you uncomment the [METRICS WITH CERTMANAGER] patch under config/default/kustomization.yaml
# to mount the "metrics-server-cert" secret in the Manager Deployment.
#patches:
#  - path: monitor_tls_patch.yaml
#    target:
#      kind: ServiceMonitor
</file>

<file path="kube-scheduler-simulator/scenario/config/prometheus/monitor_tls_patch.yaml">
# Patch for Prometheus ServiceMonitor to enable secure TLS configuration
# using certificates managed by cert-manager
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: controller-manager-metrics-monitor
  namespace: system
spec:
  endpoints:
    - tlsConfig:
        insecureSkipVerify: false
        ca:
          secret:
            name: metrics-server-cert
            key: ca.crt
        cert:
          secret:
            name: metrics-server-cert
            key: tls.crt
        keySecret:
          name: metrics-server-cert
          key: tls.key
</file>

<file path="kube-scheduler-simulator/scenario/config/prometheus/monitor.yaml">
# Prometheus Monitor Service (Metrics)
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    control-plane: controller-manager
    app.kubernetes.io/name: scenario
    app.kubernetes.io/managed-by: kustomize
  name: controller-manager-metrics-monitor
  namespace: system
spec:
  endpoints:
    - path: /metrics
      port: https # Ensure this is the name of the port that exposes HTTPS metrics
      scheme: https
      bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
      tlsConfig:
        # TODO(user): The option insecureSkipVerify: true is not recommended for production since it disables
        # certificate verification, exposing the system to potential man-in-the-middle attacks.
        # For production environments, it is recommended to use cert-manager for automatic TLS certificate management.
        # To apply this configuration, enable cert-manager and use the patch located at config/prometheus/servicemonitor_tls_patch.yaml,
        # which securely references the certificate from the 'metrics-server-cert' secret.
        insecureSkipVerify: true
  selector:
    matchLabels:
      control-plane: controller-manager
      app.kubernetes.io/name: scenario
</file>

<file path="kube-scheduler-simulator/scenario/config/rbac/kustomization.yaml">
resources:
# All RBAC will be applied under this service account in
# the deployment namespace. You may comment out this resource
# if your manager will use a service account that exists at
# runtime. Be sure to update RoleBinding and ClusterRoleBinding
# subjects if changing service account names.
- service_account.yaml
- role.yaml
- role_binding.yaml
- leader_election_role.yaml
- leader_election_role_binding.yaml
# The following RBAC configurations are used to protect
# the metrics endpoint with authn/authz. These configurations
# ensure that only authorized users and service accounts
# can access the metrics endpoint. Comment the following
# permissions if you want to disable this protection.
# More info: https://book.kubebuilder.io/reference/metrics.html
- metrics_auth_role.yaml
- metrics_auth_role_binding.yaml
- metrics_reader_role.yaml
# For each CRD, "Admin", "Editor" and "Viewer" roles are scaffolded by
# default, aiding admins in cluster management. Those roles are
# not used by the {{ .ProjectName }} itself. You can comment the following lines
# if you do not want those helpers be installed with your Project.
- scenario_admin_role.yaml
- scenario_editor_role.yaml
- scenario_viewer_role.yaml
</file>

<file path="kube-scheduler-simulator/scenario/config/rbac/leader_election_role_binding.yaml">
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/name: scenario
    app.kubernetes.io/managed-by: kustomize
  name: leader-election-rolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: leader-election-role
subjects:
- kind: ServiceAccount
  name: controller-manager
  namespace: system
</file>

<file path="kube-scheduler-simulator/scenario/config/rbac/leader_election_role.yaml">
# permissions to do leader election.
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/name: scenario
    app.kubernetes.io/managed-by: kustomize
  name: leader-election-role
rules:
- apiGroups:
  - ""
  resources:
  - configmaps
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - patch
  - delete
- apiGroups:
  - coordination.k8s.io
  resources:
  - leases
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - patch
  - delete
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - create
  - patch
</file>

<file path="kube-scheduler-simulator/scenario/config/rbac/metrics_auth_role_binding.yaml">
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: metrics-auth-rolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: metrics-auth-role
subjects:
- kind: ServiceAccount
  name: controller-manager
  namespace: system
</file>

<file path="kube-scheduler-simulator/scenario/config/rbac/metrics_auth_role.yaml">
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: metrics-auth-role
rules:
- apiGroups:
  - authentication.k8s.io
  resources:
  - tokenreviews
  verbs:
  - create
- apiGroups:
  - authorization.k8s.io
  resources:
  - subjectaccessreviews
  verbs:
  - create
</file>

<file path="kube-scheduler-simulator/scenario/config/rbac/metrics_reader_role.yaml">
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: metrics-reader
rules:
- nonResourceURLs:
  - "/metrics"
  verbs:
  - get
</file>

<file path="kube-scheduler-simulator/scenario/config/rbac/role_binding.yaml">
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/name: scenario
    app.kubernetes.io/managed-by: kustomize
  name: manager-rolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: manager-role
subjects:
- kind: ServiceAccount
  name: controller-manager
  namespace: system
</file>

<file path="kube-scheduler-simulator/scenario/config/rbac/role.yaml">
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: manager-role
rules:
- apiGroups:
  - simulation.kube-scheduler-simulator.x-k8s.io
  resources:
  - scenarios
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - simulation.kube-scheduler-simulator.x-k8s.io
  resources:
  - scenarios/finalizers
  verbs:
  - update
- apiGroups:
  - simulation.kube-scheduler-simulator.x-k8s.io
  resources:
  - scenarios/status
  verbs:
  - get
  - patch
  - update
</file>

<file path="kube-scheduler-simulator/scenario/config/rbac/scenario_admin_role.yaml">
# This rule is not used by the project scenario itself.
# It is provided to allow the cluster admin to help manage permissions for users.
#
# Grants full permissions ('*') over simulation.kube-scheduler-simulator.x-k8s.io.
# This role is intended for users authorized to modify roles and bindings within the cluster,
# enabling them to delegate specific permissions to other users or groups as needed.

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/name: scenario
    app.kubernetes.io/managed-by: kustomize
  name: scenario-admin-role
rules:
- apiGroups:
  - simulation.kube-scheduler-simulator.x-k8s.io
  resources:
  - scenarios
  verbs:
  - '*'
- apiGroups:
  - simulation.kube-scheduler-simulator.x-k8s.io
  resources:
  - scenarios/status
  verbs:
  - get
</file>

<file path="kube-scheduler-simulator/scenario/config/rbac/scenario_editor_role.yaml">
# This rule is not used by the project scenario itself.
# It is provided to allow the cluster admin to help manage permissions for users.
#
# Grants permissions to create, update, and delete resources within the simulation.kube-scheduler-simulator.x-k8s.io.
# This role is intended for users who need to manage these resources
# but should not control RBAC or manage permissions for others.

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/name: scenario
    app.kubernetes.io/managed-by: kustomize
  name: scenario-editor-role
rules:
- apiGroups:
  - simulation.kube-scheduler-simulator.x-k8s.io
  resources:
  - scenarios
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - simulation.kube-scheduler-simulator.x-k8s.io
  resources:
  - scenarios/status
  verbs:
  - get
</file>

<file path="kube-scheduler-simulator/scenario/config/rbac/scenario_viewer_role.yaml">
# This rule is not used by the project scenario itself.
# It is provided to allow the cluster admin to help manage permissions for users.
#
# Grants read-only access to simulation.kube-scheduler-simulator.x-k8s.io resources.
# This role is intended for users who need visibility into these resources
# without permissions to modify them. It is ideal for monitoring purposes and limited-access viewing.

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/name: scenario
    app.kubernetes.io/managed-by: kustomize
  name: scenario-viewer-role
rules:
- apiGroups:
  - simulation.kube-scheduler-simulator.x-k8s.io
  resources:
  - scenarios
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - simulation.kube-scheduler-simulator.x-k8s.io
  resources:
  - scenarios/status
  verbs:
  - get
</file>

<file path="kube-scheduler-simulator/scenario/config/rbac/service_account.yaml">
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/name: scenario
    app.kubernetes.io/managed-by: kustomize
  name: controller-manager
  namespace: system
</file>

<file path="kube-scheduler-simulator/scenario/config/samples/kustomization.yaml">
## Append samples of your project ##
resources:
- simulation_v1alpha1_scenario.yaml
# +kubebuilder:scaffold:manifestskustomizesamples
</file>

<file path="kube-scheduler-simulator/scenario/config/samples/simulation_v1alpha1_scenario.yaml">
apiVersion: simulation.kube-scheduler-simulator.x-k8s.io/v1alpha1
kind: Scenario
metadata:
  labels:
    app.kubernetes.io/name: scenario
    app.kubernetes.io/managed-by: kustomize
  name: scenario-sample
spec:
  # TODO(user): Add fields here
</file>

<file path="kube-scheduler-simulator/scenario/hack/boilerplate.go.txt">
/*
Copyright 2022.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/
</file>

<file path="kube-scheduler-simulator/scenario/internal/controller/scenario_controller_test.go">
/*
Copyright 2025.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package controller

import (
	"context"

	. "github.com/onsi/ginkgo/v2"
	. "github.com/onsi/gomega"
	"k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/types"
	"sigs.k8s.io/controller-runtime/pkg/reconcile"
	simulationv1alpha1 "sigs.k8s.io/kube-scheduler-simulator/scenario/api/v1alpha1"
)

var _ = Describe("Scenario Controller", func() {
	Context("When reconciling a resource", func() {
		const resourceName = "test-resource"

		ctx := context.Background()

		typeNamespacedName := types.NamespacedName{
			Name:      resourceName,
			Namespace: "default", // TODO(user):Modify as needed
		}
		scenario := &simulationv1alpha1.Scenario{}

		BeforeEach(func() {
			By("creating the custom resource for the Kind Scenario")
			err := k8sClient.Get(ctx, typeNamespacedName, scenario)
			if err != nil && errors.IsNotFound(err) {
				resource := &simulationv1alpha1.Scenario{
					ObjectMeta: metav1.ObjectMeta{
						Name:      resourceName,
						Namespace: "default",
					},
					// TODO(user): Specify other spec details if needed.
				}
				Expect(k8sClient.Create(ctx, resource)).To(Succeed())
			}
		})

		AfterEach(func() {
			// TODO(user): Cleanup logic after each test, like removing the resource instance.
			resource := &simulationv1alpha1.Scenario{}
			err := k8sClient.Get(ctx, typeNamespacedName, resource)
			Expect(err).NotTo(HaveOccurred())

			By("Cleanup the specific resource instance Scenario")
			Expect(k8sClient.Delete(ctx, resource)).To(Succeed())
		})
		It("should successfully reconcile the resource", func() {
			By("Reconciling the created resource")
			controllerReconciler := &ScenarioReconciler{
				Client: k8sClient,
				Scheme: k8sClient.Scheme(),
			}

			_, err := controllerReconciler.Reconcile(ctx, reconcile.Request{
				NamespacedName: typeNamespacedName,
			})
			Expect(err).NotTo(HaveOccurred())
			// TODO(user): Add more specific assertions depending on your controller's reconciliation logic.
			// Example: If you expect a certain status condition after reconciliation, verify it here.
		})
	})
})
</file>

<file path="kube-scheduler-simulator/scenario/internal/controller/scenario_controller.go">
/*
Copyright 2022.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package controller

import (
	"context"

	"k8s.io/apimachinery/pkg/runtime"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/log"
	simulationv1alpha1 "sigs.k8s.io/kube-scheduler-simulator/scenario/api/v1alpha1"
)

// ScenarioReconciler reconciles a Scenario object.
type ScenarioReconciler struct {
	client.Client
	Scheme *runtime.Scheme
}

// +kubebuilder:rbac:groups=simulation.kube-scheduler-simulator.x-k8s.io,resources=scenarios,verbs=get;list;watch;create;update;patch;delete
// +kubebuilder:rbac:groups=simulation.kube-scheduler-simulator.x-k8s.io,resources=scenarios/status,verbs=get;update;patch
// +kubebuilder:rbac:groups=simulation.kube-scheduler-simulator.x-k8s.io,resources=scenarios/finalizers,verbs=update

// Reconcile is part of the main kubernetes reconciliation loop which aims to
// move the current state of the cluster closer to the desired state.
// TODO(user): Modify the Reconcile function to compare the state specified by
// the Scenario object against the actual cluster state, and then
// perform operations to make the cluster state reflect the state specified by
// the user.
//
// For more details, check Reconcile and its Result here:
// - https://pkg.go.dev/sigs.k8s.io/controller-runtime@v0.20.0/pkg/reconcile
func (r *ScenarioReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
	_ = log.FromContext(ctx)

	// TODO(user): your logic here

	return ctrl.Result{}, nil
}

// SetupWithManager sets up the controller with the Manager.
func (r *ScenarioReconciler) SetupWithManager(mgr ctrl.Manager) error {
	return ctrl.NewControllerManagedBy(mgr).
		For(&simulationv1alpha1.Scenario{}).
		Named("scenario").
		Complete(r)
}
</file>

<file path="kube-scheduler-simulator/scenario/internal/controller/suite_test.go">
/*
Copyright 2022.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package controller

import (
	"context"
	"os"
	"path/filepath"
	"testing"

	. "github.com/onsi/ginkgo/v2"
	. "github.com/onsi/gomega"
	"k8s.io/client-go/kubernetes/scheme"
	"k8s.io/client-go/rest"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/envtest"
	logf "sigs.k8s.io/controller-runtime/pkg/log"
	"sigs.k8s.io/controller-runtime/pkg/log/zap"
	simulationv1alpha1 "sigs.k8s.io/kube-scheduler-simulator/scenario/api/v1alpha1"
)

// These tests use Ginkgo (BDD-style Go testing framework). Refer to
// http://onsi.github.io/ginkgo/ to learn more about Ginkgo.

var (
	ctx       context.Context
	cancel    context.CancelFunc
	testEnv   *envtest.Environment
	cfg       *rest.Config
	k8sClient client.Client
)

func TestControllers(t *testing.T) {
	RegisterFailHandler(Fail)

	RunSpecs(t, "Controller Suite")
}

var _ = BeforeSuite(func() {
	logf.SetLogger(zap.New(zap.WriteTo(GinkgoWriter), zap.UseDevMode(true)))

	ctx, cancel = context.WithCancel(context.TODO())

	var err error
	err = simulationv1alpha1.AddToScheme(scheme.Scheme)
	Expect(err).NotTo(HaveOccurred())

	// +kubebuilder:scaffold:scheme

	By("bootstrapping test environment")
	testEnv = &envtest.Environment{
		CRDDirectoryPaths:     []string{filepath.Join("..", "..", "config", "crd", "bases")},
		ErrorIfCRDPathMissing: true,
	}

	// Retrieve the first found binary directory to allow running tests from IDEs
	if getFirstFoundEnvTestBinaryDir() != "" {
		testEnv.BinaryAssetsDirectory = getFirstFoundEnvTestBinaryDir()
	}

	// cfg is defined in this file globally.
	cfg, err = testEnv.Start()
	Expect(err).NotTo(HaveOccurred())
	Expect(cfg).NotTo(BeNil())

	k8sClient, err = client.New(cfg, client.Options{Scheme: scheme.Scheme})
	Expect(err).NotTo(HaveOccurred())
	Expect(k8sClient).NotTo(BeNil())
})

var _ = AfterSuite(func() {
	By("tearing down the test environment")
	cancel()
	err := testEnv.Stop()
	Expect(err).NotTo(HaveOccurred())
})

// getFirstFoundEnvTestBinaryDir locates the first binary in the specified path.
// ENVTEST-based tests depend on specific binaries, usually located in paths set by
// controller-runtime. When running tests directly (e.g., via an IDE) without using
// Makefile targets, the 'BinaryAssetsDirectory' must be explicitly configured.
//
// This function streamlines the process by finding the required binaries, similar to
// setting the 'KUBEBUILDER_ASSETS' environment variable. To ensure the binaries are
// properly set up, run 'make setup-envtest' beforehand.
func getFirstFoundEnvTestBinaryDir() string {
	basePath := filepath.Join("..", "..", "bin", "k8s")
	entries, err := os.ReadDir(basePath)
	if err != nil {
		logf.Log.Error(err, "Failed to read directory", "path", basePath)
		return ""
	}
	for _, entry := range entries {
		if entry.IsDir() {
			return filepath.Join(basePath, entry.Name())
		}
	}
	return ""
}
</file>

<file path="kube-scheduler-simulator/scenario/test/e2e/e2e_suite_test.go">
/*
Copyright 2025.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package e2e

import (
	"fmt"
	"os"
	"os/exec"
	"testing"

	. "github.com/onsi/ginkgo/v2"
	. "github.com/onsi/gomega"
	"sigs.k8s.io/kube-scheduler-simulator/scenario/test/utils"
)

var (
	// Optional Environment Variables:
	// - PROMETHEUS_INSTALL_SKIP=true: Skips Prometheus Operator installation during test setup.
	// - CERT_MANAGER_INSTALL_SKIP=true: Skips CertManager installation during test setup.
	// These variables are useful if Prometheus or CertManager is already installed, avoiding
	// re-installation and conflicts.
	skipPrometheusInstall  = os.Getenv("PROMETHEUS_INSTALL_SKIP") == "true"
	skipCertManagerInstall = os.Getenv("CERT_MANAGER_INSTALL_SKIP") == "true"
	// isPrometheusOperatorAlreadyInstalled will be set true when prometheus CRDs be found on the cluster.
	isPrometheusOperatorAlreadyInstalled = false
	// isCertManagerAlreadyInstalled will be set true when CertManager CRDs be found on the cluster.
	isCertManagerAlreadyInstalled = false

	// projectImage is the name of the image which will be build and loaded
	// with the code source changes to be tested.
	projectImage = "example.com/scenario:v0.0.1"
)

// TestE2E runs the end-to-end (e2e) test suite for the project. These tests execute in an isolated,
// temporary environment to validate project changes with the the purposed to be used in CI jobs.
// The default setup requires Kind, builds/loads the Manager Docker image locally, and installs
// CertManager and Prometheus.
func TestE2E(t *testing.T) {
	RegisterFailHandler(Fail)
	_, _ = fmt.Fprintf(GinkgoWriter, "Starting scenario integration test suite\n")
	RunSpecs(t, "e2e suite")
}

var _ = BeforeSuite(func() {
	By("Ensure that Prometheus is enabled")
	_ = utils.UncommentCode("config/default/kustomization.yaml", "#- ../prometheus", "#")

	By("building the manager(Operator) image")
	cmd := exec.Command("make", "docker-build", fmt.Sprintf("IMG=%s", projectImage))
	_, err := utils.Run(cmd)
	ExpectWithOffset(1, err).NotTo(HaveOccurred(), "Failed to build the manager(Operator) image")

	// TODO(user): If you want to change the e2e test vendor from Kind, ensure the image is
	// built and available before running the tests. Also, remove the following block.
	By("loading the manager(Operator) image on Kind")
	err = utils.LoadImageToKindClusterWithName(projectImage)
	ExpectWithOffset(1, err).NotTo(HaveOccurred(), "Failed to load the manager(Operator) image into Kind")

	// The tests-e2e are intended to run on a temporary cluster that is created and destroyed for testing.
	// To prevent errors when tests run in environments with Prometheus or CertManager already installed,
	// we check for their presence before execution.
	// Setup Prometheus and CertManager before the suite if not skipped and if not already installed
	if !skipPrometheusInstall {
		By("checking if prometheus is installed already")
		isPrometheusOperatorAlreadyInstalled = utils.IsPrometheusCRDsInstalled()
		if !isPrometheusOperatorAlreadyInstalled {
			_, _ = fmt.Fprintf(GinkgoWriter, "Installing Prometheus Operator...\n")
			Expect(utils.InstallPrometheusOperator()).To(Succeed(), "Failed to install Prometheus Operator")
		} else {
			_, _ = fmt.Fprintf(GinkgoWriter, "WARNING: Prometheus Operator is already installed. Skipping installation...\n")
		}
	}
	if !skipCertManagerInstall {
		By("checking if cert manager is installed already")
		isCertManagerAlreadyInstalled = utils.IsCertManagerCRDsInstalled()
		if !isCertManagerAlreadyInstalled {
			_, _ = fmt.Fprintf(GinkgoWriter, "Installing CertManager...\n")
			Expect(utils.InstallCertManager()).To(Succeed(), "Failed to install CertManager")
		} else {
			_, _ = fmt.Fprintf(GinkgoWriter, "WARNING: CertManager is already installed. Skipping installation...\n")
		}
	}
})

var _ = AfterSuite(func() {
	// Teardown Prometheus and CertManager after the suite if not skipped and if they were not already installed
	if !skipPrometheusInstall && !isPrometheusOperatorAlreadyInstalled {
		_, _ = fmt.Fprintf(GinkgoWriter, "Uninstalling Prometheus Operator...\n")
		utils.UninstallPrometheusOperator()
	}
	if !skipCertManagerInstall && !isCertManagerAlreadyInstalled {
		_, _ = fmt.Fprintf(GinkgoWriter, "Uninstalling CertManager...\n")
		utils.UninstallCertManager()
	}
})
</file>

<file path="kube-scheduler-simulator/scenario/test/e2e/e2e_test.go">
/*
Copyright 2025.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package e2e

import (
	"encoding/json"
	"fmt"
	"os"
	"os/exec"
	"path/filepath"
	"time"

	. "github.com/onsi/ginkgo/v2"
	. "github.com/onsi/gomega"
	"sigs.k8s.io/kube-scheduler-simulator/scenario/test/utils"
)

// namespace where the project is deployed in.
const namespace = "scenario-system"

// serviceAccountName created for the project.
const serviceAccountName = "scenario-controller-manager"

// metricsServiceName is the name of the metrics service of the project.
const metricsServiceName = "scenario-controller-manager-metrics-service"

// metricsRoleBindingName is the name of the RBAC that will be created to allow get the metrics data.
const metricsRoleBindingName = "scenario-metrics-binding"

var _ = Describe("Manager", Ordered, func() {
	var controllerPodName string

	// Before running the tests, set up the environment by creating the namespace,
	// enforce the restricted security policy to the namespace, installing CRDs,
	// and deploying the controller.
	BeforeAll(func() {
		By("creating manager namespace")
		cmd := exec.Command("kubectl", "create", "ns", namespace)
		_, err := utils.Run(cmd)
		Expect(err).NotTo(HaveOccurred(), "Failed to create namespace")

		By("labeling the namespace to enforce the restricted security policy")
		cmd = exec.Command("kubectl", "label", "--overwrite", "ns", namespace,
			"pod-security.kubernetes.io/enforce=restricted")
		_, err = utils.Run(cmd)
		Expect(err).NotTo(HaveOccurred(), "Failed to label namespace with restricted policy")

		By("installing CRDs")
		cmd = exec.Command("make", "install")
		_, err = utils.Run(cmd)
		Expect(err).NotTo(HaveOccurred(), "Failed to install CRDs")

		By("deploying the controller-manager")
		cmd = exec.Command("make", "deploy", fmt.Sprintf("IMG=%s", projectImage))
		_, err = utils.Run(cmd)
		Expect(err).NotTo(HaveOccurred(), "Failed to deploy the controller-manager")
	})

	// After all tests have been executed, clean up by undeploying the controller, uninstalling CRDs,
	// and deleting the namespace.
	AfterAll(func() {
		By("cleaning up the curl pod for metrics")
		cmd := exec.Command("kubectl", "delete", "pod", "curl-metrics", "-n", namespace)
		_, _ = utils.Run(cmd)

		By("undeploying the controller-manager")
		cmd = exec.Command("make", "undeploy")
		_, _ = utils.Run(cmd)

		By("uninstalling CRDs")
		cmd = exec.Command("make", "uninstall")
		_, _ = utils.Run(cmd)

		By("removing manager namespace")
		cmd = exec.Command("kubectl", "delete", "ns", namespace)
		_, _ = utils.Run(cmd)
	})

	// After each test, check for failures and collect logs, events,
	// and pod descriptions for debugging.
	AfterEach(func() {
		specReport := CurrentSpecReport()
		if specReport.Failed() {
			By("Fetching controller manager pod logs")
			cmd := exec.Command("kubectl", "logs", controllerPodName, "-n", namespace)
			controllerLogs, err := utils.Run(cmd)
			if err == nil {
				_, _ = fmt.Fprintf(GinkgoWriter, "Controller logs:\n %s", controllerLogs)
			} else {
				_, _ = fmt.Fprintf(GinkgoWriter, "Failed to get Controller logs: %s", err)
			}

			By("Fetching Kubernetes events")
			cmd = exec.Command("kubectl", "get", "events", "-n", namespace, "--sort-by=.lastTimestamp")
			eventsOutput, err := utils.Run(cmd)
			if err == nil {
				_, _ = fmt.Fprintf(GinkgoWriter, "Kubernetes events:\n%s", eventsOutput)
			} else {
				_, _ = fmt.Fprintf(GinkgoWriter, "Failed to get Kubernetes events: %s", err)
			}

			By("Fetching curl-metrics logs")
			cmd = exec.Command("kubectl", "logs", "curl-metrics", "-n", namespace)
			metricsOutput, err := utils.Run(cmd)
			if err == nil {
				_, _ = fmt.Fprintf(GinkgoWriter, "Metrics logs:\n %s", metricsOutput)
			} else {
				_, _ = fmt.Fprintf(GinkgoWriter, "Failed to get curl-metrics logs: %s", err)
			}

			By("Fetching controller manager pod description")
			cmd = exec.Command("kubectl", "describe", "pod", controllerPodName, "-n", namespace)
			podDescription, err := utils.Run(cmd)
			if err == nil {
				fmt.Println("Pod description:\n", podDescription)
			} else {
				fmt.Println("Failed to describe controller pod")
			}
		}
	})

	SetDefaultEventuallyTimeout(2 * time.Minute)
	SetDefaultEventuallyPollingInterval(time.Second)

	Context("Manager", func() {
		It("should run successfully", func() {
			By("validating that the controller-manager pod is running as expected")
			verifyControllerUp := func(g Gomega) {
				// Get the name of the controller-manager pod
				cmd := exec.Command("kubectl", "get",
					"pods", "-l", "control-plane=controller-manager",
					"-o", "go-template={{ range .items }}"+
						"{{ if not .metadata.deletionTimestamp }}"+
						"{{ .metadata.name }}"+
						"{{ \"\\n\" }}{{ end }}{{ end }}",
					"-n", namespace,
				)

				podOutput, err := utils.Run(cmd)
				g.Expect(err).NotTo(HaveOccurred(), "Failed to retrieve controller-manager pod information")
				podNames := utils.GetNonEmptyLines(podOutput)
				g.Expect(podNames).To(HaveLen(1), "expected 1 controller pod running")
				controllerPodName = podNames[0]
				g.Expect(controllerPodName).To(ContainSubstring("controller-manager"))

				// Validate the pod's status
				cmd = exec.Command("kubectl", "get",
					"pods", controllerPodName, "-o", "jsonpath={.status.phase}",
					"-n", namespace,
				)
				output, err := utils.Run(cmd)
				g.Expect(err).NotTo(HaveOccurred())
				g.Expect(output).To(Equal("Running"), "Incorrect controller-manager pod status")
			}
			Eventually(verifyControllerUp).Should(Succeed())
		})

		It("should ensure the metrics endpoint is serving metrics", func() {
			By("creating a ClusterRoleBinding for the service account to allow access to metrics")
			cmd := exec.Command("kubectl", "create", "clusterrolebinding", metricsRoleBindingName,
				"--clusterrole=scenario-metrics-reader",
				fmt.Sprintf("--serviceaccount=%s:%s", namespace, serviceAccountName),
			)
			_, err := utils.Run(cmd)
			Expect(err).NotTo(HaveOccurred(), "Failed to create ClusterRoleBinding")

			By("validating that the metrics service is available")
			cmd = exec.Command("kubectl", "get", "service", metricsServiceName, "-n", namespace)
			_, err = utils.Run(cmd)
			Expect(err).NotTo(HaveOccurred(), "Metrics service should exist")

			By("validating that the ServiceMonitor for Prometheus is applied in the namespace")
			cmd = exec.Command("kubectl", "get", "ServiceMonitor", "-n", namespace)
			_, err = utils.Run(cmd)
			Expect(err).NotTo(HaveOccurred(), "ServiceMonitor should exist")

			By("getting the service account token")
			token, err := serviceAccountToken()
			Expect(err).NotTo(HaveOccurred())
			Expect(token).NotTo(BeEmpty())

			By("waiting for the metrics endpoint to be ready")
			verifyMetricsEndpointReady := func(g Gomega) {
				cmd := exec.Command("kubectl", "get", "endpoints", metricsServiceName, "-n", namespace)
				output, err := utils.Run(cmd)
				g.Expect(err).NotTo(HaveOccurred())
				g.Expect(output).To(ContainSubstring("8443"), "Metrics endpoint is not ready")
			}
			Eventually(verifyMetricsEndpointReady).Should(Succeed())

			By("verifying that the controller manager is serving the metrics server")
			verifyMetricsServerStarted := func(g Gomega) {
				cmd := exec.Command("kubectl", "logs", controllerPodName, "-n", namespace)
				output, err := utils.Run(cmd)
				g.Expect(err).NotTo(HaveOccurred())
				g.Expect(output).To(ContainSubstring("controller-runtime.metrics\tServing metrics server"),
					"Metrics server not yet started")
			}
			Eventually(verifyMetricsServerStarted).Should(Succeed())

			By("creating the curl-metrics pod to access the metrics endpoint")
			cmd = exec.Command("kubectl", "run", "curl-metrics", "--restart=Never",
				"--namespace", namespace,
				"--image=curlimages/curl:latest",
				"--overrides",
				fmt.Sprintf(`{
					"spec": {
						"containers": [{
							"name": "curl",
							"image": "curlimages/curl:latest",
							"command": ["/bin/sh", "-c"],
							"args": ["curl -v -k -H 'Authorization: Bearer %s' https://%s.%s.svc.cluster.local:8443/metrics"],
							"securityContext": {
								"allowPrivilegeEscalation": false,
								"capabilities": {
									"drop": ["ALL"]
								},
								"runAsNonRoot": true,
								"runAsUser": 1000,
								"seccompProfile": {
									"type": "RuntimeDefault"
								}
							}
						}],
						"serviceAccount": "%s"
					}
				}`, token, metricsServiceName, namespace, serviceAccountName))
			_, err = utils.Run(cmd)
			Expect(err).NotTo(HaveOccurred(), "Failed to create curl-metrics pod")

			By("waiting for the curl-metrics pod to complete.")
			verifyCurlUp := func(g Gomega) {
				cmd := exec.Command("kubectl", "get", "pods", "curl-metrics",
					"-o", "jsonpath={.status.phase}",
					"-n", namespace)
				output, err := utils.Run(cmd)
				g.Expect(err).NotTo(HaveOccurred())
				g.Expect(output).To(Equal("Succeeded"), "curl pod in wrong status")
			}
			Eventually(verifyCurlUp, 5*time.Minute).Should(Succeed())

			By("getting the metrics by checking curl-metrics logs")
			metricsOutput := getMetricsOutput()
			Expect(metricsOutput).To(ContainSubstring(
				"controller_runtime_reconcile_total",
			))
		})

		// +kubebuilder:scaffold:e2e-webhooks-checks

		// TODO: Customize the e2e test suite with scenarios specific to your project.
		// Consider applying sample/CR(s) and check their status and/or verifying
		// the reconciliation by using the metrics, i.e.:
		// metricsOutput := getMetricsOutput()
		// Expect(metricsOutput).To(ContainSubstring(
		//    fmt.Sprintf(`controller_runtime_reconcile_total{controller="%s",result="success"} 1`,
		//    strings.ToLower(<Kind>),
		// ))
	})
})

// serviceAccountToken returns a token for the specified service account in the given namespace.
// It uses the Kubernetes TokenRequest API to generate a token by directly sending a request
// and parsing the resulting token from the API response.
func serviceAccountToken() (string, error) {
	const tokenRequestRawString = `{
		"apiVersion": "authentication.k8s.io/v1",
		"kind": "TokenRequest"
	}`

	// Temporary file to store the token request
	secretName := fmt.Sprintf("%s-token-request", serviceAccountName)
	tokenRequestFile := filepath.Join("/tmp", secretName)
	err := os.WriteFile(tokenRequestFile, []byte(tokenRequestRawString), os.FileMode(0o644))
	if err != nil {
		return "", err
	}

	var out string
	verifyTokenCreation := func(g Gomega) {
		// Execute kubectl command to create the token
		cmd := exec.Command("kubectl", "create", "--raw", fmt.Sprintf(
			"/api/v1/namespaces/%s/serviceaccounts/%s/token",
			namespace,
			serviceAccountName,
		), "-f", tokenRequestFile)

		output, err := cmd.CombinedOutput()
		g.Expect(err).NotTo(HaveOccurred())

		// Parse the JSON output to extract the token
		var token tokenRequest
		err = json.Unmarshal(output, &token)
		g.Expect(err).NotTo(HaveOccurred())

		out = token.Status.Token
	}
	Eventually(verifyTokenCreation).Should(Succeed())

	return out, err
}

// getMetricsOutput retrieves and returns the logs from the curl pod used to access the metrics endpoint.
func getMetricsOutput() string {
	By("getting the curl-metrics logs")
	cmd := exec.Command("kubectl", "logs", "curl-metrics", "-n", namespace)
	metricsOutput, err := utils.Run(cmd)
	Expect(err).NotTo(HaveOccurred(), "Failed to retrieve logs from curl pod")
	Expect(metricsOutput).To(ContainSubstring("< HTTP/1.1 200 OK"))
	return metricsOutput
}

// tokenRequest is a simplified representation of the Kubernetes TokenRequest API response,
// containing only the token field that we need to extract.
type tokenRequest struct {
	Status struct {
		Token string `json:"token"`
	} `json:"status"`
}
</file>

<file path="kube-scheduler-simulator/scenario/test/utils/utils.go">
/*
Copyright 2025.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package utils

import (
	"bufio"
	"bytes"
	"fmt"
	"os"
	"os/exec"
	"strings"

	. "github.com/onsi/ginkgo/v2" //nolint:golint,revive
)

const (
	prometheusOperatorVersion = "v0.77.1"
	prometheusOperatorURL     = "https://github.com/prometheus-operator/prometheus-operator/" +
		"releases/download/%s/bundle.yaml"

	certmanagerVersion = "v1.16.3"
	certmanagerURLTmpl = "https://github.com/cert-manager/cert-manager/releases/download/%s/cert-manager.yaml"
)

func warnError(err error) {
	_, _ = fmt.Fprintf(GinkgoWriter, "warning: %v\n", err)
}

// Run executes the provided command within this context.
func Run(cmd *exec.Cmd) (string, error) {
	dir, _ := GetProjectDir()
	cmd.Dir = dir

	if err := os.Chdir(cmd.Dir); err != nil {
		_, _ = fmt.Fprintf(GinkgoWriter, "chdir dir: %s\n", err)
	}

	cmd.Env = append(os.Environ(), "GO111MODULE=on")
	command := strings.Join(cmd.Args, " ")
	_, _ = fmt.Fprintf(GinkgoWriter, "running: %s\n", command)
	output, err := cmd.CombinedOutput()
	if err != nil {
		return string(output), fmt.Errorf("%s failed with error: (%w) %s", command, err, string(output))
	}

	return string(output), nil
}

// InstallPrometheusOperator installs the prometheus Operator to be used to export the enabled metrics.
func InstallPrometheusOperator() error {
	url := fmt.Sprintf(prometheusOperatorURL, prometheusOperatorVersion)
	cmd := exec.Command("kubectl", "create", "-f", url)
	_, err := Run(cmd)
	return err
}

// UninstallPrometheusOperator uninstalls the prometheus.
func UninstallPrometheusOperator() {
	url := fmt.Sprintf(prometheusOperatorURL, prometheusOperatorVersion)
	cmd := exec.Command("kubectl", "delete", "-f", url)
	if _, err := Run(cmd); err != nil {
		warnError(err)
	}
}

// IsPrometheusCRDsInstalled checks if any Prometheus CRDs are installed
// by verifying the existence of key CRDs related to Prometheus.
func IsPrometheusCRDsInstalled() bool {
	// List of common Prometheus CRDs
	prometheusCRDs := []string{
		"prometheuses.monitoring.coreos.com",
		"prometheusrules.monitoring.coreos.com",
		"prometheusagents.monitoring.coreos.com",
	}

	cmd := exec.Command("kubectl", "get", "crds", "-o", "custom-columns=NAME:.metadata.name")
	output, err := Run(cmd)
	if err != nil {
		return false
	}
	crdList := GetNonEmptyLines(output)
	for _, crd := range prometheusCRDs {
		for _, line := range crdList {
			if strings.Contains(line, crd) {
				return true
			}
		}
	}

	return false
}

// UninstallCertManager uninstalls the cert manager.
func UninstallCertManager() {
	url := fmt.Sprintf(certmanagerURLTmpl, certmanagerVersion)
	cmd := exec.Command("kubectl", "delete", "-f", url)
	if _, err := Run(cmd); err != nil {
		warnError(err)
	}
}

// InstallCertManager installs the cert manager bundle.
func InstallCertManager() error {
	url := fmt.Sprintf(certmanagerURLTmpl, certmanagerVersion)
	cmd := exec.Command("kubectl", "apply", "-f", url)
	if _, err := Run(cmd); err != nil {
		return err
	}
	// Wait for cert-manager-webhook to be ready, which can take time if cert-manager
	// was re-installed after uninstalling on a cluster.
	cmd = exec.Command("kubectl", "wait", "deployment.apps/cert-manager-webhook",
		"--for", "condition=Available",
		"--namespace", "cert-manager",
		"--timeout", "5m",
	)

	_, err := Run(cmd)
	return err
}

// IsCertManagerCRDsInstalled checks if any Cert Manager CRDs are installed
// by verifying the existence of key CRDs related to Cert Manager.
func IsCertManagerCRDsInstalled() bool {
	// List of common Cert Manager CRDs
	certManagerCRDs := []string{
		"certificates.cert-manager.io",
		"issuers.cert-manager.io",
		"clusterissuers.cert-manager.io",
		"certificaterequests.cert-manager.io",
		"orders.acme.cert-manager.io",
		"challenges.acme.cert-manager.io",
	}

	// Execute the kubectl command to get all CRDs
	cmd := exec.Command("kubectl", "get", "crds")
	output, err := Run(cmd)
	if err != nil {
		return false
	}

	// Check if any of the Cert Manager CRDs are present
	crdList := GetNonEmptyLines(output)
	for _, crd := range certManagerCRDs {
		for _, line := range crdList {
			if strings.Contains(line, crd) {
				return true
			}
		}
	}

	return false
}

// LoadImageToKindClusterWithName loads a local docker image to the kind cluster.
func LoadImageToKindClusterWithName(name string) error {
	cluster := "kind"
	if v, ok := os.LookupEnv("KIND_CLUSTER"); ok {
		cluster = v
	}
	kindOptions := []string{"load", "docker-image", name, "--name", cluster}
	cmd := exec.Command("kind", kindOptions...)
	_, err := Run(cmd)
	return err
}

// GetNonEmptyLines converts given command output string into individual objects
// according to line breakers, and ignores the empty elements in it.
func GetNonEmptyLines(output string) []string {
	var res []string
	elements := strings.Split(output, "\n")
	for _, element := range elements {
		if element != "" {
			res = append(res, element)
		}
	}

	return res
}

// GetProjectDir will return the directory where the project is.
func GetProjectDir() (string, error) {
	wd, err := os.Getwd()
	if err != nil {
		return wd, err
	}
	wd = strings.Replace(wd, "/test/e2e", "", -1)
	return wd, nil
}

// UncommentCode searches for target in the file and remove the comment prefix
// of the target content. The target content may span multiple lines.
func UncommentCode(filename, target, prefix string) error {
	// false positive
	// nolint:gosec
	content, err := os.ReadFile(filename)
	if err != nil {
		return err
	}
	strContent := string(content)

	idx := strings.Index(strContent, target)
	if idx < 0 {
		return fmt.Errorf("unable to find the code %s to be uncomment", target)
	}

	out := new(bytes.Buffer)
	_, err = out.Write(content[:idx])
	if err != nil {
		return err
	}

	scanner := bufio.NewScanner(bytes.NewBufferString(target))
	if !scanner.Scan() {
		return nil
	}
	for {
		_, err := out.WriteString(strings.TrimPrefix(scanner.Text(), prefix))
		if err != nil {
			return err
		}
		// Avoid writing a newline in case the previous line was the last in target.
		if !scanner.Scan() {
			break
		}
		if _, err := out.WriteString("\n"); err != nil {
			return err
		}
	}

	_, err = out.Write(content[idx+len(target):])
	if err != nil {
		return err
	}
	// false positive
	// nolint:gosec
	return os.WriteFile(filename, out.Bytes(), 0o644)
}
</file>

<file path="kube-scheduler-simulator/scenario/.dockerignore">
# More info: https://docs.docker.com/engine/reference/builder/#dockerignore-file
# Ignore build and test binaries.
bin/
</file>

<file path="kube-scheduler-simulator/scenario/.gitignore">
# Binaries for programs and plugins
*.exe
*.exe~
*.dll
*.so
*.dylib
bin/*
Dockerfile.cross

# Test binary, built with `go test -c`
*.test

# Output of the go coverage tool, specifically when used with LiteIDE
*.out

# Go workspace file
go.work

# Kubernetes Generated files - skip generated files, except for vendored files
!vendor/**/zz_generated.*

# editor and IDE paraphernalia
.idea
.vscode
*.swp
*.swo
*~
</file>

<file path="kube-scheduler-simulator/scenario/.golangci.yml">
run:
  skip-dirs:
    - docs
linters-settings:
  # if you fix this gci config, you should modify the gci command on `make format` as well.
  gci:
    sections:
      - Standard
      - Default
      - Prefix(sigs.k8s.io/kube-scheduler-simulator/scenario)

  gocritic:
    disabled-checks:
      - appendAssign

  revive:
    rules:
      # default settings
      # from: https://github.com/mgechev/revive/blob/master/defaults.toml
      - name: blank-imports
      - name: context-as-argument
      - name: context-keys-type
      - name: dot-imports
      - name: error-return
      - name: error-strings
      - name: error-naming
      - name: exported
      - name: if-return
      - name: increment-decrement
      - name: var-naming
      - name: var-declaration
      - name: package-comments
      - name: range
      - name: receiver-naming
      - name: time-naming
      - name: unexported-return
      - name: indent-error-flow
      - name: errorf
      - name: empty-block
      - name: superfluous-else
      - name: unused-parameter
      - name: unreachable-code
      - name: redefines-builtin-id
      # additional settings
      - name: duplicated-imports

linters:
  disable-all: true
  enable:
    - unused
    - govet
    - staticcheck
    - typecheck
    - errcheck
    - gosimple
    - asciicheck
    - bodyclose
    - cyclop
    - dogsled
    - durationcheck
    - errorlint
    - exportloopref
    - forcetypeassert
    - funlen
    - gci
    - gochecknoinits
    - gocognit
    - gocritic
    - gocyclo
    - godot
    - goerr113
    - gofmt
    - gofumpt
    - gosec
    - makezero
    - nakedret
    - nestif
    - nilerr
    - nolintlint
    - paralleltest
    - predeclared
    - revive
    - rowserrcheck
    - sqlclosecheck
    - thelper
    - unconvert
    - unparam
    - wastedassign
    - prealloc

issues:
  exclude-rules:
    - path: _test\.go
      linters:
        - funlen
        - goerr113
        - errcheck
        - cyclop
</file>

<file path="kube-scheduler-simulator/scenario/Dockerfile">
# Build the manager binary
FROM docker.io/golang:1.23 AS builder
ARG TARGETOS
ARG TARGETARCH

WORKDIR /workspace
# Copy the Go Modules manifests
COPY go.mod go.mod
COPY go.sum go.sum
# cache deps before building and copying source so that we don't need to re-download as much
# and so that source changes don't invalidate our downloaded layer
RUN go mod download

# Copy the go source
COPY cmd/main.go cmd/main.go
COPY api/ api/
COPY internal/ internal/

# Build
# the GOARCH has not a default value to allow the binary be built according to the host where the command
# was called. For example, if we call make docker-build in a local env which has the Apple Silicon M1 SO
# the docker BUILDPLATFORM arg will be linux/arm64 when for Apple x86 it will be linux/amd64. Therefore,
# by leaving it empty we can ensure that the container and binary shipped on it will have the same platform.
RUN CGO_ENABLED=0 GOOS=${TARGETOS:-linux} GOARCH=${TARGETARCH} go build -a -o manager cmd/main.go

# Use distroless as minimal base image to package the manager binary
# Refer to https://github.com/GoogleContainerTools/distroless for more details
FROM gcr.io/distroless/static:nonroot
WORKDIR /
COPY --from=builder /workspace/manager .
USER 65532:65532

ENTRYPOINT ["/manager"]
</file>

<file path="kube-scheduler-simulator/scenario/Makefile">
# Image URL to use all building/pushing image targets
IMG ?= controller:latest

# Get the currently used golang install path (in GOPATH/bin, unless GOBIN is set)
ifeq (,$(shell go env GOBIN))
GOBIN=$(shell go env GOPATH)/bin
else
GOBIN=$(shell go env GOBIN)
endif

# CONTAINER_TOOL defines the container tool to be used for building images.
# Be aware that the target commands are only tested with Docker which is
# scaffolded by default. However, you might want to replace it to use other
# tools. (i.e. podman)
CONTAINER_TOOL ?= docker

# Setting SHELL to bash allows bash commands to be executed by recipes.
# Options are set to exit when a recipe line exits non-zero or a piped command fails.
SHELL = /usr/bin/env bash -o pipefail
.SHELLFLAGS = -ec

.PHONY: all
all: build

##@ General

# The help target prints out all targets with their descriptions organized
# beneath their categories. The categories are represented by '##@' and the
# target descriptions by '##'. The awk command is responsible for reading the
# entire set of makefiles included in this invocation, looking for lines of the
# file as xyz: ## something, and then pretty-format the target and help. Then,
# if there's a line with ##@ something, that gets pretty-printed as a category.
# More info on the usage of ANSI control characters for terminal formatting:
# https://en.wikipedia.org/wiki/ANSI_escape_code#SGR_parameters
# More info on the awk command:
# http://linuxcommand.org/lc3_adv_awk.php

.PHONY: help
help: ## Display this help.
	@awk 'BEGIN {FS = ":.*##"; printf "\nUsage:\n  make \033[36m<target>\033[0m\n"} /^[a-zA-Z_0-9-]+:.*?##/ { printf "  \033[36m%-15s\033[0m %s\n", $$1, $$2 } /^##@/ { printf "\n\033[1m%s\033[0m\n", substr($$0, 5) } ' $(MAKEFILE_LIST)

##@ Development

.PHONY: manifests
manifests: controller-gen ## Generate WebhookConfiguration, ClusterRole and CustomResourceDefinition objects.
	$(CONTROLLER_GEN) rbac:roleName=manager-role crd webhook paths="./..." output:crd:artifacts:config=config/crd/bases

.PHONY: generate
generate: controller-gen ## Generate code containing DeepCopy, DeepCopyInto, and DeepCopyObject method implementations.
	$(CONTROLLER_GEN) object:headerFile="hack/boilerplate.go.txt" paths="./..."

.PHONY: fmt
fmt: ## Run go fmt against code.
	go fmt ./...

.PHONY: vet
vet: ## Run go vet against code.
	go vet ./...

.PHONY: test
test: manifests generate fmt vet setup-envtest ## Run tests.
	KUBEBUILDER_ASSETS="$(shell $(ENVTEST) use $(ENVTEST_K8S_VERSION) --bin-dir $(LOCALBIN) -p path)" go test $$(go list ./... | grep -v /e2e) -coverprofile cover.out

# TODO(user): To use a different vendor for e2e tests, modify the setup under 'tests/e2e'.
# The default setup assumes Kind is pre-installed and builds/loads the Manager Docker image locally.
# Prometheus and CertManager are installed by default; skip with:
# - PROMETHEUS_INSTALL_SKIP=true
# - CERT_MANAGER_INSTALL_SKIP=true
.PHONY: test-e2e
test-e2e: manifests generate fmt vet ## Run the e2e tests. Expected an isolated environment using Kind.
	@command -v kind >/dev/null 2>&1 || { \
		echo "Kind is not installed. Please install Kind manually."; \
		exit 1; \
	}
	@kind get clusters | grep -q 'kind' || { \
		echo "No Kind cluster is running. Please start a Kind cluster before running the e2e tests."; \
		exit 1; \
	}
	go test ./test/e2e/ -v -ginkgo.v

.PHONY: lint
lint: golangci-lint ## Run golangci-lint linter
	$(GOLANGCI_LINT) run

.PHONY: lint-fix
lint-fix: golangci-lint ## Run golangci-lint linter and perform fixes
	$(GOLANGCI_LINT) run --fix

.PHONY: lint-config
lint-config: golangci-lint ## Verify golangci-lint linter configuration
	$(GOLANGCI_LINT) config verify

##@ Build

.PHONY: build
build: manifests generate fmt vet ## Build manager binary.
	go build -o bin/manager cmd/main.go

.PHONY: run
run: manifests generate fmt vet ## Run a controller from your host.
	go run ./cmd/main.go

# If you wish to build the manager image targeting other platforms you can use the --platform flag.
# (i.e. docker build --platform linux/arm64). However, you must enable docker buildKit for it.
# More info: https://docs.docker.com/develop/develop-images/build_enhancements/
.PHONY: docker-build
docker-build: ## Build docker image with the manager.
	$(CONTAINER_TOOL) build -t ${IMG} .

.PHONY: docker-push
docker-push: ## Push docker image with the manager.
	$(CONTAINER_TOOL) push ${IMG}

# PLATFORMS defines the target platforms for the manager image be built to provide support to multiple
# architectures. (i.e. make docker-buildx IMG=myregistry/mypoperator:0.0.1). To use this option you need to:
# - be able to use docker buildx. More info: https://docs.docker.com/build/buildx/
# - have enabled BuildKit. More info: https://docs.docker.com/develop/develop-images/build_enhancements/
# - be able to push the image to your registry (i.e. if you do not set a valid value via IMG=<myregistry/image:<tag>> then the export will fail)
# To adequately provide solutions that are compatible with multiple platforms, you should consider using this option.
PLATFORMS ?= linux/arm64,linux/amd64,linux/s390x,linux/ppc64le
.PHONY: docker-buildx
docker-buildx: ## Build and push docker image for the manager for cross-platform support
	# copy existing Dockerfile and insert --platform=${BUILDPLATFORM} into Dockerfile.cross, and preserve the original Dockerfile
	sed -e '1 s/\(^FROM\)/FROM --platform=\$$\{BUILDPLATFORM\}/; t' -e ' 1,// s//FROM --platform=\$$\{BUILDPLATFORM\}/' Dockerfile > Dockerfile.cross
	- $(CONTAINER_TOOL) buildx create --name scenario-builder
	$(CONTAINER_TOOL) buildx use scenario-builder
	- $(CONTAINER_TOOL) buildx build --push --platform=$(PLATFORMS) --tag ${IMG} -f Dockerfile.cross .
	- $(CONTAINER_TOOL) buildx rm scenario-builder
	rm Dockerfile.cross

.PHONY: build-installer
build-installer: manifests generate kustomize ## Generate a consolidated YAML with CRDs and deployment.
	mkdir -p dist
	cd config/manager && $(KUSTOMIZE) edit set image controller=${IMG}
	$(KUSTOMIZE) build config/default > dist/install.yaml

##@ Deployment

ifndef ignore-not-found
  ignore-not-found = false
endif

.PHONY: install
install: manifests kustomize ## Install CRDs into the K8s cluster specified in ~/.kube/config.
	$(KUSTOMIZE) build config/crd | $(KUBECTL) apply -f -

.PHONY: uninstall
uninstall: manifests kustomize ## Uninstall CRDs from the K8s cluster specified in ~/.kube/config. Call with ignore-not-found=true to ignore resource not found errors during deletion.
	$(KUSTOMIZE) build config/crd | $(KUBECTL) delete --ignore-not-found=$(ignore-not-found) -f -

.PHONY: deploy
deploy: manifests kustomize ## Deploy controller to the K8s cluster specified in ~/.kube/config.
	cd config/manager && $(KUSTOMIZE) edit set image controller=${IMG}
	$(KUSTOMIZE) build config/default | $(KUBECTL) apply -f -

.PHONY: undeploy
undeploy: kustomize ## Undeploy controller from the K8s cluster specified in ~/.kube/config. Call with ignore-not-found=true to ignore resource not found errors during deletion.
	$(KUSTOMIZE) build config/default | $(KUBECTL) delete --ignore-not-found=$(ignore-not-found) -f -

##@ Dependencies

## Location to install dependencies to
LOCALBIN ?= $(shell pwd)/bin
$(LOCALBIN):
	mkdir -p $(LOCALBIN)

## Tool Binaries
KUBECTL ?= kubectl
KUSTOMIZE ?= $(LOCALBIN)/kustomize
CONTROLLER_GEN ?= $(LOCALBIN)/controller-gen
ENVTEST ?= $(LOCALBIN)/setup-envtest
GOLANGCI_LINT = $(LOCALBIN)/golangci-lint

## Tool Versions
KUSTOMIZE_VERSION ?= v5.5.0
CONTROLLER_TOOLS_VERSION ?= v0.17.1
#ENVTEST_VERSION is the version of controller-runtime release branch to fetch the envtest setup script (i.e. release-0.20)
ENVTEST_VERSION ?= $(shell go list -m -f "{{ .Version }}" sigs.k8s.io/controller-runtime | awk -F'[v.]' '{printf "release-%d.%d", $$2, $$3}')
#ENVTEST_K8S_VERSION is the version of Kubernetes to use for setting up ENVTEST binaries (i.e. 1.31)
ENVTEST_K8S_VERSION ?= $(shell go list -m -f "{{ .Version }}" k8s.io/api | awk -F'[v.]' '{printf "1.%d", $$3}')
GOLANGCI_LINT_VERSION ?= v1.63.4

.PHONY: kustomize
kustomize: $(KUSTOMIZE) ## Download kustomize locally if necessary.
$(KUSTOMIZE): $(LOCALBIN)
	$(call go-install-tool,$(KUSTOMIZE),sigs.k8s.io/kustomize/kustomize/v5,$(KUSTOMIZE_VERSION))

.PHONY: controller-gen
controller-gen: $(CONTROLLER_GEN) ## Download controller-gen locally if necessary.
$(CONTROLLER_GEN): $(LOCALBIN)
	$(call go-install-tool,$(CONTROLLER_GEN),sigs.k8s.io/controller-tools/cmd/controller-gen,$(CONTROLLER_TOOLS_VERSION))

.PHONY: setup-envtest
setup-envtest: envtest ## Download the binaries required for ENVTEST in the local bin directory.
	@echo "Setting up envtest binaries for Kubernetes version $(ENVTEST_K8S_VERSION)..."
	@$(ENVTEST) use $(ENVTEST_K8S_VERSION) --bin-dir $(LOCALBIN) -p path || { \
		echo "Error: Failed to set up envtest binaries for version $(ENVTEST_K8S_VERSION)."; \
		exit 1; \
	}

.PHONY: envtest
envtest: $(ENVTEST) ## Download setup-envtest locally if necessary.
$(ENVTEST): $(LOCALBIN)
	$(call go-install-tool,$(ENVTEST),sigs.k8s.io/controller-runtime/tools/setup-envtest,$(ENVTEST_VERSION))

.PHONY: golangci-lint
golangci-lint: $(GOLANGCI_LINT) ## Download golangci-lint locally if necessary.
$(GOLANGCI_LINT): $(LOCALBIN)
	$(call go-install-tool,$(GOLANGCI_LINT),github.com/golangci/golangci-lint/cmd/golangci-lint,$(GOLANGCI_LINT_VERSION))

# go-install-tool will 'go install' any package with custom target and name of binary, if it doesn't exist
# $1 - target path with name of binary
# $2 - package url which can be installed
# $3 - specific version of package
define go-install-tool
@[ -f "$(1)-$(3)" ] || { \
set -e; \
package=$(2)@$(3) ;\
echo "Downloading $${package}" ;\
rm -f $(1) || true ;\
GOBIN=$(LOCALBIN) go install $${package} ;\
mv $(1) $(1)-$(3) ;\
} ;\
ln -sf $(1)-$(3) $(1)
endef
</file>

<file path="kube-scheduler-simulator/scenario/PROJECT">
# Code generated by tool. DO NOT EDIT.
# This file is used to track the info used to scaffold your project
# and allow the plugins properly work.
# More info: https://book.kubebuilder.io/reference/project-config.html
domain: kube-scheduler-simulator.x-k8s.io
layout:
- go.kubebuilder.io/v4
projectName: scenario
repo: sigs.k8s.io/kube-scheduler-simulator/scenario
resources:
- api:
    crdVersion: v1
    namespaced: true
  controller: true
  domain: kube-scheduler-simulator.x-k8s.io
  group: simulation
  kind: Scenario
  path: sigs.k8s.io/kube-scheduler-simulator/scenario/api/v1alpha1
  version: v1alpha1
version: "3"
</file>

<file path="kube-scheduler-simulator/scenario/README.md">
# scenario

**This Scenario is currently under development.** 

**All the following descriptions are subject to change as they are undecided.** 
**And, the implementations are subject to breaking changes without notice.**

This module contains the Scenario CRD and the controller for it.
The Scenario allows you to write scenario for scenario-based simulation of scheduler.

## Description

This Scenario and the controller are discussed and designed in [KEP-140: Scenario-based simulation](../keps/140-scenario-based-simulation).

It's designed mainly to run with simulator. 
The simulator won't create any actual resources, so you can run the simulation for your scheduler without any huge real resources.
You can see more details about simulator in [../simulator](../simulator).

The Scenario and the controller can be run in your cluster, but **we do not recommend to do it in your real cluster**.
(probably no problem on running with a fake cluster like minikube or kind)

By using it on your cluster, you can check your scheduler's behavior by directly using scheduler in your cluster.

**Several important notes for when you want to run it on your cluster** (these are why we don't recommend):
- The controller **removes all resources** in your cluster.
- The controller creates the actual resources in your cluster for simulation.
- The size of Scenario resources sometimes becomes big since it contains both defined scenario and the simulation result
  - the big resource may degrade the performance of etcd on cluster.

TODO: add the note about [the simulator operator](../keps/159-scheduler-simulator-operator) and [SchedulerSimulation](../keps/184-scheduler-simulation).

## Getting Started 

### with simulator

TODO: write it how to run it with simulator.

### Prerequisites
- go version v1.23.0+
- docker version 17.03+.
- kubectl version v1.11.3+.
- Access to a Kubernetes v1.11.3+ cluster.

### To Deploy on the cluster
**Build and push your image to the location specified by `IMG`:**

```sh
make docker-build docker-push IMG=<some-registry>/scenario:tag
```

**NOTE:** This image ought to be published in the personal registry you specified.
And it is required to have access to pull the image from the working environment.
Make sure you have the proper permission to the registry if the above commands don’t work.

**Install the CRDs into the cluster:**

```sh
make install
```

**Deploy the Manager to the cluster with the image specified by `IMG`:**

```sh
make deploy IMG=<some-registry>/scenario:tag
```

> **NOTE**: If you encounter RBAC errors, you may need to grant yourself cluster-admin
privileges or be logged in as admin.

**Create instances of your solution**
You can apply the samples (examples) from the config/sample:

```sh
kubectl apply -k config/samples/
```

>**NOTE**: Ensure that the samples has default values to test it out.

### To Uninstall
**Delete the instances (CRs) from the cluster:**

```sh
kubectl delete -k config/samples/
```

**Delete the APIs(CRDs) from the cluster:**

```sh
make uninstall
```

**UnDeploy the controller from the cluster:**

```sh
make undeploy
```

## Project Distribution

Following the options to release and provide this solution to the users.

### By providing a bundle with all YAML files

1. Build the installer for the image built and published in the registry:

```sh
make build-installer IMG=<some-registry>/scenario:tag
```

**NOTE:** The makefile target mentioned above generates an 'install.yaml'
file in the dist directory. This file contains all the resources built
with Kustomize, which are necessary to install this project without its
dependencies.

2. Using the installer

Users can just run 'kubectl apply -f <URL for YAML BUNDLE>' to install
the project, i.e.:

```sh
kubectl apply -f https://raw.githubusercontent.com/<org>/scenario/<tag or branch>/dist/install.yaml
```

### By providing a Helm Chart

1. Build the chart using the optional helm plugin

```sh
kubebuilder edit --plugins=helm/v1-alpha
```

2. See that a chart was generated under 'dist/chart', and users
can obtain this solution from there.

**NOTE:** If you change the project, you need to update the Helm Chart
using the same command above to sync the latest changes. Furthermore,
if you create webhooks, you need to use the above command with
the '--force' flag and manually ensure that any custom configuration
previously added to 'dist/chart/values.yaml' or 'dist/chart/manager/manager.yaml'
is manually re-applied afterwards.

## Contributing
// TODO(user): Add detailed information on how you would like others to contribute to this project

**NOTE:** Run `make help` for more information on all potential `make` targets

More information can be found via the [Kubebuilder Documentation](https://book.kubebuilder.io/introduction.html)
</file>

<file path="kube-scheduler-simulator/simulator/cmd/scheduler/Dockerfile">
FROM golang:1.22 AS build-env
ARG TARGETOS
ARG TARGETARCH

ENV GOOS=${TARGETOS:-linux}
ENV GOARCH=${TARGETARCH}
ENV CGO_ENABLED=0
ENV GO111MODULE=on

RUN go env -w GOCACHE=/go-cache
RUN go env -w GOMODCACHE=/gomod-cache

WORKDIR /go/src/simulator

COPY go.mod go.sum ./
RUN --mount=type=cache,target=/gomod-cache go mod download

COPY . .
RUN --mount=type=cache,target=/gomod-cache --mount=type=cache,target=/go-cache \
    go build -v -o ./bin/scheduler ./cmd/scheduler/scheduler.go

FROM alpine:3.14.0

COPY --from=build-env /go/src/simulator/bin/scheduler /scheduler
RUN chmod a+x /scheduler

CMD ["/scheduler", "--config", "/config/scheduler.yaml", "--master", "http://simulator-cluster:3131"]
</file>

<file path="kube-scheduler-simulator/simulator/cmd/scheduler/kubeconfig.yaml">
apiVersion: v1
kind: Config

clusters:
  - cluster:
      server: http://simulator-cluster:3131
    name: simulator

contexts:
  - context:
      cluster: simulator
    name: simulator

current-context: simulator
</file>

<file path="kube-scheduler-simulator/simulator/cmd/scheduler/scheduler.go">
package main

import (
	"fmt"
	"os"

	"k8s.io/component-base/cli"
	_ "k8s.io/component-base/logs/json/register" // for JSON log format registration
	_ "k8s.io/component-base/metrics/prometheus/clientgo"
	_ "k8s.io/component-base/metrics/prometheus/version" // for version metric registration
	"k8s.io/klog"

	"sigs.k8s.io/kube-scheduler-simulator/simulator/pkg/debuggablescheduler"
	// Import the DRL scheduler plugin
	DRL "sigs.k8s.io/kube-scheduler-simulator/simulator/docs/sample/drl"
)

func main() {
	command, cancelFn, err := debuggablescheduler.NewSchedulerCommand(
		debuggablescheduler.WithPlugin(DRL.Name, DRL.New),
	)
	if err != nil {
		klog.Info(fmt.Sprintf("failed to build the debuggablescheduler command: %+v", err))
		os.Exit(1)
	}
	code := cli.Run(command)

	cancelFn()
	os.Exit(code)
}
</file>

<file path="kube-scheduler-simulator/simulator/cmd/scheduler/scheduler.yaml">
kind: KubeSchedulerConfiguration
apiVersion: kubescheduler.config.k8s.io/v1
profiles:
  - schedulerName: default-scheduler
    plugins:
      multiPoint:
        enabled:
          - name: DRL
            weight: 20
</file>

<file path="kube-scheduler-simulator/simulator/cmd/simulator/Dockerfile">
FROM golang:1.22 AS build-env

ARG TARGETOS
ARG TARGETARCH

ENV GOOS=${TARGETOS:-linux}
ENV GOARCH=${TARGETARCH}
ENV CGO_ENABLED=0
ENV GO111MODULE=on

RUN go env -w GOCACHE=/go-cache
RUN go env -w GOMODCACHE=/gomod-cache

WORKDIR /go/src/simulator

COPY go.mod go.sum ./
RUN --mount=type=cache,target=/gomod-cache go mod download

COPY . .
RUN --mount=type=cache,target=/gomod-cache --mount=type=cache,target=/go-cache \
    go build -v -o ./bin/simulator ./cmd/simulator/simulator.go

FROM alpine:3.14.0

COPY --from=build-env /go/src/simulator/bin/simulator /simulator
RUN chmod a+x /simulator

EXPOSE 1212
CMD ["/simulator"]
</file>

<file path="kube-scheduler-simulator/simulator/cmd/simulator/simulator.go">
package main

import (
	"context"
	"os"
	"os/signal"
	"syscall"
	"time"

	clientv3 "go.etcd.io/etcd/client/v3"
	"golang.org/x/xerrors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/util/wait"
	"k8s.io/client-go/discovery"
	"k8s.io/client-go/discovery/cached/memory"
	"k8s.io/client-go/dynamic"
	clientset "k8s.io/client-go/kubernetes"
	"k8s.io/client-go/rest"
	"k8s.io/client-go/restmapper"
	"k8s.io/klog/v2"

	"sigs.k8s.io/kube-scheduler-simulator/simulator/config"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/resourceapplier"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/server"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/server/di"
)

const (
	kubeAPIServerPollInterval = 5 * time.Second
	kubeAPIServerReadyTimeout = 2 * time.Minute
	importTimeout             = 2 * time.Minute
)

// entry point.
func main() {
	if err := startSimulator(); err != nil {
		klog.Fatalf("failed with error on running simulator: %+v", err)
	}
}

// startSimulator starts simulator and needed k8s components.
//
//nolint:funlen,cyclop
func startSimulator() error {
	cfg, err := config.NewConfig()
	if err != nil {
		return xerrors.Errorf("get config: %w", err)
	}

	restCfg := &rest.Config{
		Host: cfg.KubeAPIServerURL,
	}
	client := clientset.NewForConfigOrDie(restCfg)
	dynamicClient := dynamic.NewForConfigOrDie(restCfg)
	discoverClient := discovery.NewDiscoveryClient(client.RESTClient())
	cachedDiscoveryClient := memory.NewMemCacheClient(discoverClient)
	restMapper := restmapper.NewDeferredDiscoveryRESTMapper(cachedDiscoveryClient)

	var importClusterDynamicClient dynamic.Interface
	if cfg.ExternalImportEnabled || cfg.ResourceSyncEnabled {
		importClusterDynamicClient, err = dynamic.NewForConfig(cfg.ExternalKubeClientCfg)
		if err != nil {
			return xerrors.Errorf("creates a new dynamic Clientset for the ExternalKubeClientCfg: %w", err)
		}
	}

	etcdclient, err := clientv3.New(clientv3.Config{
		Endpoints:   []string{cfg.EtcdURL},
		DialTimeout: 2 * time.Second,
	})
	if err != nil {
		return xerrors.Errorf("create an etcd client: %w", err)
	}

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	err = wait.PollUntilContextTimeout(ctx, kubeAPIServerPollInterval, kubeAPIServerReadyTimeout, true, func(_ context.Context) (bool, error) {
		_, err := client.CoreV1().Namespaces().Get(context.Background(), "kube-system", metav1.GetOptions{})
		if err != nil {
			klog.Infof("waiting for kube-system namespace to be ready: %v", err)
			return false, nil
		}
		klog.Info("kubeapi-server is ready")
		return true, nil
	})
	if err != nil {
		return xerrors.Errorf("kubeapi-server is not ready: %w", err)
	}

	dic, err := di.NewDIContainer(client, dynamicClient, restMapper, etcdclient, restCfg, cfg.InitialSchedulerCfg, cfg.ExternalImportEnabled, cfg.ResourceSyncEnabled, importClusterDynamicClient, cfg.Port, resourceapplier.Options{})
	if err != nil {
		return xerrors.Errorf("create di container: %w", err)
	}

	// If ExternalImportEnabled is enabled, the simulator import resources
	// from the target cluster that indicated by the `KUBECONFIG`.
	if cfg.ExternalImportEnabled {
		// This must be called after `StartScheduler`
		timeoutCtx, timeoutCancel := context.WithTimeout(ctx, importTimeout)
		defer timeoutCancel()
		if err := dic.OneshotClusterResourceImporter().ImportClusterResources(timeoutCtx, cfg.ResourceImportLabelSelector); err != nil {
			return xerrors.Errorf("import from the target cluster: %w", err)
		}
	}

	dic.SchedulerService().SetSchedulerConfig(cfg.InitialSchedulerCfg)

	if cfg.ResourceSyncEnabled {
		// Start the resource syncer to sync resources from the target cluster.
		if err = dic.ResourceSyncer().Run(ctx); err != nil {
			return xerrors.Errorf("start syncing: %w", err)
		}
	}

	// start simulator server
	s := server.NewSimulatorServer(cfg, dic)
	shutdownFn, err := s.Start(cfg.Port)
	if err != nil {
		return xerrors.Errorf("start simulator server: %w", err)
	}
	defer shutdownFn()

	// wait the signal
	quit := make(chan os.Signal, 1)
	signal.Notify(quit, syscall.SIGTERM, os.Interrupt)
	<-quit

	return nil
}
</file>

<file path="kube-scheduler-simulator/simulator/config/v1alpha1/defaults.go">
/*
Copyright 2017 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package v1alpha1

// func addDefaultingFuncs(scheme *runtime.Scheme) error {
//	 return RegisterDefaults(scheme)
// }
</file>

<file path="kube-scheduler-simulator/simulator/config/v1alpha1/doc.go">
/*
Copyright 2017 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

// +k8s:deepcopy-gen=package,register
// +k8s:conversion-gen=sigs.k8s.io/kube-scheduler-simulator/simulator/config
// +k8s:defaulter-gen=TypeMeta

// Package v1alpha1 is the v1alpha1 version of the kube-scheduler-simulator's config API
// +groupName=kube-scheduler-simulator-config
package v1alpha1 // Package v1alpha1 import "sigs.k8s.io/kube-scheduler-simulator/simulator/config/v1alpha1"
</file>

<file path="kube-scheduler-simulator/simulator/config/v1alpha1/register.go">
/*
Copyright 2017 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package v1alpha1

import (
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/runtime/schema"
)

var (
	SchemeBuilder      = runtime.NewSchemeBuilder(addKnownTypes)
	localSchemeBuilder = &SchemeBuilder
	AddToScheme        = SchemeBuilder.AddToScheme
)

// GroupName is the group name use in this package.
const (
	GroupName    = "kube-scheduler-simulator-config"
	GroupVersion = "v1alpha1"
)

// SchemeGroupVersion is group version used to register these objects.
var SchemeGroupVersion = schema.GroupVersion{Group: GroupName, Version: GroupVersion}

// Kind takes an unqualified kind and returns a Group qualified GroupKind.
func Kind(kind string) schema.GroupKind {
	return SchemeGroupVersion.WithKind(kind).GroupKind()
}

// Resource takes an unqualified resource and returns a Group qualified GroupResource.
func Resource(resource string) schema.GroupResource {
	return SchemeGroupVersion.WithResource(resource).GroupResource()
}

// func init() {
// 	 // We only register manually written functions here. The registration of the
// 	 // generated functions takes place in the generated files. The separation
// 	 // makes the code compile even when the generated files are missing.
// 	 localSchemeBuilder.Register(addKnownTypes, addDefaultingFuncs)
// }

func addKnownTypes(scheme *runtime.Scheme) error {
	// TODO this will get cleaned up with the scheme types are fixed
	scheme.AddKnownTypes(SchemeGroupVersion,
		&SimulatorConfiguration{},
	)
	return nil
}
</file>

<file path="kube-scheduler-simulator/simulator/config/v1alpha1/types.go">
/*
Copyright 2017 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package v1alpha1

import metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"

// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object

type SimulatorConfiguration struct {
	metav1.TypeMeta `json:",inline"`

	// This is the port number on which kube-scheduler-simulator
	// server is started.
	Port int `json:"port,omitempty"`

	// This is the URL for etcd.
	EtcdURL string `json:"etcdURL,omitempty"`

	// This URL represents the URL once web UI is started.
	// The simulator and internal kube-apiserver set the allowed
	// origin for CorsAllowedOriginList
	CorsAllowedOriginList []string `json:"corsAllowedOriginList,omitempty"`

	// This is for the beta feature "Importing cluster's resources".
	// This variable is used to find Kubeconfig required to access your
	// cluster for importing resources to scheduler simulator.
	KubeConfig string `json:"kubeConfig,omitempty"`

	// This is the URL for kube-apiserver.
	KubeAPIServerURL string `json:"kubeApiServerUrl,omitempty"`

	// This is the host of kube-apiserver which the simulator
	// starts internally. Its default value is 127.0.0.1.
	KubeAPIHost string `json:"kubeApiHost,omitempty"`

	// This is the port of kube-apiserver. Its default value is 3131.
	KubeAPIPort int `json:"kubeApiPort,omitempty"`

	// The path to a KubeSchedulerConfiguration file.
	// If passed, the simulator will start the scheduler
	// with that configuration. Or, if you use web UI,
	// you can change the configuration from the web UI as well.
	KubeSchedulerConfigPath string `json:"kubeSchedulerConfigPath,omitempty"`

	// This variable indicates whether the simulator will
	// import resources from an user cluster's or not.
	// Note, this is still a beta feature.
	ExternalImportEnabled bool `json:"externalImportEnabled,omitempty"`

	ResourceImportLabelSelector metav1.LabelSelector `json:"resourceImportLabelSelector,omitempty"`

	// This variable indicates whether the simulator will
	// sync resources from an user cluster's or not.
	ResourceSyncEnabled bool `json:"resourceSyncEnabled,omitempty"`

	// This variable indicates whether an external scheduler
	// is used.
	ExternalSchedulerEnabled bool `json:"externalSchedulerEnabled,omitempty"`
}
</file>

<file path="kube-scheduler-simulator/simulator/config/v1alpha1/zz_generated.conversion.go">
//go:build !ignore_autogenerated
// +build !ignore_autogenerated

/*
Copyright 2024 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

// Code generated by conversion-gen. DO NOT EDIT.

package v1alpha1

import (
	runtime "k8s.io/apimachinery/pkg/runtime"
)

func init() {
	localSchemeBuilder.Register(RegisterConversions)
}

// RegisterConversions adds conversion functions to the given scheme.
// Public to allow building arbitrary schemes.
func RegisterConversions(s *runtime.Scheme) error {
	return nil
}
</file>

<file path="kube-scheduler-simulator/simulator/config/v1alpha1/zz_generated.deepcopy.go">
//go:build !ignore_autogenerated
// +build !ignore_autogenerated

/*
Copyright 2024 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

// Code generated by deepcopy-gen. DO NOT EDIT.

package v1alpha1

import (
	runtime "k8s.io/apimachinery/pkg/runtime"
)

// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *SimulatorConfiguration) DeepCopyInto(out *SimulatorConfiguration) {
	*out = *in
	out.TypeMeta = in.TypeMeta
	if in.CorsAllowedOriginList != nil {
		in, out := &in.CorsAllowedOriginList, &out.CorsAllowedOriginList
		*out = make([]string, len(*in))
		copy(*out, *in)
	}
	return
}

// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new SimulatorConfiguration.
func (in *SimulatorConfiguration) DeepCopy() *SimulatorConfiguration {
	if in == nil {
		return nil
	}
	out := new(SimulatorConfiguration)
	in.DeepCopyInto(out)
	return out
}

// DeepCopyObject is an autogenerated deepcopy function, copying the receiver, creating a new runtime.Object.
func (in *SimulatorConfiguration) DeepCopyObject() runtime.Object {
	if c := in.DeepCopy(); c != nil {
		return c
	}
	return nil
}
</file>

<file path="kube-scheduler-simulator/simulator/config/v1alpha1/zz_generated.defaults.go">
//go:build !ignore_autogenerated
// +build !ignore_autogenerated

/*
Copyright 2024 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

// Code generated by defaulter-gen. DO NOT EDIT.

package v1alpha1

import (
	runtime "k8s.io/apimachinery/pkg/runtime"
)

// RegisterDefaults adds defaulters functions to the given scheme.
// Public to allow building arbitrary schemes.
// All generated defaulters are covering - they call all nested defaulters.
func RegisterDefaults(scheme *runtime.Scheme) error {
	return nil
}
</file>

<file path="kube-scheduler-simulator/simulator/config/config_test.go">
package config

import (
	"sort"
	"testing"

	"github.com/stretchr/testify/assert"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	configv1 "k8s.io/kube-scheduler/config/v1"
)

func Test_decodeSchedulerCfg(t *testing.T) {
	t.Parallel()

	tests := []struct {
		name    string
		buf     []byte
		want    *configv1.KubeSchedulerConfiguration
		wantErr bool
	}{
		{
			name: "success with normal configuration",
			buf: []byte(`
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
- pluginConfig:
  - args:
      scoringStrategy:
        resources:
        - name: cpu
          weight: 1
        type: MostAllocated
    name: NodeResourcesFit
`),
			want: &configv1.KubeSchedulerConfiguration{
				TypeMeta: metav1.TypeMeta{
					Kind:       "KubeSchedulerConfiguration",
					APIVersion: "kubescheduler.config.k8s.io/v1",
				},
				Profiles: []configv1.KubeSchedulerProfile{
					{
						PluginConfig: []configv1.PluginConfig{
							{
								Name: "NodeResourcesFit",
								Args: runtime.RawExtension{
									Object: &configv1.NodeResourcesFitArgs{
										ScoringStrategy: &configv1.ScoringStrategy{
											Resources: []configv1.ResourceSpec{
												{
													Name:   "cpu",
													Weight: 1,
												},
											},
											Type: "MostAllocated",
										},
									},
								},
							},
						},
					},
				},
			},
		},
		{
			name: "fail because of wrong apiVersion",
			buf: []byte(`
apiVersion: kubescheduler.config.k8s.io/v1beta1
kind: KubeSchedulerConfiguration
profiles:
- pluginConfig:
  - args:
      scoringStrategy:
        resources:
        - name: cpu
          weight: 1
        type: MostAllocated
    name: NodeResourcesFit
`),
			want:    nil,
			wantErr: true,
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			got, err := decodeSchedulerCfg(tt.buf)
			if (err != nil) != tt.wantErr {
				t.Errorf("decodeSchedulerCfg() error = %v, wantErr %v", err, tt.wantErr)
				return
			}
			if got != nil {
				if len(got.Profiles) != len(tt.want.Profiles) {
					t.Errorf("unmatch length of profiles, want: %v, got: %v", len(tt.want.Profiles), len(got.Profiles))
					return
				}

				for k := range got.Profiles {
					sort.SliceStable(got.Profiles[k].PluginConfig, func(i, j int) bool {
						return got.Profiles[k].PluginConfig[i].Name < got.Profiles[k].PluginConfig[j].Name
					})
					sort.SliceStable(tt.want.Profiles[k].PluginConfig, func(i, j int) bool {
						return tt.want.Profiles[k].PluginConfig[i].Name < tt.want.Profiles[k].PluginConfig[j].Name
					})
				}
				// remove Raw to assert
				for i := range got.Profiles {
					prof := &got.Profiles[i]
					for j := range prof.PluginConfig {
						prof.PluginConfig[j].Args.Raw = nil
					}
				}
			}
			assert.Equal(t, tt.want, got)
		})
	}
}

func Test_parseStringListEnv(t *testing.T) {
	t.Parallel()
	tests := []struct {
		name string
		arg  string
		want []string
	}{
		{
			name: "happy path: can parse the list which has multiple elements",
			arg:  "hoge,fuga,foo",
			want: []string{
				"hoge",
				"fuga",
				"foo",
			},
		},
		{
			name: "happy path: can parse the list which has the space between elements",
			arg:  "hoge,         fuga, foo    ",
			want: []string{
				"hoge",
				"fuga",
				"foo",
			},
		},
		{
			name: "happy path: do nothing with non-list string",
			arg:  "hoge",
			want: []string{
				"hoge",
			},
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			assert.Equalf(t, tt.want, parseStringListEnv(tt.arg), "parseStringListEnv(%v)", tt.arg)
		})
	}
}

func Test_validateURLs(t *testing.T) {
	t.Parallel()
	tests := []struct {
		name    string
		urls    []string
		wantErr bool
	}{
		{
			name: "all urls are valid",
			urls: []string{
				"https://hoge.com/hoge",
				"http://hoge2.com/hoge",
			},
			wantErr: false,
		},
		{
			name: "one url is invalid",
			urls: []string{
				"https://hoge.com/hoge",
				"invalid",
			},
			wantErr: true,
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			err := validateURLs(tt.urls)
			if (err != nil) != tt.wantErr {
				t.Errorf("unexpected error result is returned. got: %v, wantErr: %v", err, tt.wantErr)
			}
		})
	}
}
</file>

<file path="kube-scheduler-simulator/simulator/config/config.go">
package config

import (
	"errors"
	"net/url"
	"os"
	"strconv"
	"strings"

	"golang.org/x/xerrors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/client-go/rest"
	"k8s.io/client-go/tools/clientcmd"
	"k8s.io/klog/v2"
	configv1 "k8s.io/kube-scheduler/config/v1"
	"k8s.io/kubernetes/pkg/scheduler/apis/config/scheme"

	"sigs.k8s.io/kube-scheduler-simulator/simulator/config/v1alpha1"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler/config"
)

// ErrEmptyConfig represents the required config variable don't exist.
var ErrEmptyConfig = errors.New("config is required, but empty")

// configYaml represents the value from the config file.
var configYaml = &v1alpha1.SimulatorConfiguration{}

// defaultSchedulerCfgPath is where we have the scheduler config in the container by default.
const defaultSchedulerCfgPath = "/config/scheduler.yaml"

// Config is configuration for simulator.
type Config struct {
	Port                  int
	KubeAPIServerURL      string
	EtcdURL               string
	CorsAllowedOriginList []string
	// ExternalImportEnabled indicates whether the simulator will import resources from a target cluster once
	// when it's started.
	ExternalImportEnabled bool
	// ResourceImportLabelSelector is the label selector used to determine which resources from the target cluster should be imported.
	ResourceImportLabelSelector metav1.LabelSelector
	// ResourceSyncEnabled indicates whether the simulator will keep syncing resources from a target cluster.
	ResourceSyncEnabled bool
	// ExternalKubeClientCfg is KubeConfig to get resources from external cluster.
	// This field should be set when ExternalImportEnabled == true or ResourceSyncEnabled == true.
	ExternalKubeClientCfg *rest.Config
	InitialSchedulerCfg   *configv1.KubeSchedulerConfiguration
}

const (
	// defaultFilePath is the config file path.
	// TODO: move it to somewhere configurable from outside if users want.
	defaultFilePath = "./config.yaml"
)

// NewConfig gets some settings from config file or environment variables.
//
//nolint:cyclop
func NewConfig() (*Config, error) {
	if err := LoadYamlConfig(defaultFilePath); err != nil {
		return nil, err
	}

	port, err := getPort()
	if err != nil {
		return nil, xerrors.Errorf("get port: %w", err)
	}

	etcdurl, err := getEtcdURL()
	if err != nil {
		return nil, xerrors.Errorf("get etcd URL: %w", err)
	}

	corsAllowedOriginList, err := getCorsAllowedOriginList()
	if err != nil {
		return nil, xerrors.Errorf("get frontend URL: %w", err)
	}

	apiurl, err := getKubeAPIServerURL()
	if err != nil {
		return nil, xerrors.Errorf("get kube API server URL: %w", err)
	}

	externalimportenabled := getExternalImportEnabled()
	resourceSyncEnabled := getResourceSyncEnabled()
	externalKubeClientCfg := &rest.Config{}
	if externalimportenabled && resourceSyncEnabled {
		return nil, xerrors.Errorf("externalImportEnabled and resourceSyncEnabled cannot be used simultaneously.")
	}
	if externalimportenabled || resourceSyncEnabled {
		externalKubeClientCfg, err = clientcmd.BuildConfigFromFlags("", configYaml.KubeConfig)
		if err != nil {
			return nil, xerrors.Errorf("get kube clientconfig: %w", err)
		}
	}

	initialschedulerCfg, err := GetSchedulerCfg()
	if err != nil {
		return nil, xerrors.Errorf("get SchedulerCfg: %w", err)
	}

	return &Config{
		Port:                        port,
		KubeAPIServerURL:            apiurl,
		EtcdURL:                     etcdurl,
		CorsAllowedOriginList:       corsAllowedOriginList,
		InitialSchedulerCfg:         initialschedulerCfg,
		ExternalImportEnabled:       externalimportenabled,
		ResourceImportLabelSelector: configYaml.ResourceImportLabelSelector,
		ExternalKubeClientCfg:       externalKubeClientCfg,
		ResourceSyncEnabled:         resourceSyncEnabled,
	}, nil
}

// LoadYamlConfig read the yaml file and set configYaml.
func LoadYamlConfig(configFile string) error {
	if configFile == "" {
		klog.V(1).InfoS("Config file not specified")
		return nil
	}

	conf, err := os.ReadFile(configFile)
	if err != nil {
		return xerrors.Errorf("failed to read config file: %w", err)
	}

	versionedConfig := &v1alpha1.SimulatorConfiguration{}

	decoder := scheme.Codecs.UniversalDecoder(v1alpha1.SchemeGroupVersion)
	if err := runtime.DecodeInto(decoder, conf, versionedConfig); err != nil {
		return xerrors.Errorf("failed decoding simulator's config %w", err)
	}

	configYaml = versionedConfig

	return nil
}

// getPort gets port from environment variable named PORT first, if empty from the config file.
func getPort() (int, error) {
	p := os.Getenv("PORT")
	port, err := strconv.Atoi(p)
	if p == "" || err != nil {
		port = configYaml.Port
		if port == 0 {
			return 0, xerrors.Errorf("get PORT from config: %w", ErrEmptyConfig)
		}
	}
	return port, nil
}

// getKubeAPIServerURL gets KubeAPIServerURL from environment variable first, if empty from the config file.
func getKubeAPIServerURL() (string, error) {
	url := os.Getenv("KUBE_APISERVER_URL")
	if url == "" {
		url = configYaml.KubeAPIServerURL
		if url == "" {
			return "", xerrors.Errorf("get KUBE_APISERVER_URL from config: %w", ErrEmptyConfig)
		}
	}
	return url, nil
}

// getEtcdURL gets EtcdURL from environment variable first,
// if empty from the config file.
func getEtcdURL() (string, error) {
	e := os.Getenv("KUBE_SCHEDULER_SIMULATOR_ETCD_URL")
	if e == "" {
		e = configYaml.EtcdURL
		if e == "" {
			return "", xerrors.Errorf("get KUBE_SCHEDULER_SIMULATOR_ETCD_URL from config: %w", ErrEmptyConfig)
		}
	}
	return e, nil
}

// getCorsAllowedOriginList fetches CorsAllowedOriginList from the env named CORS_ALLOWED_ORIGIN_LIST
// if empty from the config file.
// This allowed list is applied to kube-apiserver and the simulator server.
//
// Let's say CORS_ALLOWED_ORIGIN_LIST=http://localhost:3000,http://localhost:3001,http://localhost:3002 is given.
// Then, getCorsAllowedOriginList returns []string{"http://localhost:3000", "http://localhost:3001", "http://localhost:3002"}
func getCorsAllowedOriginList() ([]string, error) {
	e := os.Getenv("CORS_ALLOWED_ORIGIN_LIST")
	urls := parseStringListEnv(e)

	if err := validateURLs(urls); e == "" || err != nil {
		urls = configYaml.CorsAllowedOriginList
	}
	if err := validateURLs(urls); err != nil {
		return nil, xerrors.Errorf("validate origins in cors-allowed-origin-list: %w", err)
	}

	return urls, nil
}

// validateURLs checks if all URLs in slice is valid or not.
func validateURLs(urls []string) error {
	for _, u := range urls {
		_, err := url.ParseRequestURI(u)
		if err != nil {
			return xerrors.Errorf("parse request uri: %w", err)
		}
	}
	return nil
}

func parseStringListEnv(e string) []string {
	list := strings.Split(e, ",")
	for i := range list {
		// remove space
		list[i] = strings.TrimSpace(list[i])
	}

	return list
}

// GetSchedulerCfg reads KUBE_SCHEDULER_CONFIG_PATH which means initial kube-scheduler configuration
// if empty from the config file.
// and converts it into *configv1.KubeSchedulerConfiguration.
// KUBE_SCHEDULER_CONFIG_PATH is not required.
// If KUBE_SCHEDULER_CONFIG_PATH is not set, the default configuration of kube-scheduler will be used.
func GetSchedulerCfg() (*configv1.KubeSchedulerConfiguration, error) {
	kubeSchedulerConfigPath := os.Getenv("KUBE_SCHEDULER_CONFIG_PATH")
	if kubeSchedulerConfigPath == "" {
		kubeSchedulerConfigPath = configYaml.KubeSchedulerConfigPath
		if kubeSchedulerConfigPath == "" {
			config.SetKubeSchedulerCfgPath(defaultSchedulerCfgPath)
			dsc, err := config.DefaultSchedulerConfig()
			if err != nil {
				return nil, xerrors.Errorf("create default scheduler config: %w", err)
			}
			return dsc, nil
		}
	}
	config.SetKubeSchedulerCfgPath(kubeSchedulerConfigPath)
	data, err := os.ReadFile(kubeSchedulerConfigPath)
	if err != nil {
		return nil, xerrors.Errorf("read scheduler config file: %w", err)
	}

	sc, err := decodeSchedulerCfg(data)
	if err != nil {
		return nil, xerrors.Errorf("decode scheduler config file: %w", err)
	}

	return sc, nil
}

// getExternalImportEnabled reads EXTERNAL_IMPORT_ENABLED and convert it to bool
// if empty from the config file.
// This function will return `true` if `EXTERNAL_IMPORT_ENABLED` is "1".
func getExternalImportEnabled() bool {
	isExternalImportEnabledString := os.Getenv("EXTERNAL_IMPORT_ENABLED")
	if isExternalImportEnabledString == "" {
		isExternalImportEnabledString = strconv.FormatBool(configYaml.ExternalImportEnabled)
	}
	isExternalImportEnabled, _ := strconv.ParseBool(isExternalImportEnabledString)
	return isExternalImportEnabled
}

// getResourceSyncEnabled reads RESOURCE_SYNC_ENABLED and converts it to bool
// if empty from the config file.
// This function will return `true` if `RESOURCE_SYNC_ENABLED` is "1".
func getResourceSyncEnabled() bool {
	resourceSyncEnabledString := os.Getenv("RESOURCE_SYNC_ENABLED")
	if resourceSyncEnabledString == "" {
		resourceSyncEnabledString = strconv.FormatBool(configYaml.ResourceSyncEnabled)
	}
	resourceSyncEnabled, _ := strconv.ParseBool(resourceSyncEnabledString)
	return resourceSyncEnabled
}

func decodeSchedulerCfg(buf []byte) (*configv1.KubeSchedulerConfiguration, error) {
	decoder := scheme.Codecs.UniversalDeserializer()
	obj, _, err := decoder.Decode(buf, nil, nil)
	if err != nil {
		return nil, xerrors.Errorf("load an k8s object from buffer: %w", err)
	}

	sc, ok := obj.(*configv1.KubeSchedulerConfiguration)
	if !ok {
		return nil, xerrors.Errorf("convert to *configv1.KubeSchedulerConfiguration, but got unexpected type: %T", obj)
	}

	if err = sc.DecodeNestedObjects(decoder); err != nil {
		return nil, xerrors.Errorf("decode nested plugin args: %w", err)
	}
	return sc, nil
}

func GetKubeClientConfig() (*rest.Config, error) {
	kubeConfig := clientcmd.NewNonInteractiveDeferredLoadingClientConfig(
		clientcmd.NewDefaultClientConfigLoadingRules(), &clientcmd.ConfigOverrides{})
	clientConfig, err := kubeConfig.ClientConfig()
	if err != nil {
		return nil, xerrors.Errorf("get client config: %w", err)
	}
	return clientConfig, nil
}
</file>

<file path="kube-scheduler-simulator/simulator/docs/api-samples/v1/export.md">
# /v1/export samples

## case1: export from a simulator in its initial state

### Request
```
GET /api/v1/export HTTP/1.1
Host: localhost:1212
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:93.0) Gecko/20100101 Firefox/93.0
Accept: application/json, text/plain, */*
Accept-Language: ja,en-US;q=0.7,en;q=0.3
Accept-Encoding: gzip, deflate
Origin: http://localhost:3000
Connection: close
Referer: http://localhost:3000/
Sec-Fetch-Dest: empty
Sec-Fetch-Mode: cors
Sec-Fetch-Site: cross-site
```

### Response
```
HTTP/1.1 200 OK
Access-Control-Allow-Credentials: true
Access-Control-Allow-Origin: http://localhost:3000
Content-Type: application/json; charset=UTF-8
Vary: Origin
Date: Sat, 15 Jan 2022 06:59:35 GMT
Connection: close
Content-Length: 4364

{"pods":[],"nodes":[],"pvs":[],"pvcs":[],"storageClasses":[],"priorityClasses":[{"metadata":{"name":"system-cluster-critical","uid":"6baf27a9-4774-4703-b210-680d58f7374a","resourceVersion":"51","generation":1,"creationTimestamp":"2022-01-15T06:59:18Z","managedFields":[{"manager":"simulator","operation":"Update","apiVersion":"scheduling.k8s.io/v1","time":"2022-01-15T06:59:18Z","fieldsType":"FieldsV1","fieldsV1":{"f:description":{},"f:preemptionPolicy":{},"f:value":{}}}]},"value":2000000000,"description":"Used for system critical pods that must run in the cluster, but can be moved to another node if necessary.","preemptionPolicy":"PreemptLowerPriority"},{"metadata":{"name":"system-node-critical","uid":"b862e029-c72e-422d-8be4-3eaae6138c0a","resourceVersion":"50","generation":1,"creationTimestamp":"2022-01-15T06:59:18Z","managedFields":[{"manager":"simulator","operation":"Update","apiVersion":"scheduling.k8s.io/v1","time":"2022-01-15T06:59:18Z","fieldsType":"FieldsV1","fieldsV1":{"f:description":{},"f:preemptionPolicy":{},"f:value":{}}}]},"value":2000001000,"description":"Used for system critical pods that must not be moved from their current node.","preemptionPolicy":"PreemptLowerPriority"}],"schedulerConfig":{"parallelism":16,"leaderElection":{"leaderElect":true,"leaseDuration":"15s","renewDeadline":"10s","retryPeriod":"2s","resourceLock":"leases","resourceName":"kube-scheduler","resourceNamespace":"kube-system"},"clientConnection":{"kubeconfig":"","acceptContentTypes":"","contentType":"application/vnd.kubernetes.protobuf","qps":50,"burst":100},"healthzBindAddress":"0.0.0.0:10251","metricsBindAddress":"0.0.0.0:10251","enableProfiling":true,"enableContentionProfiling":true,"percentageOfNodesToScore":0,"podInitialBackoffSeconds":1,"podMaxBackoffSeconds":10,"profiles":[{"schedulerName":"default-scheduler","plugins":{"queueSort":{"enabled":[{"name":"PrioritySort"}]},"preFilter":{"enabled":[{"name":"NodeResourcesFit"},{"name":"NodePorts"},{"name":"VolumeRestrictions"},{"name":"PodTopologySpread"},{"name":"InterPodAffinity"},{"name":"VolumeBinding"},{"name":"NodeAffinity"}]},"filter":{"enabled":[{"name":"NodeUnschedulable"},{"name":"NodeName"},{"name":"TaintToleration"},{"name":"NodeAffinity"},{"name":"NodePorts"},{"name":"NodeResourcesFit"},{"name":"VolumeRestrictions"},{"name":"EBSLimits"},{"name":"GCEPDLimits"},{"name":"NodeVolumeLimits"},{"name":"AzureDiskLimits"},{"name":"VolumeBinding"},{"name":"VolumeZone"},{"name":"PodTopologySpread"},{"name":"InterPodAffinity"}]},"postFilter":{"enabled":[{"name":"DefaultPreemption"}]},"preScore":{"enabled":[{"name":"InterPodAffinity"},{"name":"PodTopologySpread"},{"name":"TaintToleration"},{"name":"NodeAffinity"}]},"score":{"enabled":[{"name":"NodeResourcesBalancedAllocation","weight":1},{"name":"ImageLocality","weight":1},{"name":"InterPodAffinity","weight":1},{"name":"NodeResourcesFit","weight":1},{"name":"NodeAffinity","weight":1},{"name":"PodTopologySpread","weight":2},{"name":"TaintToleration","weight":1}]},"reserve":{"enabled":[{"name":"VolumeBinding"}]},"permit":{},"preBind":{"enabled":[{"name":"VolumeBinding"}]},"bind":{"enabled":[{"name":"DefaultBinder"}]},"postBind":{}},"pluginConfig":[{"name":"DefaultPreemption","args":{"kind":"DefaultPreemptionArgs","apiVersion":"kubescheduler.config.k8s.io/v1","minCandidateNodesPercentage":10,"minCandidateNodesAbsolute":100}},{"name":"InterPodAffinity","args":{"kind":"InterPodAffinityArgs","apiVersion":"kubescheduler.config.k8s.io/v1","hardPodAffinityWeight":1}},{"name":"NodeAffinity","args":{"kind":"NodeAffinityArgs","apiVersion":"kubescheduler.config.k8s.io/v1"}},{"name":"NodeResourcesBalancedAllocation","args":{"kind":"NodeResourcesBalancedAllocationArgs","apiVersion":"kubescheduler.config.k8s.io/v1","resources":[{"name":"cpu","weight":1},{"name":"memory","weight":1}]}},{"name":"NodeResourcesFit","args":{"kind":"NodeResourcesFitArgs","apiVersion":"kubescheduler.config.k8s.io/v1","scoringStrategy":{"type":"LeastAllocated","resources":[{"name":"cpu","weight":1},{"name":"memory","weight":1}]}}},{"name":"PodTopologySpread","args":{"kind":"PodTopologySpreadArgs","apiVersion":"kubescheduler.config.k8s.io/v1","defaultingType":"System"}},{"name":"VolumeBinding","args":{"kind":"VolumeBindingArgs","apiVersion":"kubescheduler.config.k8s.io/v1","bindTimeoutSeconds":600}}]}]}}
```

## case2: export with some PVs, PVCs and PriorityClasses
### Request
```
GET /api/v1/export HTTP/1.1
Host: localhost:1212
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:93.0) Gecko/20100101 Firefox/93.0
Accept: application/json, text/plain, */*
Accept-Language: ja,en-US;q=0.7,en;q=0.3
Accept-Encoding: gzip, deflate
Origin: http://localhost:3000
Connection: close
Referer: http://localhost:3000/
Sec-Fetch-Dest: empty
Sec-Fetch-Mode: cors
Sec-Fetch-Site: cross-site
```

### Response
```
HTTP/1.1 200 OK
Access-Control-Allow-Credentials: true
Access-Control-Allow-Origin: http://localhost:3000
Content-Type: application/json; charset=UTF-8
Vary: Origin
Date: Sat, 15 Jan 2022 07:02:59 GMT
Connection: close
Content-Length: 7760

{"pods":[],"nodes":[],"pvs":[{"metadata":{"name":"pv1","uid":"0a0aee91-61cc-4a5e-9937-c7e6bb3580d7","resourceVersion":"183","creationTimestamp":"2022-01-15T07:02:27Z","annotations":{"pv.kubernetes.io/bound-by-controller":"yes"},"managedFields":[{"manager":"simulator","operation":"Apply","apiVersion":"v1","time":"2022-01-15T07:02:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:spec":{"f:accessModes":{},"f:capacity":{"f:storage":{}},"f:hostPath":{"f:path":{},"f:type":{}},"f:persistentVolumeReclaimPolicy":{},"f:volumeMode":{}}}},{"manager":"simulator","operation":"Update","apiVersion":"v1","time":"2022-01-15T07:02:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:pv.kubernetes.io/bound-by-controller":{}}},"f:spec":{"f:claimRef":{}}}},{"manager":"simulator","operation":"Update","apiVersion":"v1","time":"2022-01-15T07:02:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:phase":{}}},"subresource":"status"}]},"spec":{"capacity":{"storage":"1Gi"},"hostPath":{"path":"/tmp/data","type":"DirectoryOrCreate"},"accessModes":["ReadWriteOnce"],"claimRef":{"kind":"PersistentVolumeClaim","namespace":"default","name":"pvc1","uid":"5e1d250e-ab9c-4699-8f35-afe4a0c2faa4","apiVersion":"v1","resourceVersion":"176"},"persistentVolumeReclaimPolicy":"Delete","volumeMode":"Filesystem"},"status":{"phase":"Bound"}},{"metadata":{"name":"pv2","uid":"65307f8e-763a-455f-8e15-1c8861e95d97","resourceVersion":"187","creationTimestamp":"2022-01-15T07:02:30Z","managedFields":[{"manager":"simulator","operation":"Apply","apiVersion":"v1","time":"2022-01-15T07:02:30Z","fieldsType":"FieldsV1","fieldsV1":{"f:spec":{"f:accessModes":{},"f:capacity":{"f:storage":{}},"f:hostPath":{"f:path":{},"f:type":{}},"f:persistentVolumeReclaimPolicy":{},"f:volumeMode":{}}}},{"manager":"simulator","operation":"Update","apiVersion":"v1","time":"2022-01-15T07:02:30Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:phase":{}}},"subresource":"status"}]},"spec":{"capacity":{"storage":"1Gi"},"hostPath":{"path":"/tmp/data","type":"DirectoryOrCreate"},"accessModes":["ReadWriteOnce"],"persistentVolumeReclaimPolicy":"Delete","volumeMode":"Filesystem"},"status":{"phase":"Available"}}],"pvcs":[{"metadata":{"name":"pvc1","namespace":"default","uid":"5e1d250e-ab9c-4699-8f35-afe4a0c2faa4","resourceVersion":"185","creationTimestamp":"2022-01-15T07:02:25Z","annotations":{"pv.kubernetes.io/bind-completed":"yes","pv.kubernetes.io/bound-by-controller":"yes"},"managedFields":[{"manager":"simulator","operation":"Apply","apiVersion":"v1","time":"2022-01-15T07:02:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:spec":{"f:accessModes":{},"f:resources":{"f:requests":{"f:storage":{}}},"f:volumeMode":{}}}},{"manager":"simulator","operation":"Update","apiVersion":"v1","time":"2022-01-15T07:02:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:pv.kubernetes.io/bind-completed":{},"f:pv.kubernetes.io/bound-by-controller":{}}},"f:spec":{"f:volumeName":{}}}},{"manager":"simulator","operation":"Update","apiVersion":"v1","time":"2022-01-15T07:02:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:accessModes":{},"f:capacity":{".":{},"f:storage":{}},"f:phase":{}}},"subresource":"status"}]},"spec":{"accessModes":["ReadWriteOnce"],"resources":{"requests":{"storage":"1Gi"}},"volumeName":"pv1","volumeMode":"Filesystem"},"status":{"phase":"Bound","accessModes":["ReadWriteOnce"],"capacity":{"storage":"1Gi"}}}],"storageClasses":[],"priorityClasses":[{"metadata":{"name":"system-cluster-critical","uid":"6baf27a9-4774-4703-b210-680d58f7374a","resourceVersion":"51","generation":1,"creationTimestamp":"2022-01-15T06:59:18Z","managedFields":[{"manager":"simulator","operation":"Update","apiVersion":"scheduling.k8s.io/v1","time":"2022-01-15T06:59:18Z","fieldsType":"FieldsV1","fieldsV1":{"f:description":{},"f:preemptionPolicy":{},"f:value":{}}}]},"value":2000000000,"description":"Used for system critical pods that must run in the cluster, but can be moved to another node if necessary.","preemptionPolicy":"PreemptLowerPriority"},{"metadata":{"name":"system-node-critical","uid":"b862e029-c72e-422d-8be4-3eaae6138c0a","resourceVersion":"50","generation":1,"creationTimestamp":"2022-01-15T06:59:18Z","managedFields":[{"manager":"simulator","operation":"Update","apiVersion":"scheduling.k8s.io/v1","time":"2022-01-15T06:59:18Z","fieldsType":"FieldsV1","fieldsV1":{"f:description":{},"f:preemptionPolicy":{},"f:value":{}}}]},"value":2000001000,"description":"Used for system critical pods that must not be moved from their current node.","preemptionPolicy":"PreemptLowerPriority"}],"schedulerConfig":{"parallelism":16,"leaderElection":{"leaderElect":true,"leaseDuration":"15s","renewDeadline":"10s","retryPeriod":"2s","resourceLock":"leases","resourceName":"kube-scheduler","resourceNamespace":"kube-system"},"clientConnection":{"kubeconfig":"","acceptContentTypes":"","contentType":"application/vnd.kubernetes.protobuf","qps":50,"burst":100},"healthzBindAddress":"0.0.0.0:10251","metricsBindAddress":"0.0.0.0:10251","enableProfiling":true,"enableContentionProfiling":true,"percentageOfNodesToScore":0,"podInitialBackoffSeconds":1,"podMaxBackoffSeconds":10,"profiles":[{"schedulerName":"default-scheduler","plugins":{"queueSort":{"enabled":[{"name":"PrioritySort"}]},"preFilter":{"enabled":[{"name":"NodeResourcesFit"},{"name":"NodePorts"},{"name":"VolumeRestrictions"},{"name":"PodTopologySpread"},{"name":"InterPodAffinity"},{"name":"VolumeBinding"},{"name":"NodeAffinity"}]},"filter":{"enabled":[{"name":"NodeUnschedulable"},{"name":"NodeName"},{"name":"TaintToleration"},{"name":"NodeAffinity"},{"name":"NodePorts"},{"name":"NodeResourcesFit"},{"name":"VolumeRestrictions"},{"name":"EBSLimits"},{"name":"GCEPDLimits"},{"name":"NodeVolumeLimits"},{"name":"AzureDiskLimits"},{"name":"VolumeBinding"},{"name":"VolumeZone"},{"name":"PodTopologySpread"},{"name":"InterPodAffinity"}]},"postFilter":{"enabled":[{"name":"DefaultPreemption"}]},"preScore":{"enabled":[{"name":"InterPodAffinity"},{"name":"PodTopologySpread"},{"name":"TaintToleration"},{"name":"NodeAffinity"}]},"score":{"enabled":[{"name":"NodeResourcesBalancedAllocation","weight":1},{"name":"ImageLocality","weight":1},{"name":"InterPodAffinity","weight":1},{"name":"NodeResourcesFit","weight":1},{"name":"NodeAffinity","weight":1},{"name":"PodTopologySpread","weight":2},{"name":"TaintToleration","weight":1}]},"reserve":{"enabled":[{"name":"VolumeBinding"}]},"permit":{},"preBind":{"enabled":[{"name":"VolumeBinding"}]},"bind":{"enabled":[{"name":"DefaultBinder"}]},"postBind":{}},"pluginConfig":[{"name":"DefaultPreemption","args":{"kind":"DefaultPreemptionArgs","apiVersion":"kubescheduler.config.k8s.io/v1","minCandidateNodesPercentage":10,"minCandidateNodesAbsolute":100}},{"name":"InterPodAffinity","args":{"kind":"InterPodAffinityArgs","apiVersion":"kubescheduler.config.k8s.io/v1","hardPodAffinityWeight":1}},{"name":"NodeAffinity","args":{"kind":"NodeAffinityArgs","apiVersion":"kubescheduler.config.k8s.io/v1"}},{"name":"NodeResourcesBalancedAllocation","args":{"kind":"NodeResourcesBalancedAllocationArgs","apiVersion":"kubescheduler.config.k8s.io/v1","resources":[{"name":"cpu","weight":1},{"name":"memory","weight":1}]}},{"name":"NodeResourcesFit","args":{"kind":"NodeResourcesFitArgs","apiVersion":"kubescheduler.config.k8s.io/v1","scoringStrategy":{"type":"LeastAllocated","resources":[{"name":"cpu","weight":1},{"name":"memory","weight":1}]}}},{"name":"PodTopologySpread","args":{"kind":"PodTopologySpreadArgs","apiVersion":"kubescheduler.config.k8s.io/v1","defaultingType":"System"}},{"name":"VolumeBinding","args":{"kind":"VolumeBindingArgs","apiVersion":"kubescheduler.config.k8s.io/v1","bindTimeoutSeconds":600}}]}]}}

```
</file>

<file path="kube-scheduler-simulator/simulator/docs/api-samples/v1/import.md">
# /v1/import samples

## case1: import resources with some PVs and PVCs

### Request
```
POST /api/v1/import HTTP/1.1
Host: localhost:1212
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:93.0) Gecko/20100101 Firefox/93.0
Accept: application/json, text/plain, */*
Accept-Language: ja,en-US;q=0.7,en;q=0.3
Accept-Encoding: gzip, deflate
Origin: http://localhost:3000
Connection: close
Referer: http://localhost:3000/
Sec-Fetch-Dest: empty
Sec-Fetch-Mode: cors
Sec-Fetch-Site: cross-site
Content-Type: application/json; charset=UTF-8
Content-Length: 4580

{"pods":[],"nodes":[],"pvs":[{"metadata":{"name":"pv1","uid":"db4a5204-ef32-4ff4-b112-be4090b3e57e","resourceVersion":"1488","creationTimestamp":"2021-12-28T03:59:58Z","annotations":{"pv.kubernetes.io/bound-by-controller":"yes"},"managedFields":[{"manager":"simulator","operation":"Apply","apiVersion":"v1","time":"2021-12-28T03:59:58Z","fieldsType":"FieldsV1","fieldsV1":{"f:spec":{"f:accessModes":{},"f:capacity":{"f:storage":{}},"f:hostPath":{"f:path":{},"f:type":{}},"f:persistentVolumeReclaimPolicy":{},"f:volumeMode":{}}}},{"manager":"simulator","operation":"Update","apiVersion":"v1","time":"2021-12-28T03:59:58Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:pv.kubernetes.io/bound-by-controller":{}}},"f:spec":{"f:claimRef":{}}}},{"manager":"simulator","operation":"Update","apiVersion":"v1","time":"2021-12-28T03:59:58Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:phase":{}}},"subresource":"status"}]},"spec":{"capacity":{"storage":"1Gi"},"hostPath":{"path":"/tmp/data","type":"DirectoryOrCreate"},"accessModes":["ReadWriteOnce"],"claimRef":{"kind":"PersistentVolumeClaim","namespace":"default","name":"pvc1","uid":"eaeda676-2c00-425f-b4c2-ad090a2e3c5a","apiVersion":"v1","resourceVersion":"1481"},"persistentVolumeReclaimPolicy":"Delete","volumeMode":"Filesystem"},"status":{"phase":"Bound"}}],"pvcs":[{"metadata":{"name":"pvc1","namespace":"default","uid":"eaeda676-2c00-425f-b4c2-ad090a2e3c5a","resourceVersion":"1490","creationTimestamp":"2021-12-28T03:59:56Z","annotations":{"pv.kubernetes.io/bind-completed":"yes","pv.kubernetes.io/bound-by-controller":"yes"},"managedFields":[{"manager":"simulator","operation":"Apply","apiVersion":"v1","time":"2021-12-28T03:59:56Z","fieldsType":"FieldsV1","fieldsV1":{"f:spec":{"f:accessModes":{},"f:resources":{"f:requests":{"f:storage":{}}},"f:volumeMode":{}}}},{"manager":"simulator","operation":"Update","apiVersion":"v1","time":"2021-12-28T03:59:58Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:pv.kubernetes.io/bind-completed":{},"f:pv.kubernetes.io/bound-by-controller":{}}},"f:spec":{"f:volumeName":{}}}},{"manager":"simulator","operation":"Update","apiVersion":"v1","time":"2021-12-28T03:59:58Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:accessModes":{},"f:capacity":{".":{},"f:storage":{}},"f:phase":{}}},"subresource":"status"}]},"spec":{"accessModes":["ReadWriteOnce"],"resources":{"requests":{"storage":"1Gi"}},"volumeName":"pv1","volumeMode":"Filesystem"},"status":{"phase":"Bound","accessModes":["ReadWriteOnce"],"capacity":{"storage":"1Gi"}}}],"storageClasses":[],"priorityClasses":[],"schedulerConfig":{"parallelism":16,"leaderElection":{"leaderElect":true,"leaseDuration":"15s","renewDeadline":"10s","retryPeriod":"2s","resourceLock":"leases","resourceName":"kube-scheduler","resourceNamespace":"kube-system"},"clientConnection":{"kubeconfig":"","acceptContentTypes":"","contentType":"application/vnd.kubernetes.protobuf","qps":50,"burst":100},"healthzBindAddress":"0.0.0.0:10251","metricsBindAddress":"0.0.0.0:10251","enableProfiling":true,"enableContentionProfiling":true,"percentageOfNodesToScore":0,"podInitialBackoffSeconds":1,"podMaxBackoffSeconds":10,"profiles":[{"schedulerName":"default-scheduler","plugins":{"queueSort":{"enabled":[{"name":"PrioritySort"}]},"preFilter":{"enabled":[{"name":"NodeResourcesFit"},{"name":"NodePorts"},{"name":"VolumeRestrictions"},{"name":"PodTopologySpread"},{"name":"InterPodAffinity"},{"name":"VolumeBinding"},{"name":"NodeAffinity"}]},"filter":{"enabled":[{"name":"NodeUnschedulable"},{"name":"NodeName"},{"name":"TaintToleration"},{"name":"NodeAffinity"},{"name":"NodePorts"},{"name":"NodeResourcesFit"},{"name":"VolumeRestrictions"},{"name":"EBSLimits"},{"name":"GCEPDLimits"},{"name":"NodeVolumeLimits"},{"name":"AzureDiskLimits"},{"name":"VolumeBinding"},{"name":"VolumeZone"},{"name":"PodTopologySpread"},{"name":"InterPodAffinity"}]},"postFilter":{"enabled":[{"name":"DefaultPreemption"}]},"preScore":{"enabled":[{"name":"InterPodAffinity"},{"name":"PodTopologySpread"},{"name":"TaintToleration"},{"name":"NodeAffinity"}]},"score":{"enabled":[{"name":"NodeResourcesBalancedAllocation","weight":1},{"name":"ImageLocality","weight":1},{"name":"InterPodAffinity","weight":1},{"name":"NodeResourcesFit","weight":1},{"name":"NodeAffinity","weight":1},{"name":"PodTopologySpread","weight":2},{"name":"TaintToleration","weight":1}]},"reserve":{"enabled":[{"name":"VolumeBinding"}]},"permit":{},"preBind":{"enabled":[{"name":"VolumeBinding"}]},"bind":{"enabled":[{"name":"DefaultBinder"}]},"postBind":{}}}]}}
```

### Response
```
HTTP/1.1 200 OK
Access-Control-Allow-Credentials: true
Access-Control-Allow-Origin: http://localhost:3000
Vary: Origin
Date: Sun, 02 Jan 2022 15:10:37 GMT
Content-Length: 0
Connection: close

```
</file>

<file path="kube-scheduler-simulator/simulator/docs/api-samples/v1/README.md">
## API samples

These docs are examples of requests and responses of APIs.
</file>

<file path="kube-scheduler-simulator/simulator/docs/sample/debuggable-scheduler/kubeconfig.yaml">
apiVersion: v1
kind: Config

clusters:
  - cluster:
      server: http://localhost:3131
    name: simulator

contexts:
  - context:
      cluster: simulator
    name: simulator

current-context: simulator
</file>

<file path="kube-scheduler-simulator/simulator/docs/sample/debuggable-scheduler/main.go">
package main

import (
	"fmt"
	"os"

	_ "k8s.io/client-go/plugin/pkg/client/auth/gcp"
	"k8s.io/component-base/cli"
	_ "k8s.io/component-base/logs/json/register" // for JSON log format registration
	_ "k8s.io/component-base/metrics/prometheus/clientgo"
	_ "k8s.io/component-base/metrics/prometheus/version" // for version metric registration
	"k8s.io/klog/v2"

	drlScheduler "sigs.k8s.io/kube-scheduler-simulator/simulator/docs/sample/drl"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/pkg/debuggablescheduler"
)

func main() {
	command, cancelFn, err := debuggablescheduler.NewSchedulerCommand(
		debuggablescheduler.WithPlugin(drlScheduler.Name, drlScheduler.New),
	)
	if err != nil {
		klog.Info(fmt.Sprintf("failed to build the scheduler command: %+v", err))
		os.Exit(1)
	}
	code := cli.Run(command)

	cancelFn()
	os.Exit(code)
}
</file>

<file path="kube-scheduler-simulator/simulator/docs/sample/debuggable-scheduler/scheduler.yaml">
kind: KubeSchedulerConfiguration
apiVersion: kubescheduler.config.k8s.io/v1
clientConnection:
  kubeconfig: kubeconfig.yaml
profiles:
  - schedulerName: default-scheduler
    plugins:
      multiPoint:
        enabled:
          - name: NodeNumber
            weight: 10
</file>

<file path="kube-scheduler-simulator/simulator/docs/sample/drl/plugin.go">
package drlScheduler

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"math/rand"
	"net/http"
	"time"

	v1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/klog/v2"
	"k8s.io/kubernetes/pkg/scheduler/framework"
)

const (
	// Name is the plugin name used in logs and configuration.
	Name = "DRL"
	// stateKey is the key used to store whether we've sent resource info in this cycle
	stateKey = "DRLSchedulerResourceInfoSent"
)

// Ensure we implement the Score extension.
var _ framework.ScorePlugin = &ResourceAwareScorer{}
var _ framework.PreScorePlugin = &ResourceAwareScorer{}

// NodeResourceInfo holds resource information for a node
type NodeResourceInfo struct {
	NodeName        string `json:"nodeName"`
	CPUTotal        int64  `json:"cpuTotal"`        // in millicores
	CPURemaining    int64  `json:"cpuRemaining"`    // in millicores
	MemoryTotal     int64  `json:"memoryTotal"`     // in bytes
	MemoryRemaining int64  `json:"memoryRemaining"` // in bytes
}

// ClusterState holds resource information for all nodes
type ClusterState struct {
	Nodes     []NodeResourceInfo `json:"nodes"`
	Timestamp int64              `json:"timestamp"`
}

// ResourceAwareScorer holds the scheduler handle for accessing the snapshot.
type ResourceAwareScorer struct {
	handle framework.Handle
}

// resourceSentState is a cycle state for tracking if resource info has been sent
type resourceSentState struct {
	sent bool
}

// Clone implements the StateData interface
func (s *resourceSentState) Clone() framework.StateData {
	return &resourceSentState{sent: s.sent}
}

// Name returns the plugin's name.
func (pl *ResourceAwareScorer) Name() string {
	return Name
}

// New initializes the plugin.
func New(_ context.Context, arg runtime.Object, h framework.Handle) (framework.Plugin, error) {
	// You could parse arguments here if needed, similar to the NodeNumber example
	// var args SomeArgsType
	// if arg != nil {
	//     err := frameworkruntime.DecodeInto(arg, &args)
	//     if err != nil {
	//         return nil, fmt.Errorf("failed to decode args: %w", err)
	//     }
	// }

	return &ResourceAwareScorer{handle: h}, nil
}

// calculateNodeResources calculates total and remaining resources for each node
func (pl *ResourceAwareScorer) calculateNodeResources(ctx context.Context) (*ClusterState, error) {
	snapshot := pl.handle.SnapshotSharedLister()
	if snapshot == nil {
		return nil, fmt.Errorf("snapshot is nil")
	}

	nodeInfos, err := snapshot.NodeInfos().List()
	if err != nil {
		return nil, fmt.Errorf("error listing nodes: %v", err)
	}

	clusterState := &ClusterState{
		Nodes:     make([]NodeResourceInfo, 0, len(nodeInfos)),
		Timestamp: time.Now().Unix(),
	}

	for _, nodeInfo := range nodeInfos {
		if nodeInfo == nil || nodeInfo.Node() == nil {
			continue
		}

		node := nodeInfo.Node()

		// Get capacity from node status
		cpuCapacity := node.Status.Capacity.Cpu().MilliValue()
		memCapacity := node.Status.Capacity.Memory().Value()

		// Calculate used resources by summing up all pod requests
		var cpuUsed, memUsed int64
		for _, podInfo := range nodeInfo.Pods {
			if podInfo == nil || podInfo.Pod == nil {
				continue
			}

			for _, container := range podInfo.Pod.Spec.Containers {
				cpuUsed += container.Resources.Requests.Cpu().MilliValue()
				memUsed += container.Resources.Requests.Memory().Value()
			}
		}

		// Calculate remaining resources
		cpuRemaining := cpuCapacity - cpuUsed
		memRemaining := memCapacity - memUsed

		nodeResourceInfo := NodeResourceInfo{
			NodeName:        node.Name,
			CPUTotal:        cpuCapacity,
			CPURemaining:    cpuRemaining,
			MemoryTotal:     memCapacity,
			MemoryRemaining: memRemaining,
		}

		clusterState.Nodes = append(clusterState.Nodes, nodeResourceInfo)
	}

	return clusterState, nil
}

// sendResourceInfoToEndpoint sends the resource information to localhost:5000
func (pl *ResourceAwareScorer) sendResourceInfoToEndpoint(clusterState *ClusterState) error {
	jsonData, err := json.Marshal(clusterState)
	if err != nil {
		return fmt.Errorf("error marshaling resource data: %v", err)
	}

	klog.V(4).Infof("Sending cluster state to endpoint: %s", string(jsonData))

	resp, err := http.Post("http://localhost:5000", "application/json", bytes.NewBuffer(jsonData))
	if err != nil {
		return fmt.Errorf("error sending resource data to endpoint: %v", err)
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		return fmt.Errorf("endpoint returned non-OK status: %d", resp.StatusCode)
	}

	return nil
}

// PreScore is called before Score to calculate and send resource information once per scheduling cycle
func (pl *ResourceAwareScorer) PreScore(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodes []*framework.NodeInfo) *framework.Status {
	klog.InfoS("Execute PreScore on DRL plugin", "pod", klog.KObj(pod))

	// Calculate and send resource information
	clusterState, err := pl.calculateNodeResources(ctx)
	if err != nil {
		klog.ErrorS(err, "Failed to calculate node resources")
		// Continue with scoring even if resource calculation fails
	} else {
		err = pl.sendResourceInfoToEndpoint(clusterState)
		if err != nil {
			klog.ErrorS(err, "Failed to send resource info to endpoint")
			// Continue with scoring even if sending fails
		}
	}

	// Mark that we've sent the resource info in this cycle
	state.Write(stateKey, &resourceSentState{sent: true})

	return nil
}

// EventsToRegister returns the events to register
func (pl *ResourceAwareScorer) EventsToRegister() []framework.ClusterEvent {
	return []framework.ClusterEvent{
		{Resource: framework.Node, ActionType: framework.Add},
		{Resource: framework.Node, ActionType: framework.Update},
		{Resource: framework.Pod, ActionType: framework.Add},
		{Resource: framework.Pod, ActionType: framework.Update},
		{Resource: framework.Pod, ActionType: framework.Delete},
	}
}

// Score generates a random score for the node
func (pl *ResourceAwareScorer) Score(
	ctx context.Context,
	state *framework.CycleState,
	pod *v1.Pod,
	nodeName string,
) (int64, *framework.Status) {
	klog.InfoS("Execute Score on DRL plugin", "pod", klog.KObj(pod), "node", nodeName)

	// Generate a random score between 40 and 70
	rand.Seed(time.Now().UnixNano())
	score := rand.Int63n(31) + 40 // Random number between 40 and 70
	klog.Infof("Node %s scored %d for pod %s/%s", nodeName, score, pod.Namespace, pod.Name)

	return score, framework.NewStatus(framework.Success)
}

// ScoreExtensions returns nil as we don't implement NormalizeScore.
func (pl *ResourceAwareScorer) ScoreExtensions() framework.ScoreExtensions {
	return nil
}
</file>

<file path="kube-scheduler-simulator/simulator/docs/sample/nodenumber/plugin.go">
package nodenumber

import (
	"context"
	"errors"
	"strconv"

	"golang.org/x/xerrors"
	v1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/klog/v2"
	"k8s.io/kubernetes/pkg/scheduler/framework"
	frameworkruntime "k8s.io/kubernetes/pkg/scheduler/framework/runtime"
)

// NodeNumber is an example plugin that favors nodes that have the number suffix which is the same as the number suffix of the pod name.
// But if a reverse option is true, it favors nodes that have the number suffix which **isn't** the same as the number suffix of pod name.
//
// For example:
// With reverse option false, when schedule a pod named Pod1, a Node named Node1 gets a lower score than a node named Node9.
//
// NOTE: this plugin only handle single digit numbers only.
type NodeNumber struct {
	// if reverse is true, it favors nodes that doesn't have the same number suffix.
	//
	// For example:
	// When schedule a pod named Pod1, a Node named Node1 gets a lower score than a node named Node9.
	reverse bool
}

var (
	_ framework.ScorePlugin    = &NodeNumber{}
	_ framework.PreScorePlugin = &NodeNumber{}
)

const (
	// Name is the name of the plugin used in the plugin registry and configurations.
	Name             = "NodeNumber"
	preScoreStateKey = "PreScore" + Name
)

// Name returns the name of the plugin. It is used in logs, etc.
func (pl *NodeNumber) Name() string {
	return Name
}

// preScoreState computed at PreScore and used at Score.
type preScoreState struct {
	podSuffixNumber int
}

// Clone implements the mandatory Clone interface. We don't really copy the data since
// there is no need for that.
func (s *preScoreState) Clone() framework.StateData {
	return s
}

func (pl *NodeNumber) PreScore(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodes []*framework.NodeInfo) *framework.Status {
	klog.InfoS("execute PreScore on NodeNumber plugin", "pod", klog.KObj(pod))

	podNameLastChar := pod.Name[len(pod.Name)-1:]
	podnum, err := strconv.Atoi(podNameLastChar)
	if err != nil {
		// return success even if its suffix is non-number.
		return nil
	}

	s := &preScoreState{
		podSuffixNumber: podnum,
	}
	state.Write(preScoreStateKey, s)

	return nil
}

func (pl *NodeNumber) EventsToRegister() []framework.ClusterEvent {
	return []framework.ClusterEvent{
		{Resource: framework.Node, ActionType: framework.Add},
	}
}

var ErrNotExpectedPreScoreState = errors.New("unexpected pre score state")

// Score invoked at the score extension point.
func (pl *NodeNumber) Score(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeName string) (int64, *framework.Status) {
	klog.InfoS("execute Score on NodeNumber plugin", "pod", klog.KObj(pod))
	data, err := state.Read(preScoreStateKey)
	if err != nil {
		// return success even if there is no value in preScoreStateKey, since the
		// suffix of pod name maybe non-number.
		return 0, nil
	}

	s, ok := data.(*preScoreState)
	if !ok {
		err = xerrors.Errorf("fetched pre score state is not *preScoreState, but %T, %w", data, ErrNotExpectedPreScoreState)
		return 0, framework.AsStatus(err)
	}

	nodeNameLastChar := nodeName[len(nodeName)-1:]

	nodenum, err := strconv.Atoi(nodeNameLastChar)
	if err != nil {
		// return success even if its suffix is non-number.
		return 0, nil
	}

	var matchScore int64 = 10
	var nonMatchScore int64 = 0 //nolint:revive // for better readability.
	if pl.reverse {
		matchScore = 0
		nonMatchScore = 10
	}

	if s.podSuffixNumber == nodenum {
		// if match, node get high score.
		return matchScore, nil
	}

	return nonMatchScore, nil
}

// ScoreExtensions of the Score plugin.
func (pl *NodeNumber) ScoreExtensions() framework.ScoreExtensions {
	return nil
}

// New initializes a new plugin and returns it.
func New(ctx context.Context, arg runtime.Object, h framework.Handle) (framework.Plugin, error) {
	typedArg := NodeNumberArgs{Reverse: false}
	if arg != nil {
		err := frameworkruntime.DecodeInto(arg, &typedArg)
		if err != nil {
			return nil, xerrors.Errorf("decode arg into NodeNumberArgs: %w", err)
		}
		klog.Info("NodeNumberArgs is successfully applied")
	}
	return &NodeNumber{reverse: typedArg.Reverse}, nil
}

// NodeNumberArgs is arguments for node number plugin.
//
//nolint:revive
type NodeNumberArgs struct {
	metav1.TypeMeta

	Reverse bool `json:"reverse"`
}
</file>

<file path="kube-scheduler-simulator/simulator/docs/sample/plugin-extender/docker-compose.yaml">
services:
  extender:
    image: test/test:test   # TODO(user): replace it with your extender's image
    ports:
      - "80:80"
    networks:
      - simulator-internal-network
  init-container:
      image: busybox
      volumes:
        - conf:/config
        - ${PWD}/simulator/cmd/scheduler:/host-config:ro    
      command: sh -c "cp -rf /host-config/* /config/"
  simulator-scheduler:
    image: simulator-scheduler
    container_name: simulator-scheduler
    environment:
      - KUBECONFIG=/config/kubeconfig.yaml
    volumes:
      - conf:/config
    depends_on:
      - init-container
      - simulator-cluster
    restart: always
    tty: true
    networks:
      - simulator-internal-network
  simulator-server:
    image: simulator-server
    container_name: simulator-server
    environment:
      - PORT=1212
      - KUBE_SCHEDULER_SIMULATOR_ETCD_URL=http://simulator-cluster:2379
      - KUBE_APISERVER_URL=http://simulator-cluster:3131
    volumes:
      - ./simulator/config.yaml:/config.yaml
      - ./simulator/kubeconfig.yaml:/kubeconfig.yaml
      - /var/run/docker.sock:/var/run/docker.sock
      - conf:/config
    ports:
      - "1212:1212"
    restart: always
    tty: true
    networks:
      - simulator-internal-network
    depends_on:
      fake-source-cluster:
        condition: "service_healthy"
        required: false
  simulator-frontend:
    image: simulator-frontend
    restart: always
    container_name: simulator-frontend
    environment:
      - HOST=0.0.0.0
      - BASE_URL=http://${SIMULATOR_EXTERNAL_IP:-localhost}:1212
      - KUBE_API_SERVER_URL=http://${SIMULATOR_EXTERNAL_IP:-localhost}:3131
    ports:
    - "3000:3000"
    tty: true
  simulator-cluster:
    image: registry.k8s.io/kwok/cluster:v0.6.0-k8s.v1.30.2
    container_name: simulator-cluster
    restart: always
    ports:
      - "3131:3131"
    volumes:
      - simulator-etcd-data:/var/lib/etcd
      - ./kwok.yaml:/root/.kwok/kwok.yaml
    environment:
      - KWOK_KUBE_APISERVER_PORT=3131
    networks:
      - simulator-internal-network
  fake-source-cluster:
    image: registry.k8s.io/kwok/cluster:v0.6.0-k8s.v1.30.2
    container_name: fake-source-cluster
    restart: always
    healthcheck:
      test: "kwokctl kubectl get nodes"
      start_period: 60s
      start_interval: 1s
      interval: 600s
    ports:
      - "3132:3132"
    environment:
      - KWOK_KUBE_APISERVER_PORT=3132
    networks:
      - simulator-internal-network
    profiles:
      - externalImportEnabled
networks:
  simulator-internal-network:
    driver: bridge
volumes:
  simulator-etcd-data:
  conf:
</file>

<file path="kube-scheduler-simulator/simulator/docs/sample/plugin-extender/extender.go">
package extender

import (
	"context"
	"encoding/json"

	v1 "k8s.io/api/core/v1"
	"k8s.io/klog/v2"
	"k8s.io/kubernetes/pkg/scheduler/framework"
	"k8s.io/kubernetes/pkg/scheduler/framework/plugins/noderesources"

	"sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler/plugin"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/util"
)

type noderesourcefitPreFilterPluginExtender struct {
	handle plugin.SimulatorHandle
}

// New initializes noderesourcefitPreFilterPluginExtender in plugin.PluginExtenders.
func New(handle plugin.SimulatorHandle) plugin.PluginExtenders {
	e := &noderesourcefitPreFilterPluginExtender{
		handle: handle,
	}

	return plugin.PluginExtenders{PreFilterPluginExtender: e} // only PreFilterPluginExtender
}

func (e *noderesourcefitPreFilterPluginExtender) BeforePreFilter(ctx context.Context, state *framework.CycleState, pod *v1.Pod) (*framework.PreFilterResult, *framework.Status) {
	klog.Info("execute BeforePreFilter on noderesourcefitPreFilterPluginExtender", "pod", klog.KObj(pod))
	// do nothing.
	return nil, nil
}

// AfterPreFilter checks what noderesource plugin stores into the cyclestate for this scheduling,
// and store it through the SimulatorHandle.
//
// By this func, each Pod will get noderesourcefit-prefilter-data after scheduling like:
// ---
// kind: Pod
// apiVersion: v1
// metadata:
//
//	name: pod-8ldq5
//	namespace: default
//	annotations:
//	  noderesourcefit-prefilter-data: >-
//	    {"MilliCPU":100,"Memory":17179869184,"EphemeralStorage":0,"AllowedPodNumber":0,"ScalarResources":null}
func (e *noderesourcefitPreFilterPluginExtender) AfterPreFilter(ctx context.Context, state *framework.CycleState, pod *v1.Pod, preFilterResult *framework.PreFilterResult, preFilterStatus *framework.Status) (*framework.PreFilterResult, *framework.Status) {
	klog.Info("execute AfterPreFilter on noderesourcefitPreFilterPluginExtender", "pod", klog.KObj(pod))

	c, err := state.Read("PreFilter" + noderesources.Name)
	if err != nil {
		klog.Info("no state data", "pod", klog.KObj(pod))
		return preFilterResult, preFilterStatus
	}

	// use util.PrivateFieldsDecoder to access private fields of c.
	value := util.PrivateFieldsDecoder(c, "Resource")
	data := value.Interface().(framework.Resource)

	j, err := json.Marshal(data)
	if err != nil {
		klog.Info("json marshal failed in extender", "pod", klog.KObj(pod))
		return preFilterResult, preFilterStatus
	}

	prefilterData := string(j)

	// store data via plugin.SimulatorHandle
	e.handle.AddCustomResult(pod.Namespace, pod.Name, "noderesourcefit-prefilter-data", prefilterData)

	return preFilterResult, preFilterStatus
}
</file>

<file path="kube-scheduler-simulator/simulator/docs/api.md">
# API reference

This page describe the simulator's HTTP API endpoint.

## Get scheduler configuration

get current scheduler configuration.

### HTTP Request

`GET /api/v1/schedulerconfiguration`

### Response

[v1.KubeSchedulerConfiguration](https://github.com/kubernetes/kubernetes/blob/release-1.25/staging/src/k8s.io/kube-scheduler/config/v1/types.go#L43)

| code  | description |
| ----- | -------- |
| 200   | |


## Update scheduler configuration

update scheduler configuration and restart scheduler with new configuration.

### HTTP Request

`POST /api/v1/schedulerconfiguration`

### Request Body

[v1.KubeSchedulerConfiguration](https://github.com/kubernetes/kubernetes/blob/release-1.25/staging/src/k8s.io/kube-scheduler/config/v1/types.go#L43)

### Response

empty

| code  | description |
| ----- | -------- |
| 202   | |
| 500 | something went wrong (see logs of the simulator server) |

## Reset all resources and scheduler configutarion

clean up all resources and restore the initial scheduler configuration.
(If you didn't pass the initial scheduler configuration via `KUBE_SCHEDULER_CONFIG_PATH`, the default scheduler configuration will be restored.)

### HTTP Request

`PUT /api/v1/reset`

### Request Body

empty

### Response

empty

| code  | description |
| ----- | -------- |
| 202   | |
| 500 | something went wrong (see logs of the simulator server) |

## Export

Get all resources and current scheduler configuration.

### HTTP Request

`GET /api/v1/export`


### Response

[ResourcesForLoad](/simulator/server/handler/snapshot.go#L21)

You can find sample requests/responses [here](api-samples/v1/export.md)

| code  | description |
| ----- | -------- |
| 200   | |
| 500 | something went wrong (see logs of the simulator server) |

## Import

Apply resources and scheduler configuration.

### HTTP Request

`POST /api/v1/import`

### Request Body

[ResourcesForLoad](/simulator/server/handler/snapshot.go#L21)

You can find sample requests/responses [here](api-samples/v1/import.md)
### Response

| code  | description |
| ----- | -------- |
| 200   | |
| 500 | something went wrong (see logs of the simulator server) |

## Watch the simulator's resources

Watch individual changes to all k8s resources in the simulator. This endpoint uses `Server-Sent Events`.
Once this API is called, the server will be continuously sending WatchEvent every time the event happens.

### HTTP Request

`GET /api/v1/listwatchresources`

#### Parameter
You can specify the `lastResourceVersion` of each resource, which can be retrieved using the `list` API of each resource.
If you won't specify it, this API calls the `list` and returns the result as "ADDED" Events before starting watch.  

We recommend to call the `list` API before the calling and to use `XXXlastResourceVersion` parameters.

The `ResourceVersion` must be treated as opaque by clients and passed unmodified back to the server.
See also [this page](https://kubernetes.io/docs/reference/using-api/api-concepts/#resource-versions).


| parameter                | requirement | description                                                                                   |
|--------------------------|-------------|-----------------------------------------------------------------------------------------------|
| podslastResourceVersion  | OPTIONAL    | If not specified, all resources are returned as `ADDED` Events first and then start to watch. |
| nodeslastResourceVersion | OPTIONAL    | If not specified, all resources are returned as `ADDED` Events first and then start to watch. |
| pvslastResourceVersion   | OPTIONAL    | If not specified, all resources are returned as `ADDED` Events first and then start to watch. |
| pvcslastResourceVersion  | OPTIONAL    | If not specified, all resources are returned as `ADDED` Events first and then start to watch. |
| scslastResourceVersion   | OPTIONAL    | If not specified, all resources are returned as `ADDED` Events first and then start to watch. |
| pcslastResourceVersion   | OPTIONAL    | If not specified, all resources are returned as `ADDED` Events first and then start to watch. |

e.g.)
```
/api/v1/listwatchresources?podslastResourceVersion=213&nodeslastResourceVersion=213&pvslastResourceVersion=213&pvcslastResourceVersion=213&scslastResourceVersion=213&pcslastResourceVersion=213
```

### Response

[WatchEvent](/simulator/resourcewatcher/streamwriter/streamwriter.go#L18)

| code  | description |
| ----- | -------- |
| 200   | The response is server push. You should catch the WatchEvent and then handle the data each by each.|
</file>

<file path="kube-scheduler-simulator/simulator/docs/debuggable-scheduler.md">
## Debuggable scheduler

The "debuggable scheduler" is the core component of the simulator,
which is responsible for scheduling Pods like the normal scheduler, but also recording all the scheduling details to the Pod annotations.

```yaml
kind: Pod
apiVersion: v1
metadata:
  name: hoge-pod
  annotations:
    kube-scheduler-simulator.sigs.k8s.io/bind-result: '{"DefaultBinder":"success"}'
    kube-scheduler-simulator.sigs.k8s.io/filter-result: >-
      {"node-282x7":{"AzureDiskLimits":"passed","EBSLimits":"passed","GCEPDLimits":"passed","InterPodAffinity":"passed","NodeAffinity":"passed","NodeName":"passed","NodePorts":"passed","NodeResourcesFit":"passed","NodeUnschedulable":"passed","NodeVolumeLimits":"passed","PodTopologySpread":"passed","TaintToleration":"passed","VolumeBinding":"passed","VolumeRestrictions":"passed","VolumeZone":"passed"},"node-gp9t4":{"AzureDiskLimits":"passed","EBSLimits":"passed","GCEPDLimits":"passed","InterPodAffinity":"passed","NodeAffinity":"passed","NodeName":"passed","NodePorts":"passed","NodeResourcesFit":"passed","NodeUnschedulable":"passed","NodeVolumeLimits":"passed","PodTopologySpread":"passed","TaintToleration":"passed","VolumeBinding":"passed","VolumeRestrictions":"passed","VolumeZone":"passed"}}
    kube-scheduler-simulator.sigs.k8s.io/finalscore-result: >-
      {"node-282x7":{"ImageLocality":"0","InterPodAffinity":"0","NodeAffinity":"0","NodeNumber":"0","NodeResourcesBalancedAllocation":"76","NodeResourcesFit":"73","PodTopologySpread":"200","TaintToleration":"300","VolumeBinding":"0"},"node-gp9t4":{"ImageLocality":"0","InterPodAffinity":"0","NodeAffinity":"0","NodeNumber":"0","NodeResourcesBalancedAllocation":"76","NodeResourcesFit":"73","PodTopologySpread":"200","TaintToleration":"300","VolumeBinding":"0"}}
    kube-scheduler-simulator.sigs.k8s.io/permit-result: '{}'
    kube-scheduler-simulator.sigs.k8s.io/permit-result-timeout: '{}'
    kube-scheduler-simulator.sigs.k8s.io/postfilter-result: '{}'
    kube-scheduler-simulator.sigs.k8s.io/prebind-result: '{"VolumeBinding":"success"}'
    kube-scheduler-simulator.sigs.k8s.io/prefilter-result: '{}'
    kube-scheduler-simulator.sigs.k8s.io/prefilter-result-status: >-
      {"InterPodAffinity":"success","NodeAffinity":"success","NodePorts":"success","NodeResourcesFit":"success","PodTopologySpread":"success","VolumeBinding":"success","VolumeRestrictions":"success"}
    kube-scheduler-simulator.sigs.k8s.io/prescore-result: >-
      {"InterPodAffinity":"success","NodeAffinity":"success","NodeNumber":"success","PodTopologySpread":"success","TaintToleration":"success"}
    kube-scheduler-simulator.sigs.k8s.io/reserve-result: '{"VolumeBinding":"success"}'
    kube-scheduler-simulator.sigs.k8s.io/result-history: >-
      [{"noderesourcefit-prefilter-data":"{\"MilliCPU\":100,\"Memory\":17179869184,\"EphemeralStorage\":0,\"AllowedPodNumber\":0,\"ScalarResources\":null}","kube-scheduler-simulator.sigs.k8s.io/bind-result":"{\"DefaultBinder\":\"success\"}","kube-scheduler-simulator.sigs.k8s.io/filter-result":"{\"node-282x7\":{\"AzureDiskLimits\":\"passed\",\"EBSLimits\":\"passed\",\"GCEPDLimits\":\"passed\",\"InterPodAffinity\":\"passed\",\"NodeAffinity\":\"passed\",\"NodeName\":\"passed\",\"NodePorts\":\"passed\",\"NodeResourcesFit\":\"passed\",\"NodeUnschedulable\":\"passed\",\"NodeVolumeLimits\":\"passed\",\"PodTopologySpread\":\"passed\",\"TaintToleration\":\"passed\",\"VolumeBinding\":\"passed\",\"VolumeRestrictions\":\"passed\",\"VolumeZone\":\"passed\"},\"node-gp9t4\":{\"AzureDiskLimits\":\"passed\",\"EBSLimits\":\"passed\",\"GCEPDLimits\":\"passed\",\"InterPodAffinity\":\"passed\",\"NodeAffinity\":\"passed\",\"NodeName\":\"passed\",\"NodePorts\":\"passed\",\"NodeResourcesFit\":\"passed\",\"NodeUnschedulable\":\"passed\",\"NodeVolumeLimits\":\"passed\",\"PodTopologySpread\":\"passed\",\"TaintToleration\":\"passed\",\"VolumeBinding\":\"passed\",\"VolumeRestrictions\":\"passed\",\"VolumeZone\":\"passed\"}}","kube-scheduler-simulator.sigs.k8s.io/finalscore-result":"{\"node-282x7\":{\"ImageLocality\":\"0\",\"InterPodAffinity\":\"0\",\"NodeAffinity\":\"0\",\"NodeNumber\":\"0\",\"NodeResourcesBalancedAllocation\":\"76\",\"NodeResourcesFit\":\"73\",\"PodTopologySpread\":\"200\",\"TaintToleration\":\"300\",\"VolumeBinding\":\"0\"},\"node-gp9t4\":{\"ImageLocality\":\"0\",\"InterPodAffinity\":\"0\",\"NodeAffinity\":\"0\",\"NodeNumber\":\"0\",\"NodeResourcesBalancedAllocation\":\"76\",\"NodeResourcesFit\":\"73\",\"PodTopologySpread\":\"200\",\"TaintToleration\":\"300\",\"VolumeBinding\":\"0\"}}","kube-scheduler-simulator.sigs.k8s.io/permit-result":"{}","kube-scheduler-simulator.sigs.k8s.io/permit-result-timeout":"{}","kube-scheduler-simulator.sigs.k8s.io/postfilter-result":"{}","kube-scheduler-simulator.sigs.k8s.io/prebind-result":"{\"VolumeBinding\":\"success\"}","kube-scheduler-simulator.sigs.k8s.io/prefilter-result":"{}","kube-scheduler-simulator.sigs.k8s.io/prefilter-result-status":"{\"InterPodAffinity\":\"success\",\"NodeAffinity\":\"success\",\"NodePorts\":\"success\",\"NodeResourcesFit\":\"success\",\"PodTopologySpread\":\"success\",\"VolumeBinding\":\"success\",\"VolumeRestrictions\":\"success\"}","kube-scheduler-simulator.sigs.k8s.io/prescore-result":"{\"InterPodAffinity\":\"success\",\"NodeAffinity\":\"success\",\"NodeNumber\":\"success\",\"PodTopologySpread\":\"success\",\"TaintToleration\":\"success\"}","kube-scheduler-simulator.sigs.k8s.io/reserve-result":"{\"VolumeBinding\":\"success\"}","kube-scheduler-simulator.sigs.k8s.io/score-result":"{\"node-282x7\":{\"ImageLocality\":\"0\",\"InterPodAffinity\":\"0\",\"NodeAffinity\":\"0\",\"NodeNumber\":\"0\",\"NodeResourcesBalancedAllocation\":\"76\",\"NodeResourcesFit\":\"73\",\"PodTopologySpread\":\"0\",\"TaintToleration\":\"0\",\"VolumeBinding\":\"0\"},\"node-gp9t4\":{\"ImageLocality\":\"0\",\"InterPodAffinity\":\"0\",\"NodeAffinity\":\"0\",\"NodeNumber\":\"0\",\"NodeResourcesBalancedAllocation\":\"76\",\"NodeResourcesFit\":\"73\",\"PodTopologySpread\":\"0\",\"TaintToleration\":\"0\",\"VolumeBinding\":\"0\"}}","kube-scheduler-simulator.sigs.k8s.io/selected-node":"node-282x7"}]
    kube-scheduler-simulator.sigs.k8s.io/score-result: >-
      {"node-282x7":{"ImageLocality":"0","InterPodAffinity":"0","NodeAffinity":"0","NodeNumber":"0","NodeResourcesBalancedAllocation":"76","NodeResourcesFit":"73","PodTopologySpread":"0","TaintToleration":"0","VolumeBinding":"0"},"node-gp9t4":{"ImageLocality":"0","InterPodAffinity":"0","NodeAffinity":"0","NodeNumber":"0","NodeResourcesBalancedAllocation":"76","NodeResourcesFit":"73","PodTopologySpread":"0","TaintToleration":"0","VolumeBinding":"0"}}
    kube-scheduler-simulator.sigs.k8s.io/selected-node: node-282x7
```

The simulator works with the debuggable scheduler built from the upstream scheduler by default,
and the web UI just visualizes those scheduling details on the annotations.

## Integrate your plugins to the simulator

You can integrate your plugins to the simulator, that is, to the debuggable scheduler working within the simulator, by following these steps:

1. Register your plugins [here](../cmd/scheduler/scheduler.go#L17) in a very similar way you do with the normal scheduler.

```go
    command, cancelFn, err := debuggablescheduler.NewSchedulerCommand(
        debuggablescheduler.WithPlugin(yourcustomplugin.Name, yourcustomplugin.New),
        debuggablescheduler.WithPluginExtenders(noderesources.Name, extender.New), // [optional] see plugin-extender.md about PluginExtender.
    )
```

2. Rebuild the scheduler and restart.

```sh
make docker_build docker_up_local
```

### The plugin extender

We have the plugin extender feature to provide more debuggability from the debuggable scheduler.
See [plugin-extender.md](./plugin-extender.md).

### Use the debuggable scheduler in your dev cluster

The debuggable scheduler can work outside the simulator, that is, in your clusters too.
which allows you to have the same debuggability with your custom scheduler in real clusters.

It's just a single binary that doesn't contain anything but the debuggability features that we described so far.

> [!NOTE]
> We do not recommend using it in the production cluster because the debuggable scheduler contains the overhead.
> Also, you may want to pay attention to the fact that the annotations would be visible to all cluster users,
> which could leak the hints of other tenants to cluster users.

### The example debuggable scheduler

We have the sample to show how to implement the debuggable scheduler in [./sample/debuggable-scheduler](./sample/debuggable-scheduler).
</file>

<file path="kube-scheduler-simulator/simulator/docs/environment-variables.md">
## [deprecated] Environment Variables

**Deprecation notice**: We're planning to remove the configuration via environment variables.
Until deprecation, the simulator will read the configuration in the environment variable first,
if the environment variable is not set, it will read the configuration in the configuration file.
For config file, please refer to the simulator [config.yaml](./../config.yaml).

---

This page describes the environment variables that are used to configure the simulator.

Please refer to [compose.yml](./../../compose.yml) as an example use.

### For Simulator

`PORT`: (required) This is the port number on which kube-scheduler-simulator
server is started.

`KUBE_SCHEDULER_SIMULATOR_ETCD_URL`: (required) This is the URL for
etcd. The simulator runs kube-apiserver internally, and the
kube-apiserver uses this etcd.

`CORS_ALLOWED_ORIGIN_LIST`: This URL represents the URL once web UI is
started. The simulator and internal kube-apiserver set the allowed
origin for `CORS_ALLOWED_ORIGIN_LIST`.

`KUBECONFIG`: This is for the beta feature "Importing cluster's
resources". This variable is used to find Kubeconfig required to
access your cluster for importing resources to scheduler simulator.

`KUBE_APISEVER_URL`: This is the URL of kube-apiserver which the
simulator uses. This variable is used to connect to external kube-apiserver.

`KUBE_SCHEDULER_CONFIG_PATH`: The path to a KubeSchedulerConfiguration
file.  If passed, the simulator will start the scheduler with that
configuration. Or, if you use web UI, you can change the
configuration from the web UI as well.

`EXTERNAL_IMPORT_ENABLED`: This variable indicates whether the simulator
will import resources from an user cluster's or not.
Note, this is still a beta feature.
</file>

<file path="kube-scheduler-simulator/simulator/docs/extender.md">
## Extender

This document describes how to use your `Extenders` in the scheduler running in the simulator.
The `Extender` is one of the features of the scheduler's webhook on kubernetes.

The simulator stores the results of each Extender in the annotation of a pod.

(Note that it's not related to the [`plugin-extender`](./plugin-extender.md) which is one of the our simulator's feature. 
(Sorry for the confusing name 😅))

Note: This feature is not available in [external scheduler](./external-scheduler.md).

## How to use
In this example, we describe how you can run an extender with the simulator, using [k8s-scheduler-extender-example](https://github.com/everpeace/k8s-scheduler-extender-example).

+ Create k8s-scheduler-extender-example's Image: Clone [k8s-scheduler-extender-example](https://github.com/everpeace/k8s-scheduler-extender-example) repository, and follow the step `1 build a docker image` on README.

+ Set up your extender in KubeSchedulerConfiguration either through [`kubeSchedulerConfigPath`](./simulator-server-config.md) or the Web UI.
For example, if you are running the server on http://kube-scheduler-simulator-extender-1:80/scheduler/, your configuration might look like the following:

```yaml
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
leaderElection:
  leaderElect: false
profiles:
  - schedulerName: default-scheduler
extenders:
  - urlPrefix: "http://kube-scheduler-simulator-extender-1:80/scheduler"
    filterVerb: "predicates/always_true"
    prioritizeVerb: "priorities/zero_score"
    preemptVerb: "preemption"
    bindVerb: ""
    weight: 1
    enableHTTPS: false
    nodeCacheCapable: false
```

+ Run Simulator:
We have an example [`docker-compose.yaml`](./example/docker-compose.yaml); you can overwrite the [`docker-compose-local.yaml`](../../docker-compose-local.yml) file with this file, but make sure to update the extender's image name there.

To run the simulator, use the following commands:
```sh
$ make docker_build docker_up_local
```

+ Create a Pod and examine your Extender's Results:
The simulator started with the above steps should have your extender(s) enabled. You can create Pod(s) in the simulator and see the result. 
The result shows up in the Pod's annotations `kube-scheduler-simulator.sigs.k8s.io/extender-xxx` like the following:

```yaml
kind: Pod
apiVersion: v1
metadata:
  name: pod-2rsvz
...
annotations:
      kube-scheduler-simulator.sigs.k8s.io/extender-bind-result: '{}'
      kube-scheduler-simulator.sigs.k8s.io/extender-filter-result: '{"http://kube-scheduler-simulator-extender-1:80/scheduler":{"Nodes":{"metadata":{},"items":[{"metadata":{"name":"node-tzjll","generateName":"node-","uid":"a3e39211-2200-4dee-99a8-a27b2ac528b3","resourceVersion":"223","creationTimestamp":"2024-09-25T12:24:50Z","annotations":{"node.alpha.kubernetes.io/ttl":"0"},"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2024-09-25T12:24:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:node.alpha.kubernetes.io/ttl":{}}}}},{"manager":"simulator","operation":"Update","apiVersion":"v1","time":"2024-09-25T12:24:50Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{}}}}]},"spec":{},"status":{"capacity":{"cpu":"4","memory":"32Gi","pods":"110"},"allocatable":{"cpu":"4","memory":"32Gi","pods":"110"},"phase":"Running","conditions":[{"type":"Ready","status":"True","lastHeartbeatTime":null,"lastTransitionTime":null}],"daemonEndpoints":{"kubeletEndpoint":{"Port":0}},"nodeInfo":{"machineID":"","systemUUID":"","bootID":"","kernelVersion":"","osImage":"","containerRuntimeVersion":"","kubeletVersion":"","kubeProxyVersion":"","operatingSystem":"","architecture":""}}}]},"NodeNames":null,"FailedNodes":{},"FailedAndUnresolvableNodes":null,"Error":""}}'
      kube-scheduler-simulator.sigs.k8s.io/extender-preempt-result: '{}'
      kube-scheduler-simulator.sigs.k8s.io/extender-prioritize-result: '{}'
      ....
```

You can also view the annotation results from the web UI. Simply select the Pod you created and scheduled, then check the Resource Definition section to see the annotations.
</file>

<file path="kube-scheduler-simulator/simulator/docs/external-scheduler.md">
> [!CAUTION]
We previously had a feature called the external scheduler, but we removed it in favor of [debuggable scheduler](../debuggable-scheduler.md).
To maintain compatibility, the externalSchedulerEnabled setting remains for now though, will be removed in the near future.
</file>

<file path="kube-scheduler-simulator/simulator/docs/how-it-works.md">
## How the simulator works

This page describes how the simulator works.

### 0. starts the simulator.

The simulator server works with the [HTTP server](api.md).

In advance, the simulator needs to launch debuggable-scheduler, etcd, controller-manager and kube-apiserver outside.
We recommend using [KWOK](https://github.com/kubernetes-sigs/kwok), see [compose.yml](../../compose.yml) to know how we wire things up.

### 1. users request creating resource.

Users can create resources by communicating with kube-apiserver of KWOK via any clients (e.g. kubectl, k8s client library or [Web UI](../../web))

### 2. the scheduler schedules a new pod.

When a new pod is created through kube-apiserver, the scheduler starts scheduling.

### 3. the results of score/filter plugins are recorded.

Normally, when score/filter plugins are called from scheduler, they will calculate the results and return results to the scheduler.
But, in the simulator, custom plugins, that behave as score/filter plugin but records result after calculation, are used in scheduler.

### 4. the scheduler binds the pod to a node.

The scheduler finally binds the pod to a node if succeeded, or move the pod back to queue if failed.

The result store will notice that the pod has been scheduled/marked as unscheduled by the scheduler and add the scheduling results to the pod's annotation.
</file>

<file path="kube-scheduler-simulator/simulator/docs/import-cluster-resources.md">
# [Beta] Import your real cluster's resources

There are two ways to import resources from your cluster. These methods cannot be used simultaneously.
- Import resources from your cluster once when initializing the simulator.
- Keep importing resources from your cluster.

## One-shot import: Import resources once when initializing the simulator

To use this, you need to follow these two steps in the simulator configuration:
- Set `true` to `externalImportEnabled`.
- Set the path of the kubeconfig file for your cluster to `KubeConfig`. 
  - This feature only requires the read permission for resources.
- [optional] Set a label selector at `resourceImportLabelSelector` if you want to import specific resources only.

```yaml
externalImportEnabled: true
kubeConfig: "/path/to/your-cluster-kubeconfig"
resourceImportLabelSelector:
  matchLabels:
    env: dev
```

## Syncer: Keep importing resources 

To use this, you need to follow these two steps in the scheduler configuration:
- Set `true` to `resourceSyncEnabled`.
- Set the path of the kubeconfig file for the your cluster to `KubeConfig`. 
  - This feature only requires the read permission for resources.

```yaml
resourceSyncEnabled: true
kubeConfig: "/path/to/your-cluster-kubeconfig"
```

> [!NOTE]
> When you enable `resourceSyncEnabled`, adding/updating/deleting resources directly in the simulator cluster could cause a problem of syncing. 
> You can do them for debugging etc purposes though, make sure you reboot the simulator and the fake source cluster afterward.

### How it syncs Pods

We cannot simply sync all changes to Pods, 
because the real cluster has the scheduler, and it schedules all Pods in the cluster.
If we simply synced all changes to Pods, the scheduling result would also be synced, 
and may conflicted with the decision of another scheduler which is in a fake cluster.

So, we don't sync any of updated events to scheduled Pods.
Pods are synced like:

1. In a real cluster, Pod-a is created
2. In a fake cluster, Pod-a is created. (synced)
3. In a real cluster, the scheduler schedules Pod-a to Node-a. We don't copy this change to a fake cluster.
4. In a fake cluster, the scheduler, which is different one from (3), schedules Pod-a to Node-x.

It means that the scheduling results may be different between a real cluster and a fake cluster. 
But, it's OK.
Our purpose is to create a fake cluster for testing the scheduling, which gets the same load as the production cluster.

### Resources to import

It imports the following resources, which the scheduler's default plugins take into account during scheduling.

- Pods
- Nodes
- PersistentVolumes
- PersistentVolumeClaims
- StorageClasses

If you need to, you can tweak which resources to import via the option in [/simulator/cmd/simulator/simulator.go](https://github.com/kubernetes-sigs/kube-scheduler-simulator/blob/master/simulator/cmd/simulator/simulator.go):

```go
dic, err := di.NewDIContainer(..., resourceapplier.Options{
	// GVRsToSync is a list of GroupVersionResource that will be synced.
	// If GVRsToSync is nil, defaultGVRs are used.
	GVRsToSync: []schema.GroupVersionResource{
		{Group: "your-group", Version: "v1", Resource: "your-custom-resources"},
	},

	// Actually, more options are available...

	// FilterBeforeCreating is a list of additional filtering functions that are applied before creating resources.
	FilterBeforeCreating: map[schema.GroupVersionResource][]resourceapplier.FilteringFunction{},
	// MutateBeforeCreating is a list of additional mutating functions that are applied before creating resources.
	MutateBeforeCreating: map[schema.GroupVersionResource][]resourceapplier.MutatingFunction{},
	// FilterBeforeUpdating is a list of additional filtering functions that are applied before updating resources.
	FilterBeforeUpdating: map[schema.GroupVersionResource][]resourceapplier.FilteringFunction{},
	// MutateBeforeUpdating is a list of additional mutating functions that are applied before updating resources.
	MutateBeforeUpdating: map[schema.GroupVersionResource][]resourceapplier.MutatingFunction{},
})
```

> [!NOTE]
> Right now, one-shot import cannot change which resources to import.
</file>

<file path="kube-scheduler-simulator/simulator/docs/integrate-your-scheduler.md">
## Integrate your scheduler 

You can integrate your scheduler into the simulator.

To add your custom scheduler plugins, see [debuggable-scheduler.md](./debuggable-scheduler.md#integrate-your-plugins-to-the-simulator).

Also, if you just want to use your `KubeSchedulerConfig` while using default plugins,
you don't need to follow this page. Check out [simulator-server-config.md](./simulator-server-config.md) instead.
</file>

<file path="kube-scheduler-simulator/simulator/docs/kubeconfig.yaml">
apiVersion: v1
kind: Config

clusters:
  - cluster:
      server: http://localhost:3131
    name: simulator

contexts:
  - context:
      cluster: simulator
    name: simulator

current-context: simulator
</file>

<file path="kube-scheduler-simulator/simulator/docs/plugin-extender.md">
## Plugin extenders

**The plugin extender can be used only with [the external scheduler](./external-scheduler.md).**

The simulator has the concept "Plugin Extenders" which allows you to:
- export plugin's internal state more
- change specific behaviours on particular plugin by injecting the result
- etc...

(Note that it's not related to the scheduler's webhook-based extension which is also called ["extender"](./extender.md). 
(Sorry for the confusing name 😅))

The Plugin Extenders has `BeforeXXX` and `AfterXXX` for each extension point. (XXX = any extension points. e.g., Filter, Score..etc)

For example, `BeforeFilter` is literally called before Filter plugin,
and `AfterFilter` func is called after Filter plugin.

There are multiple interfaces named `XXXXPluginExtender`.

```go
// FilterPluginExtender is the extender for Filter plugin.
type FilterPluginExtender interface {
	// BeforeFilter is a function that runs before the Filter method of the original plugin.
	// If BeforeFilter returns non-success status, the simulator plugin doesn't run the Filter method of the original plugin and return that status.
	BeforeFilter(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo) *framework.Status
	// AfterFilter is a function that is run after the Filter method of the original plugin.
	// A Filter of the simulator plugin finally returns the status returned from AfterFilter.
	AfterFilter(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo, filterResult *framework.Status) *framework.Status
}
```

### export something in each Pod's annotation via `SimulatorHandle`

Each PluginExtender can have `SimulatorHandle`, and you can export some internal state through `SimulatorHandle`.

Example:

```go
func (e *noderesourcefitPreFilterPluginExtender) AfterPreFilter(ctx context.Context, state *framework.CycleState, pod *v1.Pod, preFilterResult *framework.PreFilterResult, preFilterStatus *framework.Status) (*framework.PreFilterResult, *framework.Status) {
	// see ./sample/extender/extender.go
	//...
    e.handle.AddCustomResult(pod.Namespace, pod.Name, "noderesourcefit-prefilter-data", prefilterData)
}
```

If you use the above extender, 
each Pod will get `"noderesourcefit-prefilter-data": prefilterData` annotation in each scheduling like other scheduling results.

### use plugin extender

**Currently, the plugin extender can be used only in [the external scheduler](./external-scheduler.md).**

You can use `debuggablescheduler.WithPluginExtenders` option in `debuggablescheduler.NewSchedulerCommand`
to enable some PluginExtender in particular plugin.

```go
func main() {
	command, cancelFn, err := debuggablescheduler.NewSchedulerCommand(
        debuggablescheduler.WithPluginExtenders(noderesources.Name, extender.New),
    )
    if err != nil {
        klog.Info(fmt.Sprintf("failed to build the scheduler command: %+v", err))
        os.Exit(1)
    }
    code := cli.Run(command)
    cancelFn()
    os.Exit(code)
}
```

### The example plugin extender 

We have the sample plugin extender implementation in [./sample/extender](./sample/plugin-extender).

Please follow [this](./external-scheduler.md#the-example-external-scheduler) 
to see how this sample plugin extender works with the external scheduler.

You will see each Pod gets `noderesourcefit-prefilter-data` annotation along with other scheduling results like this:

```yaml
kind: Pod
apiVersion: v1
metadata:
  name: pod-8ldq5
  namespace: default
  annotations:
    noderesourcefit-prefilter-data: >-
      {"MilliCPU":100,"Memory":17179869184,"EphemeralStorage":0,"AllowedPodNumber":0,"ScalarResources":null}
    kube-scheduler-simulator.sigs.k8s.io/bind-result: '{"DefaultBinder":"success"}'
    kube-scheduler-simulator.sigs.k8s.io/filter-result: >-
      {"node-282x7":{"AzureDiskLimits":"passed","EBSLimits":"passed","GCEPDLimits":"passed","InterPodAffinity":"passed","NodeAffinity":"passed","NodeName":"passed","NodePorts":"passed","NodeResourcesFit":"passed","NodeUnschedulable":"passed","NodeVolumeLimits":"passed","PodTopologySpread":"passed","TaintToleration":"passed","VolumeBinding":"passed","VolumeRestrictions":"passed","VolumeZone":"passed"},"node-gp9t4":{"AzureDiskLimits":"passed","EBSLimits":"passed","GCEPDLimits":"passed","InterPodAffinity":"passed","NodeAffinity":"passed","NodeName":"passed","NodePorts":"passed","NodeResourcesFit":"passed","NodeUnschedulable":"passed","NodeVolumeLimits":"passed","PodTopologySpread":"passed","TaintToleration":"passed","VolumeBinding":"passed","VolumeRestrictions":"passed","VolumeZone":"passed"}}
    kube-scheduler-simulator.sigs.k8s.io/finalscore-result: >-
      {"node-282x7":{"ImageLocality":"0","InterPodAffinity":"0","NodeAffinity":"0","NodeNumber":"0","NodeResourcesBalancedAllocation":"52","NodeResourcesFit":"47","PodTopologySpread":"200","TaintToleration":"300","VolumeBinding":"0"},"node-gp9t4":{"ImageLocality":"0","InterPodAffinity":"0","NodeAffinity":"0","NodeNumber":"0","NodeResourcesBalancedAllocation":"76","NodeResourcesFit":"73","PodTopologySpread":"200","TaintToleration":"300","VolumeBinding":"0"}}
    kube-scheduler-simulator.sigs.k8s.io/permit-result: '{}'
    kube-scheduler-simulator.sigs.k8s.io/permit-result-timeout: '{}'
    kube-scheduler-simulator.sigs.k8s.io/postfilter-result: '{}'
    kube-scheduler-simulator.sigs.k8s.io/prebind-result: '{"VolumeBinding":"success"}'
    kube-scheduler-simulator.sigs.k8s.io/prefilter-result: '{}'
    kube-scheduler-simulator.sigs.k8s.io/prefilter-result-status: >-
      {"InterPodAffinity":"success","NodeAffinity":"success","NodePorts":"success","NodeResourcesFit":"success","PodTopologySpread":"success","VolumeBinding":"success","VolumeRestrictions":"success"}
    kube-scheduler-simulator.sigs.k8s.io/prescore-result: >-
      {"InterPodAffinity":"success","NodeAffinity":"success","NodeNumber":"success","PodTopologySpread":"success","TaintToleration":"success"}
    kube-scheduler-simulator.sigs.k8s.io/reserve-result: '{"VolumeBinding":"success"}'
    kube-scheduler-simulator.sigs.k8s.io/result-history: >-
      [{"noderesourcefit-prefilter-data":"{\"MilliCPU\":100,\"Memory\":17179869184,\"EphemeralStorage\":0,\"AllowedPodNumber\":0,\"ScalarResources\":null}","kube-scheduler-simulator.sigs.k8s.io/bind-result":"{\"DefaultBinder\":\"success\"}","kube-scheduler-simulator.sigs.k8s.io/filter-result":"{\"node-282x7\":{\"AzureDiskLimits\":\"passed\",\"EBSLimits\":\"passed\",\"GCEPDLimits\":\"passed\",\"InterPodAffinity\":\"passed\",\"NodeAffinity\":\"passed\",\"NodeName\":\"passed\",\"NodePorts\":\"passed\",\"NodeResourcesFit\":\"passed\",\"NodeUnschedulable\":\"passed\",\"NodeVolumeLimits\":\"passed\",\"PodTopologySpread\":\"passed\",\"TaintToleration\":\"passed\",\"VolumeBinding\":\"passed\",\"VolumeRestrictions\":\"passed\",\"VolumeZone\":\"passed\"},\"node-gp9t4\":{\"AzureDiskLimits\":\"passed\",\"EBSLimits\":\"passed\",\"GCEPDLimits\":\"passed\",\"InterPodAffinity\":\"passed\",\"NodeAffinity\":\"passed\",\"NodeName\":\"passed\",\"NodePorts\":\"passed\",\"NodeResourcesFit\":\"passed\",\"NodeUnschedulable\":\"passed\",\"NodeVolumeLimits\":\"passed\",\"PodTopologySpread\":\"passed\",\"TaintToleration\":\"passed\",\"VolumeBinding\":\"passed\",\"VolumeRestrictions\":\"passed\",\"VolumeZone\":\"passed\"}}","kube-scheduler-simulator.sigs.k8s.io/finalscore-result":"{\"node-282x7\":{\"ImageLocality\":\"0\",\"InterPodAffinity\":\"0\",\"NodeAffinity\":\"0\",\"NodeNumber\":\"0\",\"NodeResourcesBalancedAllocation\":\"52\",\"NodeResourcesFit\":\"47\",\"PodTopologySpread\":\"200\",\"TaintToleration\":\"300\",\"VolumeBinding\":\"0\"},\"node-gp9t4\":{\"ImageLocality\":\"0\",\"InterPodAffinity\":\"0\",\"NodeAffinity\":\"0\",\"NodeNumber\":\"0\",\"NodeResourcesBalancedAllocation\":\"76\",\"NodeResourcesFit\":\"73\",\"PodTopologySpread\":\"200\",\"TaintToleration\":\"300\",\"VolumeBinding\":\"0\"}}","kube-scheduler-simulator.sigs.k8s.io/permit-result":"{}","kube-scheduler-simulator.sigs.k8s.io/permit-result-timeout":"{}","kube-scheduler-simulator.sigs.k8s.io/postfilter-result":"{}","kube-scheduler-simulator.sigs.k8s.io/prebind-result":"{\"VolumeBinding\":\"success\"}","kube-scheduler-simulator.sigs.k8s.io/prefilter-result":"{}","kube-scheduler-simulator.sigs.k8s.io/prefilter-result-status":"{\"InterPodAffinity\":\"success\",\"NodeAffinity\":\"success\",\"NodePorts\":\"success\",\"NodeResourcesFit\":\"success\",\"PodTopologySpread\":\"success\",\"VolumeBinding\":\"success\",\"VolumeRestrictions\":\"success\"}","kube-scheduler-simulator.sigs.k8s.io/prescore-result":"{\"InterPodAffinity\":\"success\",\"NodeAffinity\":\"success\",\"NodeNumber\":\"success\",\"PodTopologySpread\":\"success\",\"TaintToleration\":\"success\"}","kube-scheduler-simulator.sigs.k8s.io/reserve-result":"{\"VolumeBinding\":\"success\"}","kube-scheduler-simulator.sigs.k8s.io/score-result":"{\"node-282x7\":{\"ImageLocality\":\"0\",\"InterPodAffinity\":\"0\",\"NodeAffinity\":\"0\",\"NodeNumber\":\"0\",\"NodeResourcesBalancedAllocation\":\"52\",\"NodeResourcesFit\":\"47\",\"PodTopologySpread\":\"0\",\"TaintToleration\":\"0\",\"VolumeBinding\":\"0\"},\"node-gp9t4\":{\"ImageLocality\":\"0\",\"InterPodAffinity\":\"0\",\"NodeAffinity\":\"0\",\"NodeNumber\":\"0\",\"NodeResourcesBalancedAllocation\":\"76\",\"NodeResourcesFit\":\"73\",\"PodTopologySpread\":\"0\",\"TaintToleration\":\"0\",\"VolumeBinding\":\"0\"}}","kube-scheduler-simulator.sigs.k8s.io/selected-node":"node-gp9t4"}]
    kube-scheduler-simulator.sigs.k8s.io/score-result: >-
      {"node-282x7":{"ImageLocality":"0","InterPodAffinity":"0","NodeAffinity":"0","NodeNumber":"0","NodeResourcesBalancedAllocation":"52","NodeResourcesFit":"47","PodTopologySpread":"0","TaintToleration":"0","VolumeBinding":"0"},"node-gp9t4":{"ImageLocality":"0","InterPodAffinity":"0","NodeAffinity":"0","NodeNumber":"0","NodeResourcesBalancedAllocation":"76","NodeResourcesFit":"73","PodTopologySpread":"0","TaintToleration":"0","VolumeBinding":"0"}}
    kube-scheduler-simulator.sigs.k8s.io/selected-node: node-gp9t4
```
</file>

<file path="kube-scheduler-simulator/simulator/docs/running-simulator.md">
## Running simulator

The simulator requires Docker installed in your laptop.
We have [compose.yml](../../compose.yml) to run the simulator easily.

### Run simulator with Docker

We have [compose.yml](../../compose.yml) to run the simulator easily.
You should install [Docker](https://docs.docker.com/engine/install/) at first.

You can use either of following commands.

```bash
# pull docker images from the registry and run them.
# It's the easiest way to run up the simulator and web UI.
make docker_up

# build the images for web frontend and simulator server, then start the containers.
# You need to use this if you change the implementation of the simulator.
make docker_build_and_up
```

Then, you can access the simulator with http://localhost:3000.
If you want to deploy the simulator on a remote server and access it via a specific IP (e.g: like http://10.0.0.1:3000/),
please make sure that you have executed `export SIMULATOR_EXTERNAL_IP=your.server.ip` before running `docker compose up -d`.

Additionally, you can run a kwok cluster that acts as a fake source cluster to try out [the resource importing feature](./import-cluster-resources.md).

```
make docker_build_and_up -e COMPOSE_PROFILES=externalImportEnabled
```
</file>

<file path="kube-scheduler-simulator/simulator/docs/simulator-server-config.md">
# Simulator server configuration

Simulator server configuration used to only support setting configurations 
through environment variables, and now adds configurations through configuration files. 
The simulator reads the configuration file in the path of [./config.yaml](./../config.yaml).

```
# This is an example config for scheduler-simulator.

apiVersion: kube-scheduler-simulator-config/v1alpha1
kind: SimulatorConfiguration

# This is the port number on which kube-scheduler-simulator
# server is started.
port: 1212

# This is the URL for etcd. The simulator runs kube-apiserver
# internally, and the kube-apiserver uses this etcd.
etcdURL: "http://127.0.0.1:2379"

# This URL represents the URL once web UI is started.
# The simulator and internal kube-apiserver set the allowed
# origin for CorsAllowedOriginList
corsAllowedOriginList:
  - "http://localhost:3000"

# This is for the beta feature "Importing cluster's resources".
# This variable is used to find Kubeconfig required to access your
# cluster for importing resources to scheduler simulator.
kubeConfig: "/kubeconfig.yaml"

# This is the URL of kube-apiserver which the simulator uses.
# This variable is used to connect to external kube-apiserver.
kubeAPIServerURL: ""

# The path to a KubeSchedulerConfiguration file.
# If passed, the simulator will start the scheduler
# with that configuration. Or, if you use web UI,
# you can change the configuration from the web UI as well.
kubeSchedulerConfigPath: ""

# This variable indicates whether the simulator will
# import resources from a user cluster specified by kubeConfig.
# Note that it only imports the resources once when the simulator is started.
# You cannot make both externalImportEnabled and resourceSyncEnabled true because those features would be conflicted.
# This is still a beta feature.
externalImportEnabled: false

# This variable indicates whether the simulator will
# keep syncing resources from an user cluster's or not.
# You cannot make both externalImportEnabled and resourceSyncEnabled true because those features would be conflicted.
# Note, this is still a beta feature.
resourceSyncEnabled: false
```
</file>

<file path="kube-scheduler-simulator/simulator/errors/errors.go">
package errors

import "errors"

var ErrNotFound = errors.New("resource not found")
</file>

<file path="kube-scheduler-simulator/simulator/hack/boilerplate/boilerplate.go.txt">
/*
Copyright YEAR The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/
</file>

<file path="kube-scheduler-simulator/simulator/hack/etcd.sh">
#!/usr/bin/env bash

SRC_FILE=./config.yaml

kube::etcd::load_config() {
  while IFS= read -r line; do
    if [[ $line == etcdURL:* ]]; then
      KUBE_SCHEDULER_SIMULATOR_ETCD_URL="${line#*:}"
      KUBE_SCHEDULER_SIMULATOR_ETCD_URL=${KUBE_SCHEDULER_SIMULATOR_ETCD_URL// /}
      KUBE_SCHEDULER_SIMULATOR_ETCD_URL=${KUBE_SCHEDULER_SIMULATOR_ETCD_URL//\"/}
    fi
  done <$SRC_FILE
}

kube::etcd::cleanup() {
  kube::etcd::stop
  kube::etcd::clean_etcd_dir
}

kube::etcd::stop() {
  if [[ -n "${ETCD_PID-}" ]]; then
    kill "${ETCD_PID}" &>/dev/null || :
    wait "${ETCD_PID}" &>/dev/null || :
  fi
}

kube::etcd::clean_etcd_dir() {
  if [[ -n "${ETCD_DIR-}" ]]; then
    rm -rf "${ETCD_DIR}"
  fi
}

kube::etcd::start() {
  # Start etcd
  ETCD_DIR=${ETCD_DIR:-$(mktemp -d 2>/dev/null || mktemp -d -t test-etcd.XXXXXX)}
  if [[ -d "${ARTIFACTS:-}" ]]; then
    ETCD_LOGFILE="${ARTIFACTS}/etcd.$(uname -n).$(id -un).log.DEBUG.$(date +%Y%m%d-%H%M%S).$$"
  else
    ETCD_LOGFILE=${ETCD_LOGFILE:-"/dev/null"}
  fi
  echo "etcd --advertise-client-urls ${KUBE_SCHEDULER_SIMULATOR_ETCD_URL} --data-dir ${ETCD_DIR} --listen-client-urls ${KUBE_SCHEDULER_SIMULATOR_ETCD_URL} --log-level=debug > \"${ETCD_LOGFILE}\" 2>/dev/null"
  etcd --advertise-client-urls "${KUBE_SCHEDULER_SIMULATOR_ETCD_URL}" --data-dir "${ETCD_DIR}" --listen-client-urls "${KUBE_SCHEDULER_SIMULATOR_ETCD_URL}" --log-level=debug 2> "${ETCD_LOGFILE}" >/dev/null &
  ETCD_PID=$!

  echo "Waiting for etcd to come up."
  kube::util::wait_for_url "${KUBE_SCHEDULER_SIMULATOR_ETCD_URL}/health" "etcd: " 0.25 80
  curl -fs -X POST "${KUBE_SCHEDULER_SIMULATOR_ETCD_URL}/v3/kv/put" -d '{"key": "X3Rlc3Q=", "value": ""}'
}

kube::util::wait_for_url() {
  local url=$1
  local prefix=${2:-}
  local wait=${3:-1}
  local times=${4:-30}
  local maxtime=${5:-1}

  command -v curl >/dev/null || {
    echo "curl must be installed"
    exit 1
  }

  local i
  for i in $(seq 1 "${times}"); do
    local out
    if out=$(curl --max-time "${maxtime}" -gkfs "${@:6}" "${url}" 2>/dev/null); then
      echo "On try ${i}, ${prefix}: ${out}"
      return 0
    fi
    sleep "${wait}"
  done
  echo "Timed out waiting for ${prefix} to answer at ${url}; tried ${times} waiting ${wait} between each"
  exit 1
}

if [ -z "${KUBE_SCHEDULER_SIMULATOR_ETCD_URL}" ]; then
    kube::etcd::load_config
fi
</file>

<file path="kube-scheduler-simulator/simulator/hack/start_simulator.sh">
#!/usr/bin/env bash

source "./hack/etcd.sh"

check_if_etcd_exists() {
  echo "Checking etcd is on \$PATH"
  which etcd && return
  echo "Cannot find etcd on \$PATH."
  echo "Please see https://git.k8s.io/community/contributors/devel/sig-testing/integration-tests.md#install-etcd-dependency for instructions."
  echo "You can use 'hack/install-etcd.sh'on kubernetes/kubernetes repo to install a copy."
  exit 1
}

CLEANUP_REQUIRED=
start_etcd() {
  echo "Starting etcd instance"
  CLEANUP_REQUIRED=1
  kube::etcd::start
  echo "etcd started"
}

cleanup_etcd() {
  if [[ -z "${CLEANUP_REQUIRED}" ]]; then
    return
  fi
  echo "Cleaning up etcd"
  kube::etcd::cleanup
  CLEANUP_REQUIRED=
  echo "Clean up finished"
}

check_if_etcd_exists

start_etcd
trap cleanup_etcd EXIT

PORT=1212 CORS_ALLOWED_ORIGIN_LIST=http://localhost:3000 ./bin/simulator
</file>

<file path="kube-scheduler-simulator/simulator/hack/tools.go">
//go:build tools
// +build tools

/*
Copyright 2019 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

// This package imports things required by build scripts, to force `go mod` to see them as dependencies
package tools

import (
	_ "k8s.io/code-generator"
)
</file>

<file path="kube-scheduler-simulator/simulator/hack/update-generated-conversions.sh">
#!/bin/bash
source "$(dirname "${BASH_SOURCE}")/lib/init.sh"
source "$(dirname "${BASH_SOURCE}")/lib/generator-help.sh"

go build -o "${OS_OUTPUT_BINPATH}/conversion-gen" "k8s.io/code-generator/cmd/conversion-gen"

${OS_OUTPUT_BINPATH}/conversion-gen \
		--go-header-file "hack/boilerplate/boilerplate.go.txt" \
		--input-dirs "$(find_dirs_containing_comment_tags "+k8s:conversion-gen=")" \
		--output-file-base zz_generated.conversion
</file>

<file path="kube-scheduler-simulator/simulator/hack/update-generated-deep-copies.sh">
#!/bin/bash
source "$(dirname "${BASH_SOURCE}")/lib/init.sh"
source "$(dirname "${BASH_SOURCE}")/lib/generator-help.sh"

go build -o "${OS_OUTPUT_BINPATH}/deepcopy-gen" "k8s.io/code-generator/cmd/deepcopy-gen"

${OS_OUTPUT_BINPATH}/deepcopy-gen \
                --go-header-file "hack/boilerplate/boilerplate.go.txt" \
                --input-dirs "$(find_dirs_containing_comment_tags "+k8s:deepcopy-gen=")" \
                --output-file-base zz_generated.deepcopy
</file>

<file path="kube-scheduler-simulator/simulator/hack/update-generated-defaulters.sh">
#!/bin/bash
source "$(dirname "${BASH_SOURCE}")/lib/init.sh"
source "$(dirname "${BASH_SOURCE}")/lib/generator-help.sh"

go build -o "${OS_OUTPUT_BINPATH}/defaulter-gen" "k8s.io/code-generator/cmd/defaulter-gen"

${OS_OUTPUT_BINPATH}/defaulter-gen \
                --go-header-file "hack/boilerplate/boilerplate.go.txt" \
                --input-dirs "$(find_dirs_containing_comment_tags "+k8s:defaulter-gen=")" \
                --output-file-base zz_generated.defaults
</file>

<file path="kube-scheduler-simulator/simulator/oneshotimporter/importer_test.go">
package oneshotimporter

import (
	"context"
	"testing"

	"github.com/stretchr/testify/assert"
	v1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/runtime/schema"
	"k8s.io/client-go/dynamic/fake"
	"k8s.io/client-go/restmapper"
	scheduling "k8s.io/kubernetes/pkg/apis/scheduling/v1"
	storage "k8s.io/kubernetes/pkg/apis/storage/v1"

	"sigs.k8s.io/kube-scheduler-simulator/simulator/resourceapplier"
)

func TestService_ImportClusterResources(t *testing.T) {
	t.Parallel()

	tests := []struct {
		name               string
		labelSelector      metav1.LabelSelector
		srcObjects         []*unstructured.Unstructured
		importedObjects    []*unstructured.Unstructured
		notImportedObjects []*unstructured.Unstructured
		wantErr            bool
	}{
		{
			name:          "successfully import resources without label selector",
			labelSelector: metav1.LabelSelector{},
			srcObjects: []*unstructured.Unstructured{
				podWithNameAndLabel("pod", nil),
				podWithNameAndLabel("pod2", nil),
			},
			importedObjects: []*unstructured.Unstructured{
				podWithNameAndLabel("pod", nil),
				podWithNameAndLabel("pod2", nil),
			},
			wantErr: false,
		},
		{
			name: "successfully import resources filtered with label selector",
			labelSelector: metav1.LabelSelector{
				MatchLabels: map[string]string{"app": "test"},
			},
			srcObjects: []*unstructured.Unstructured{
				podWithNameAndLabel("test-pod-1", map[string]string{"app": "test"}),
				podWithNameAndLabel("test-pod-2", map[string]string{"app": "test2"}),
				podWithNameAndLabel("test-pod-3", nil),
			},
			importedObjects: []*unstructured.Unstructured{
				podWithNameAndLabel("test-pod-1", map[string]string{"app": "test"}),
			},
			notImportedObjects: []*unstructured.Unstructured{
				podWithNameAndLabel("test-pod-2", map[string]string{"app": "test2"}),
				podWithNameAndLabel("test-pod-3", nil),
			},
			wantErr: false,
		},
	}

	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()

			s := runtime.NewScheme()
			v1.AddToScheme(s)
			storage.AddToScheme(s)
			scheduling.AddToScheme(s)
			srcClient := fake.NewSimpleDynamicClient(s)
			destClient := fake.NewSimpleDynamicClient(s)
			applier := resourceapplier.New(destClient, mapper, resourceapplier.Options{})
			oneshotImporter := NewService(srcClient, applier)
			for _, obj := range tt.srcObjects {
				gvr, err := findGVR(obj)
				assert.NoError(t, err)
				_, err = srcClient.Resource(gvr).Namespace(obj.GetNamespace()).Create(context.Background(), obj, metav1.CreateOptions{})
				assert.NoError(t, err)
			}

			err := oneshotImporter.ImportClusterResources(context.Background(), tt.labelSelector)

			if tt.wantErr {
				assert.Error(t, err)
				return
			}
			assert.NoError(t, err)
			for _, want := range tt.importedObjects {
				gvr, err := findGVR(want)
				assert.NoError(t, err)
				got, err := destClient.Resource(gvr).Namespace(want.GetNamespace()).Get(context.Background(), want.GetName(), metav1.GetOptions{})
				assert.NoError(t, err)
				assert.Equal(t, want.GetName(), got.GetName())
			}
			for _, notWant := range tt.notImportedObjects {
				gvr, err := findGVR(notWant)
				assert.NoError(t, err)
				got, err := destClient.Resource(gvr).Namespace(notWant.GetNamespace()).Get(context.Background(), notWant.GetName(), metav1.GetOptions{})
				assert.Error(t, err)
				assert.Nil(t, got)
			}
		})
	}
}

var mapper = restmapper.NewDiscoveryRESTMapper([]*restmapper.APIGroupResources{
	{
		Group: metav1.APIGroup{
			Versions: []metav1.GroupVersionForDiscovery{
				{Version: "v1"},
			},
		},
		VersionedResources: map[string][]metav1.APIResource{
			"v1": {
				{Name: "pods", Namespaced: true, Kind: "Pod"},
			},
		},
	},
	{
		Group: metav1.APIGroup{
			Versions: []metav1.GroupVersionForDiscovery{
				{Version: "v1"},
			},
		},
		VersionedResources: map[string][]metav1.APIResource{
			"v1": {
				{Name: "nodes", Namespaced: false, Kind: "Node"},
			},
		},
	},
})

func findGVR(obj *unstructured.Unstructured) (schema.GroupVersionResource, error) {
	gvk := obj.GroupVersionKind()
	m, err := mapper.RESTMapping(gvk.GroupKind(), gvk.Version)
	if err != nil {
		return schema.GroupVersionResource{}, err
	}

	return m.Resource, nil
}

func podWithNameAndLabel(name string, labels map[string]string) *unstructured.Unstructured {
	pod := &unstructured.Unstructured{
		Object: map[string]interface{}{
			"apiVersion": "v1",
			"kind":       "Pod",
			"metadata": map[string]interface{}{
				"name":      name,
				"namespace": "default",
			},
			"spec": map[string]interface{}{
				"containers": []interface{}{
					map[string]interface{}{
						"name":  "test-container",
						"image": "test-image",
					},
				},
			},
		},
	}

	if labels != nil {
		pod.SetLabels(labels)
	}

	return pod
}
</file>

<file path="kube-scheduler-simulator/simulator/oneshotimporter/importer.go">
package oneshotimporter

//go:generate mockgen -destination=./mock_$GOPACKAGE/replicate.go . ReplicateService

import (
	"context"
	"fmt"
	"sync"

	"golang.org/x/xerrors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
	"k8s.io/apimachinery/pkg/runtime/schema"
	"k8s.io/client-go/dynamic"
	"k8s.io/klog/v2"

	"sigs.k8s.io/kube-scheduler-simulator/simulator/resourceapplier"
)

// Service has two ReplicateServices.
// importService is used to import(replicate) these resources to the simulator.
// exportService is used to export resources from a target cluster.
type Service struct {
	srcDynamicClient      dynamic.Interface
	resouceApplierService *resourceapplier.Service
	gvrs                  []schema.GroupVersionResource
}

// DefaultGVRs is a list of GroupVersionResource that we import.
// Note that this order matters - When first importing resources, we want to import namespaces first, then priorityclasses, storageclasses...
var DefaultGVRs = []schema.GroupVersionResource{
	{Group: "", Version: "v1", Resource: "namespaces"},
	{Group: "scheduling.k8s.io", Version: "v1", Resource: "priorityclasses"},
	{Group: "storage.k8s.io", Version: "v1", Resource: "storageclasses"},
	{Group: "", Version: "v1", Resource: "persistentvolumeclaims"},
	{Group: "", Version: "v1", Resource: "nodes"},
	{Group: "", Version: "v1", Resource: "persistentvolumes"},
	{Group: "", Version: "v1", Resource: "pods"},
}

// NewService initializes Service.
func NewService(srcClient dynamic.Interface, resourceApplier *resourceapplier.Service) *Service {
	gvrs := DefaultGVRs
	if resourceApplier.GVRsToSync != nil {
		gvrs = resourceApplier.GVRsToSync
	}

	return &Service{
		srcDynamicClient:      srcClient,
		resouceApplierService: resourceApplier,
		gvrs:                  gvrs,
	}
}

// ImportClusterResources gets resources from the target cluster via exportService
// and then apply those resources to the simulator.
// Note: this method doesn't handle scheduler configuration.
// If you want to use the scheduler configuration along with the imported resources on the simulator,
// you need to set the path of the scheduler configuration file to `kubeSchedulerConfigPath` value in the Simulator Server Configuration.
func (s *Service) ImportClusterResources(ctx context.Context, labelSelector metav1.LabelSelector) error {
	for _, gvr := range s.gvrs {
		if err := s.importResource(ctx, gvr, labelSelector); err != nil {
			return xerrors.Errorf("import resource %s: %w", gvr.String(), err)
		}
	}

	return nil
}

func (s *Service) importResource(ctx context.Context, gvr schema.GroupVersionResource, labelSelector metav1.LabelSelector) error {
	selector, err := metav1.LabelSelectorAsSelector(&labelSelector)
	if err != nil {
		return xerrors.Errorf("convert label selector: %w", err)
	}

	resources, err := s.srcDynamicClient.Resource(gvr).List(ctx, metav1.ListOptions{
		LabelSelector: selector.String(),
	})
	if err != nil {
		return xerrors.Errorf("list resources: %w", err)
	}

	var wg sync.WaitGroup
	for _, resource := range resources.Items {
		wg.Add(1)
		fmt.Printf("importing resource: %s\n", resource.GetName())
		go func(r *unstructured.Unstructured) {
			defer wg.Done()
			if err := s.resouceApplierService.Create(ctx, r); err != nil {
				klog.Warningf("failed to import resource: %v", err)
			}
		}(&resource)
	}
	wg.Wait()

	return nil
}
</file>

<file path="kube-scheduler-simulator/simulator/pkg/debuggablescheduler/command.go">
package debuggablescheduler

import (
	"github.com/spf13/cobra"
	"golang.org/x/xerrors"
	"k8s.io/kubernetes/cmd/kube-scheduler/app"
	"k8s.io/kubernetes/pkg/scheduler/framework/runtime"

	simulatorschedulerconfig "sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler/config"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler/extender"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler/plugin"
)

func NewSchedulerCommand(opts ...Option) (*cobra.Command, func(), error) {
	opt := &options{pluginExtender: map[string]plugin.PluginExtenderInitializer{}, outOfTreeRegistry: map[string]runtime.PluginFactory{}}
	for _, o := range opts {
		o(opt)
	}

	if opt.outOfTreeRegistry != nil {
		simulatorschedulerconfig.SetOutOfTreeRegistries(opt.outOfTreeRegistry)
	}

	configs, err := NewConfigs()
	if err != nil {
		return nil, nil, xerrors.Errorf("failed to NewConfigs(): %w", err)
	}

	// Extender service must be initialized using `KubeSchedulerConfiguration.Extenders` config which is not override for simulator (before calling OverrideExtendersCfgToSimulator()).
	// The override will be do within CreateOptions().
	extenderService, err := extender.New(configs.clientSet, configs.versioned.Extenders, configs.sharedStore)
	if err != nil {
		return nil, nil, xerrors.Errorf("failed to New Extender service: %w", err)
	}

	schedulerOpts, cancelFn, err := CreateOptions(configs, opt.pluginExtender)
	if err != nil {
		return nil, cancelFn, err
	}
	// Launch the proxy HTTP server for Extender, which is used to store the Extender's results.
	s := NewExtenderServer(extenderService)
	shutdownFn, err := s.Start(configs.port)
	if err != nil {
		return nil, nil, xerrors.Errorf("start extender proxy server: %w", err)
	}

	cancel := func() {
		cancelFn()
		shutdownFn()
	}
	command := app.NewSchedulerCommand(schedulerOpts...)

	return command, cancel, nil
}

type options struct {
	outOfTreeRegistry runtime.Registry
	pluginExtender    map[string]plugin.PluginExtenderInitializer
}

type Option func(opt *options)

// WithPlugin creates an Option based on plugin name and factory.
func WithPlugin(pluginName string, factory runtime.PluginFactory) Option {
	return func(opt *options) {
		opt.outOfTreeRegistry[pluginName] = factory
	}
}

// WithPluginExtenders creates an Option based on plugin name and plugin extenders.
func WithPluginExtenders(pluginName string, e plugin.PluginExtenderInitializer) Option {
	return func(opt *options) {
		opt.pluginExtender[pluginName] = e
	}
}
</file>

<file path="kube-scheduler-simulator/simulator/pkg/debuggablescheduler/debuggable_scheduler.go">
package debuggablescheduler

import (
	"context"
	"flag"
	"os"

	"golang.org/x/xerrors"
	clientset "k8s.io/client-go/kubernetes"
	restclient "k8s.io/client-go/rest"
	"k8s.io/client-go/tools/clientcmd"
	componentbaseconfig "k8s.io/component-base/config"
	_ "k8s.io/component-base/logs/json/register" // for JSON log format registration
	_ "k8s.io/component-base/metrics/prometheus/clientgo"
	_ "k8s.io/component-base/metrics/prometheus/version" // for version metric registration
	v1 "k8s.io/kube-scheduler/config/v1"
	"k8s.io/kubernetes/cmd/kube-scheduler/app"
	"k8s.io/kubernetes/pkg/scheduler/apis/config"
	"k8s.io/kubernetes/pkg/scheduler/apis/config/scheme"
	configv1 "k8s.io/kubernetes/pkg/scheduler/apis/config/v1"
	"k8s.io/kubernetes/pkg/scheduler/framework/runtime"

	simulatorconfig "sigs.k8s.io/kube-scheduler-simulator/simulator/config"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler"
	simulatorschedulerconfig "sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler/config"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler/extender"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler/plugin"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler/storereflector"
)

type Configs struct {
	versioned   *v1.KubeSchedulerConfiguration
	internalCfg *config.KubeSchedulerConfiguration
	clientSet   *clientset.Clientset
	sharedStore storereflector.Reflector
	port        int
}

// NewConfigs loads flags and initializes kube scheduler configuration and clientSet.
// It does the scheduler config conversion
// - parse each flags.
// - reads the scheduling config passed from users (or use the default config).
// - converts it for enabling wrapped plugins.
// - reads the kubeConfig and creates clientSet to enables storereflector to communicates with the api-server.
// - initialize the store reflector.
func NewConfigs() (Configs, error) {
	// flags defined in the upstream scheduler
	configFile := flag.String("config", "", "")
	master := flag.String("master", "", "")
	// port indicates port number of the proxy server for Extenders.
	// This flag is debuggable_scheduler's own.
	port := flag.Int("proxyPort", 1212, "")
	flag.Parse()

	versionedcfg, err := loadKubeSchedulerConfig(configFile)
	if err != nil {
		return Configs{}, xerrors.Errorf("load scheduler config: %w", err)
	}

	// Register wasm plugins to the wasm registry.
	// This _needs_ to happen before the scheduler configuration is converted.
	if err := simulatorschedulerconfig.RegisterWasmPlugins(versionedcfg); err != nil {
		return Configs{}, xerrors.Errorf("register wasm plugins: %w", err)
	}

	versioned, err := scheduler.ConvertConfigurationForSimulator(versionedcfg)
	if err != nil {
		return Configs{}, xerrors.Errorf("convert scheduler config to apply: %w", err)
	}

	internalCfg, err := scheduler.ConvertSchedulerConfigToInternalConfig(versioned)
	if err != nil {
		return Configs{}, xerrors.Errorf("convert scheduler config to internal one: %w", err)
	}

	clientSet, err := loadKubeConfig(master, internalCfg)
	if err != nil {
		return Configs{}, xerrors.Errorf("load kubeconfig: %w", err)
	}

	return Configs{
		versioned:   versioned,
		internalCfg: internalCfg,
		clientSet:   clientSet,
		sharedStore: storereflector.New(),
		port:        *port,
	}, nil
}

// CreateOptions creates the option which can be help with running the external scheduler
// and resister the storereflector to informer.
// Then, here makes the defaulting func of the KubeSchedulerConfig always returns the converted one.
// We can let the scheduler use the converted configuration under any circumstances because the scheduler will always use this defaulting func to load the configuration.
func CreateOptions(configs Configs, pluginExtender map[string]plugin.PluginExtenderInitializer) ([]app.Option, func(), error) {
	// Override the Extenders config so that the connection is directed to the simulator server.
	extender.OverrideExtendersCfgToSimulator(configs.versioned, configs.port)

	opts, err := CreateOptionForPlugin(pluginExtender, configs.sharedStore, configs.internalCfg)
	if err != nil {
		return nil, nil, xerrors.Errorf("CreateOptionForPlugin: %w", err)
	}

	ctx, cancel := context.WithCancel(context.Background())
	if err := configs.sharedStore.ResisterResultSavingToInformer(configs.clientSet, ctx.Done()); err != nil {
		return nil, cancel, xerrors.Errorf("ResisterResultSavingToInformer of sharedStore: %w", err)
	}

	// black magic: We need to use the scheduler config converted for the simulator in the external scheduler.
	// Here, we overwrite the defaulting func for KubeSchedulerConfiguration,
	// so that user's config will be replaced with the one we created here
	// when the scheduler loads the scheduler config
	// or when loading the default scheduler config.
	scheme.Scheme.AddTypeDefaultingFunc(&v1.KubeSchedulerConfiguration{}, func(obj interface{}) {
		c, ok := obj.(*v1.KubeSchedulerConfiguration)
		if !ok {
			panic("unexpected type")
		}
		configv1.SetObjectDefaults_KubeSchedulerConfiguration(c)
		c.Profiles = configs.versioned.Profiles
		c.Extenders = configs.versioned.Extenders
	})

	return opts, cancel, nil
}

// CreateOptionForPlugin creates Option for in/out of tree plugins.
// It does create the wrapped plugin registries and return the registries as app.Option.
func CreateOptionForPlugin(pluginExtender map[string]plugin.PluginExtenderInitializer, sharedStore storereflector.Reflector, internalCfg *config.KubeSchedulerConfiguration) ([]app.Option, error) {
	// loads in/out of tree plugins and wraps it for debuggable.
	registry, err := plugin.NewRegistry(sharedStore, internalCfg, pluginExtender)
	if err != nil {
		return nil, xerrors.Errorf("convert scheduler config to apply: %w", err)
	}

	return generateWithPluginOptions(registry), nil
}

// loadKubeSchedulerConfig loads specified scheduler config or default one.
func loadKubeSchedulerConfig(configFile *string) (*v1.KubeSchedulerConfiguration, error) {
	var versionedcfg *v1.KubeSchedulerConfiguration
	var err error
	if configFile == nil || *configFile == "" {
		versionedcfg, err = simulatorschedulerconfig.DefaultSchedulerConfig()
		if err != nil {
			return nil, xerrors.Errorf("get default scheduler config: %w", err)
		}
	} else {
		versionedcfg, err = loadConfigFromFile(*configFile)
		if err != nil {
			return nil, xerrors.Errorf("load scheduler config: %w", err)
		}
	}
	return versionedcfg, nil
}

// loadKubeConfig loads kubeConfig.
func loadKubeConfig(master *string, internalCfg *config.KubeSchedulerConfiguration) (*clientset.Clientset, error) {
	kubeconfig, err := simulatorconfig.GetKubeClientConfig()
	if err != nil {
		return nil, xerrors.Errorf("get kubeconfig: %w", err)
	}
	if internalCfg.ClientConnection.Kubeconfig != "" {
		kubeconfig, err = createKubeConfig(internalCfg.ClientConnection, *master)
		if err != nil {
			return nil, xerrors.Errorf("get kubeconfig specified in config: %w", err)
		}
	}
	clientSet, err := clientset.NewForConfig(kubeconfig)
	if err != nil {
		return nil, xerrors.Errorf("creates a new Clientset for kubeconfig: %w", err)
	}
	return clientSet, nil
}

// createKubeConfig creates a kubeConfig from the given config and masterOverride.
func createKubeConfig(config componentbaseconfig.ClientConnectionConfiguration, masterOverride string) (*restclient.Config, error) {
	kubeConfig, err := clientcmd.BuildConfigFromFlags(masterOverride, config.Kubeconfig)
	if err != nil {
		return nil, err
	}

	kubeConfig.DisableCompression = true
	kubeConfig.AcceptContentTypes = config.AcceptContentTypes
	kubeConfig.ContentType = config.ContentType
	kubeConfig.QPS = config.QPS
	kubeConfig.Burst = int(config.Burst)

	return kubeConfig, nil
}

func generateWithPluginOptions(registry map[string]runtime.PluginFactory) []app.Option {
	opt := make([]app.Option, 0, len(registry))
	for k, r := range registry {
		opt = append(opt, app.WithPlugin(k, r))
	}
	return opt
}

func loadConfigFromFile(file string) (*v1.KubeSchedulerConfiguration, error) {
	data, err := os.ReadFile(file)
	if err != nil {
		return nil, err
	}

	return loadConfig(data)
}

func loadConfig(data []byte) (*v1.KubeSchedulerConfiguration, error) {
	// The UniversalDecoder runs defaulting and returns the internal type by default.
	obj, gvk, err := scheme.Codecs.UniversalDecoder().Decode(data, nil, nil)
	if err != nil {
		return nil, err
	}
	if cfgObj, ok := obj.(*config.KubeSchedulerConfiguration); ok {
		// We don't set this field in pkg/scheduler/apis/config/{version}/conversion.go
		// because the field will be cleared later by API machinery during
		// conversion. See KubeSchedulerConfiguration internal type definition for
		// more details.
		cfgObj.TypeMeta.APIVersion = gvk.GroupVersion().String()

		return convertSchedulerConfigToV1Config(cfgObj)
	}
	return nil, xerrors.Errorf("couldn't decode as KubeSchedulerConfiguration, got %s", gvk)
}

func convertSchedulerConfigToV1Config(versioned *config.KubeSchedulerConfiguration) (*v1.KubeSchedulerConfiguration, error) {
	cfg := v1.KubeSchedulerConfiguration{}
	if err := scheme.Scheme.Convert(versioned, &cfg, nil); err != nil {
		return nil, xerrors.Errorf("convert configuration: %w", err)
	}

	return &cfg, nil
}
</file>

<file path="kube-scheduler-simulator/simulator/pkg/debuggablescheduler/server.go">
package debuggablescheduler

import (
	"context"
	"errors"
	"net/http"
	"strconv"
	"time"

	"github.com/labstack/echo/v4"
	"github.com/labstack/echo/v4/middleware"
	"github.com/labstack/gommon/log"

	"sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler/extender"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/server"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/server/handler"
)

// ExtenderServer is proxy server for extender.
type ExtenderServer struct {
	e *echo.Echo
}

// NewExtenderServer initialize ExtenderServer.
// This server is used as a proxy server to store Extender results.
func NewExtenderServer(service *extender.Service) ExtenderServer {
	e := echo.New()
	e.Use(middleware.Logger())

	extenderHandler := handler.NewExtenderHandler(service)
	// register apis
	v1 := e.Group("/api/v1")
	server.RouteExtender(v1, extenderHandler)
	s := ExtenderServer{e: e}
	s.e.Logger.SetLevel(log.INFO)
	return s
}

// Start starts ExtenderServer.
func (s *ExtenderServer) Start(port int) (
	func(), // function for shutdown
	error,
) {
	e := s.e

	go func() {
		if err := e.Start(":" + strconv.Itoa(port)); err != nil && !errors.Is(err, http.ErrServerClosed) {
			e.Logger.Fatalf("failed to start server successfully: %v", err)
		}
	}()
	shutdownFn := func() {
		ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
		defer cancel()
		if err := e.Shutdown(ctx); err != nil {
			e.Logger.Warnf("failed to shutdown simulator server successfully: %v", err)
		}
	}

	return shutdownFn, nil
}
</file>

<file path="kube-scheduler-simulator/simulator/pkg/externalscheduler/command.go">
// Deprecated: it was renamed to the debuggablescheduler plugin. We'll remove it soon.
package externalscheduler

import (
	"github.com/spf13/cobra"
	"k8s.io/kubernetes/cmd/kube-scheduler/app"
	"k8s.io/kubernetes/pkg/scheduler/framework/runtime"

	"sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler/plugin"
)

// Deprecated: the externalscheduler package was renamed to the debuggablescheduler plugin. We'll remove it soon.
func NewSchedulerCommand(opts ...Option) (*cobra.Command, func(), error) {
	opt := &options{pluginExtender: map[string]plugin.PluginExtenderInitializer{}, outOfTreeRegistry: map[string]runtime.PluginFactory{}}
	for _, o := range opts {
		o(opt)
	}

	scheduleropts, cancelFn, err := CreateOptionForOutOfTreePlugin(opt.outOfTreeRegistry, opt.pluginExtender)
	if err != nil {
		return nil, cancelFn, err
	}

	command := app.NewSchedulerCommand(scheduleropts...)

	return command, cancelFn, nil
}

type options struct {
	outOfTreeRegistry runtime.Registry
	pluginExtender    map[string]plugin.PluginExtenderInitializer
}

// Deprecated: the externalscheduler package was renamed to the debuggablescheduler plugin. We'll remove it soon.
type Option func(opt *options)

// WithPlugin creates an Option based on plugin name and factory.
// Deprecated: the externalscheduler package was renamed to the debuggablescheduler plugin. We'll remove it soon.
func WithPlugin(pluginName string, factory runtime.PluginFactory) Option {
	return func(opt *options) {
		opt.outOfTreeRegistry[pluginName] = factory
	}
}

// WithPluginExtenders creates an Option based on plugin name and plugin extenders.
// Deprecated: the externalscheduler package was renamed to the debuggablescheduler plugin. We'll remove it soon.
func WithPluginExtenders(pluginName string, e plugin.PluginExtenderInitializer) Option {
	return func(opt *options) {
		opt.pluginExtender[pluginName] = e
	}
}
</file>

<file path="kube-scheduler-simulator/simulator/pkg/externalscheduler/external_scheduler.go">
package externalscheduler

import (
	"context"
	"flag"
	"os"

	"golang.org/x/xerrors"
	clientset "k8s.io/client-go/kubernetes"
	restclient "k8s.io/client-go/rest"
	"k8s.io/client-go/tools/clientcmd"
	componentbaseconfig "k8s.io/component-base/config"
	_ "k8s.io/component-base/logs/json/register" // for JSON log format registration
	_ "k8s.io/component-base/metrics/prometheus/clientgo"
	_ "k8s.io/component-base/metrics/prometheus/version" // for version metric registration
	v1 "k8s.io/kube-scheduler/config/v1"
	"k8s.io/kubernetes/cmd/kube-scheduler/app"
	"k8s.io/kubernetes/pkg/scheduler/apis/config"
	"k8s.io/kubernetes/pkg/scheduler/apis/config/scheme"
	configv1 "k8s.io/kubernetes/pkg/scheduler/apis/config/v1"
	"k8s.io/kubernetes/pkg/scheduler/framework/runtime"

	simulatorconfig "sigs.k8s.io/kube-scheduler-simulator/simulator/config"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler"
	simulatorschedulerconfig "sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler/config"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler/plugin"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler/storereflector"
)

// CreateOptionForOutOfTreePlugin creates the option which can be help with running the debuggable scheduler.
// It does:
// - create the wrapped plugin registries and return the registries as app.Option
// - initialize and start the store reflector.
// - the scheduler config conversion
//   - reads the scheduling config passed from users (or use the default config)
//   - converts it for enabling wrapped plugins
//   - makes the defaulting func of the KubeSchedulerConfig always returning the converted one. We can let the scheduler use the converted configuration under any circumstances because the scheduler will always use this defaulting func to load the configuration.
//
// Deprecated: the externalscheduler package was renamed to the debuggablescheduler plugin. We'll remove it soon.
//
//nolint:funlen,cyclop
func CreateOptionForOutOfTreePlugin(outOfTreePluginRegistry runtime.Registry, pluginExtender map[string]plugin.PluginExtenderInitializer) ([]app.Option, func(), error) {
	if outOfTreePluginRegistry != nil {
		simulatorschedulerconfig.SetOutOfTreeRegistries(outOfTreePluginRegistry)
	}

	// flags defined in the upstream scheduler
	configFile := flag.String("config", "", "")
	master := flag.String("master", "", "")
	flag.Parse()

	var versionedcfg *v1.KubeSchedulerConfiguration
	var err error
	if configFile == nil {
		versionedcfg, err = simulatorschedulerconfig.DefaultSchedulerConfig()
		if err != nil {
			return nil, nil, xerrors.Errorf("get default scheduler config: %w", err)
		}
	} else {
		versionedcfg, err = loadConfigFromFile(*configFile)
		if err != nil {
			return nil, nil, xerrors.Errorf("load scheduler config: %w", err)
		}
	}

	versioned, err := scheduler.ConvertConfigurationForSimulator(versionedcfg)
	if err != nil {
		return nil, nil, xerrors.Errorf("convert scheduler config to apply: %w", err)
	}

	internalCfg, err := scheduler.ConvertSchedulerConfigToInternalConfig(versioned)
	if err != nil {
		return nil, nil, xerrors.Errorf("convert scheduler config to internal one: %w", err)
	}

	sharedStore := storereflector.New()

	registry, err := plugin.NewRegistry(sharedStore, internalCfg, pluginExtender)
	if err != nil {
		return nil, nil, xerrors.Errorf("convert scheduler config to apply: %w", err)
	}
	kubeconfig, err := simulatorconfig.GetKubeClientConfig()
	if err != nil {
		return nil, nil, xerrors.Errorf("get kubeconfig: %w", err)
	}
	if internalCfg.ClientConnection.Kubeconfig != "" {
		kubeconfig, err = createKubeConfig(internalCfg.ClientConnection, *master)
		if err != nil {
			return nil, nil, xerrors.Errorf("get kubeconfig specified in config: %w", err)
		}
	}
	clientSet, err := clientset.NewForConfig(kubeconfig)
	if err != nil {
		return nil, nil, xerrors.Errorf("creates a new Clientset for kubeconfig: %w", err)
	}

	ctx, cancel := context.WithCancel(context.Background())
	if err := sharedStore.ResisterResultSavingToInformer(clientSet, ctx.Done()); err != nil {
		return nil, cancel, xerrors.Errorf("ResisterResultSavingToInformer of sharedStore: %w", err)
	}

	// black magic: We need to use the scheduler config converted for the simulator in the debuggable scheduler.
	// Here, we overwrite the defaulting func for KubeSchedulerConfiguration,
	// so that user's config will be replaced with the one we created here
	// when the scheduler loads the scheduler config
	// or when loading the default scheduler config.
	scheme.Scheme.AddTypeDefaultingFunc(&v1.KubeSchedulerConfiguration{}, func(obj interface{}) {
		c, ok := obj.(*v1.KubeSchedulerConfiguration)
		if !ok {
			panic("unexpected type")
		}
		configv1.SetObjectDefaults_KubeSchedulerConfiguration(c)
		c.Profiles = versioned.Profiles
	})

	return generateWithPluginOptions(registry), cancel, nil
}

// createKubeConfig creates a kubeConfig from the given config and masterOverride.
func createKubeConfig(config componentbaseconfig.ClientConnectionConfiguration, masterOverride string) (*restclient.Config, error) {
	kubeConfig, err := clientcmd.BuildConfigFromFlags(masterOverride, config.Kubeconfig)
	if err != nil {
		return nil, err
	}

	kubeConfig.DisableCompression = true
	kubeConfig.AcceptContentTypes = config.AcceptContentTypes
	kubeConfig.ContentType = config.ContentType
	kubeConfig.QPS = config.QPS
	kubeConfig.Burst = int(config.Burst)

	return kubeConfig, nil
}

func generateWithPluginOptions(registry map[string]runtime.PluginFactory) []app.Option {
	opt := make([]app.Option, 0, len(registry))
	for k, r := range registry {
		opt = append(opt, app.WithPlugin(k, r))
	}
	return opt
}

func loadConfigFromFile(file string) (*v1.KubeSchedulerConfiguration, error) {
	data, err := os.ReadFile(file)
	if err != nil {
		return nil, err
	}

	return loadConfig(data)
}

func loadConfig(data []byte) (*v1.KubeSchedulerConfiguration, error) {
	// The UniversalDecoder runs defaulting and returns the internal type by default.
	obj, gvk, err := scheme.Codecs.UniversalDecoder().Decode(data, nil, nil)
	if err != nil {
		return nil, err
	}
	if cfgObj, ok := obj.(*config.KubeSchedulerConfiguration); ok {
		// We don't set this field in pkg/scheduler/apis/config/{version}/conversion.go
		// because the field will be cleared later by API machinery during
		// conversion. See KubeSchedulerConfiguration internal type definition for
		// more details.
		cfgObj.TypeMeta.APIVersion = gvk.GroupVersion().String()

		return convertSchedulerConfigToV1Config(cfgObj)
	}
	return nil, xerrors.Errorf("couldn't decode as KubeSchedulerConfiguration, got %s", gvk)
}

func convertSchedulerConfigToV1Config(versioned *config.KubeSchedulerConfiguration) (*v1.KubeSchedulerConfiguration, error) {
	cfg := v1.KubeSchedulerConfiguration{}
	if err := scheme.Scheme.Convert(versioned, &cfg, nil); err != nil {
		return nil, xerrors.Errorf("convert configuration: %w", err)
	}

	return &cfg, nil
}
</file>

<file path="kube-scheduler-simulator/simulator/pkg/README.md">
This directory contains the packages which are intended to be used from the outside of the simulator.
</file>

<file path="kube-scheduler-simulator/simulator/reset/reset.go">
package reset

import (
	"context"
	"errors"

	clientv3 "go.etcd.io/etcd/client/v3"
	"golang.org/x/xerrors"
	clientset "k8s.io/client-go/kubernetes"

	"sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/util"
)

type SchedulerService interface {
	ResetScheduler() error
}

// Service cleans up resources stored in etcd.
type Service struct {
	// initialData has the all resource data that are fetched when reset service is initialized.
	initialData map[string]string

	etcdClient   *clientv3.Client
	k8sClient    clientset.Interface
	schedService SchedulerService
}

const EtcdPrefix = "/kube-scheduler-simulator"

// NewResetService initializes Service.
// ResetService always tries to restore the cluster to the initial state.
func NewResetService(
	etcdClient *clientv3.Client,
	k8sClient clientset.Interface,
	schedService SchedulerService,
) (*Service, error) {
	s := &Service{
		initialData:  map[string]string{},
		etcdClient:   etcdClient,
		k8sClient:    k8sClient,
		schedService: schedService,
	}

	result, err := etcdClient.Get(context.Background(), EtcdPrefix, clientv3.WithPrefix())
	if err != nil {
		return nil, xerrors.Errorf("get all data in etcd: %w", err)
	}

	for _, v := range result.Kvs {
		s.initialData[string(v.Key)] = string(v.Value)
	}

	return s, nil
}

// Reset resets all resources and scheduler configuration to the initial state.
func (s *Service) Reset(ctx context.Context) error {
	if _, err := s.etcdClient.Delete(ctx, EtcdPrefix, clientv3.WithPrefix()); err != nil {
		return xerrors.Errorf("delete all data in etcd: %w", err)
	}

	// restore initial data.
	eg := util.NewErrGroupWithSemaphore(ctx)
	for k, v := range s.initialData {
		k := k
		v := v
		err := eg.Go(func() error {
			if _, err := s.etcdClient.Put(ctx, k, v); err != nil {
				return xerrors.Errorf("put initial data in etcd: key: %s, value: %s, error: %w", k, v, err)
			}
			return nil
		})
		if err != nil {
			return err
		}
	}
	if err := eg.Wait(); err != nil {
		return err
	}
	if err := s.schedService.ResetScheduler(); err != nil && !errors.Is(err, scheduler.ErrServiceDisabled) {
		return xerrors.Errorf("reset scheduler: %w", err)
	}
	return nil
}
</file>

<file path="kube-scheduler-simulator/simulator/resourceapplier/resource.go">
package resourceapplier

import (
	"context"

	v1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/runtime/schema"
	"k8s.io/klog/v2"
)

// mandatoryFilterForCreating is FilteringFunctions that we must register for creating.
// We don't allow users to opt out them.
var mandatoryFilterForCreating = map[schema.GroupVersionResource]FilteringFunction{}

// mandatoryMutateForCreating is MutatingFunctions that we must register for creating.
// We don't allow users to opt out them.
var mandatoryMutateForCreating = map[schema.GroupVersionResource]MutatingFunction{
	{Group: "", Version: "v1", Resource: "persistentvolumes"}: mutatePV,
	{Group: "", Version: "v1", Resource: "pods"}:              mutatePods,
}

// mandatoryFilterForUpdating is FilteringFunctions that we must register.
// We don't allow users to opt out them.
var mandatoryFilterForUpdating = map[schema.GroupVersionResource]FilteringFunction{
	{Group: "", Version: "v1", Resource: "pods"}: filterPodsForUpdating,
}

// mandatoryMutateForUpdating is MutatingFunctions that we must register for updating.
// We don't allow users to opt out them.
var mandatoryMutateForUpdating = map[schema.GroupVersionResource]MutatingFunction{
	{Group: "", Version: "v1", Resource: "persistentvolumes"}: mutatePV,
	{Group: "", Version: "v1", Resource: "pods"}:              mutatePods,
}

func mutatePV(ctx context.Context, resource *unstructured.Unstructured, clients *Clients) (*unstructured.Unstructured, error) {
	var pv v1.PersistentVolume
	err := runtime.DefaultUnstructuredConverter.FromUnstructured(resource.UnstructuredContent(), &pv)
	if err != nil {
		return nil, err
	}

	if pv.Status.Phase == v1.VolumeBound {
		// PersistentVolumeClaims's UID is changed in a destination cluster when importing from a source cluster,
		// and thus we need to update the PVC UID in the PersistentVolume.
		// Get PVC of pv.Spec.ClaimRef.Name.
		pvc, err := clients.DynamicClient.Resource(schema.GroupVersionResource{
			Group:    "",
			Version:  "v1",
			Resource: "persistentvolumeclaims",
		}).Namespace(pv.Spec.ClaimRef.Namespace).Get(ctx, pv.Spec.ClaimRef.Name, metav1.GetOptions{})
		if err != nil {
			return nil, err
		}

		pv.Spec.ClaimRef.UID = pvc.GetUID()
	}

	modifiedUnstructed, err := runtime.DefaultUnstructuredConverter.ToUnstructured(&pv)
	return &unstructured.Unstructured{Object: modifiedUnstructed}, err
}

func mutatePods(_ context.Context, resource *unstructured.Unstructured, _ *Clients) (*unstructured.Unstructured, error) {
	var pod v1.Pod
	err := runtime.DefaultUnstructuredConverter.FromUnstructured(resource.UnstructuredContent(), &pod)
	if err != nil {
		return nil, err
	}

	// Pods must have the default ServiceAccount because ServiceAccount is not synced.
	pod.Spec.ServiceAccountName = ""
	pod.Spec.DeprecatedServiceAccount = ""

	// If the pod has an owner, it may be deleted because resources such as ReplicaSet are not synced.
	pod.OwnerReferences = nil

	modifiedUnstructed, err := runtime.DefaultUnstructuredConverter.ToUnstructured(&pod)
	return &unstructured.Unstructured{Object: modifiedUnstructed}, err
}

// filterPods checks if a pod is already scheduled when it's updated.
// We only want to update pods that are not yet scheduled.
func filterPodsForUpdating(_ context.Context, resource *unstructured.Unstructured, _ *Clients) (bool, error) {
	var pod v1.Pod
	err := runtime.DefaultUnstructuredConverter.FromUnstructured(resource.UnstructuredContent(), &pod)
	if err != nil {
		return false, err
	}

	if pod.Spec.NodeName != "" {
		// We just ignore the not found error because the scheduler may preempt the Pods, or users may remove the resources for debugging.
		klog.Info("Skipped to update resource because we cannot find it in the destination cluster", "resource", klog.KObj(&pod.ObjectMeta))
		return false, nil
	}

	// This Pod should be applied on the destination cluster.
	return true, nil
}
</file>

<file path="kube-scheduler-simulator/simulator/resourceapplier/resourceapplier_test.go">
package resourceapplier

import (
	"context"
	"testing"

	"github.com/google/go-cmp/cmp"
	v1 "k8s.io/api/apps/v1"
	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/api/errors"
	"k8s.io/apimachinery/pkg/api/meta"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/runtime/schema"
	dynamicFake "k8s.io/client-go/dynamic/fake"
	"k8s.io/client-go/restmapper"
	scheduling "k8s.io/kubernetes/pkg/apis/scheduling/v1"
	storage "k8s.io/kubernetes/pkg/apis/storage/v1"
)

func TestResourceApplier_createPods(t *testing.T) {
	t.Parallel()

	tests := []struct {
		name           string
		podToCreate    *corev1.Pod
		podAfterCreate *corev1.Pod
		filter         FilteringFunction
		filtered       bool
		wantErr        bool
	}{
		{
			name: "create a Pod",
			podToCreate: &corev1.Pod{
				TypeMeta: metav1.TypeMeta{
					Kind:       "Pod",
					APIVersion: "v1",
				},
				ObjectMeta: metav1.ObjectMeta{
					Name:      "pod-1",
					Namespace: "default",
				},
				Spec: corev1.PodSpec{
					Containers: []corev1.Container{
						{
							Name:  "container-1",
							Image: "image-1",
						},
					},
				},
			},
			podAfterCreate: &corev1.Pod{
				TypeMeta: metav1.TypeMeta{
					Kind:       "Pod",
					APIVersion: "v1",
				},
				ObjectMeta: metav1.ObjectMeta{
					Name:      "pod-1",
					Namespace: "default",
				},
				Spec: corev1.PodSpec{
					Containers: []corev1.Container{
						{
							Name:  "container-1",
							Image: "image-1",
						},
					},
				},
			},
			filter:   nil,
			filtered: false,
			wantErr:  false,
		},
		{
			name: "create a Pod but it should not be created because of the filter",
			podToCreate: &corev1.Pod{
				TypeMeta: metav1.TypeMeta{
					Kind:       "Pod",
					APIVersion: "v1",
				},
				ObjectMeta: metav1.ObjectMeta{
					Name:      "pod-1",
					Namespace: "default",
					Labels: map[string]string{
						"ignore": "true",
					},
				},
				Spec: corev1.PodSpec{
					Containers: []corev1.Container{
						{
							Name:  "container-1",
							Image: "image-1",
						},
					},
				},
			},
			filter: func(_ context.Context, resource *unstructured.Unstructured, _ *Clients) (bool, error) {
				if resource.GetLabels()["ignore"] == "true" {
					return false, nil
				}
				return true, nil
			},
			filtered: true,
			wantErr:  false,
		},
		{
			name: "create a Pod and it should be pass the filter",
			podToCreate: &corev1.Pod{
				TypeMeta: metav1.TypeMeta{
					Kind:       "Pod",
					APIVersion: "v1",
				},
				ObjectMeta: metav1.ObjectMeta{
					Name:      "pod-1",
					Namespace: "default",
				},
				Spec: corev1.PodSpec{
					Containers: []corev1.Container{
						{
							Name:  "container-1",
							Image: "image-1",
						},
					},
				},
			},
			podAfterCreate: &corev1.Pod{
				TypeMeta: metav1.TypeMeta{
					Kind:       "Pod",
					APIVersion: "v1",
				},
				ObjectMeta: metav1.ObjectMeta{
					Name:      "pod-1",
					Namespace: "default",
				},
				Spec: corev1.PodSpec{
					Containers: []corev1.Container{
						{
							Name:  "container-1",
							Image: "image-1",
						},
					},
				},
			},
			filter: func(_ context.Context, resource *unstructured.Unstructured, _ *Clients) (bool, error) {
				if resource.GetLabels()["ignore"] == "true" {
					return false, nil
				}
				return true, nil
			},
			filtered: false,
			wantErr:  false,
		},
	}

	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()

			client, mapper := prepare()

			options := setFilter(tt.podToCreate.GroupVersionKind(), tt.filter, mapper)
			service := New(client, mapper, options)

			p, err := runtime.DefaultUnstructuredConverter.ToUnstructured(tt.podToCreate)
			if err != nil {
				t.Fatalf("failed to convert pod to unstructured: %v", err)
			}
			unstructedPod := &unstructured.Unstructured{Object: p}
			err = service.Create(context.Background(), unstructedPod)
			if (err != nil) != tt.wantErr {
				t.Errorf("createPods() error = %v, wantErr %v", err, tt.wantErr)
				return
			}

			got, err := getResource(tt.podToCreate.GroupVersionKind(), tt.podToCreate.Name, tt.podToCreate.Namespace, mapper, client)
			if err != nil {
				if tt.filtered && errors.IsNotFound(err) || tt.wantErr {
					return
				}
				t.Fatalf("failed to get pod when comparing: %v", err)
			} else if tt.filtered || tt.wantErr {
				t.Fatalf("pod should not be created but it exists")
			}

			var gotPod corev1.Pod
			err = runtime.DefaultUnstructuredConverter.FromUnstructured(got.UnstructuredContent(), &gotPod)
			if err != nil {
				t.Fatalf("failed to convert got unstructured to pod: %v", err)
			}

			if diff := cmp.Diff(*tt.podAfterCreate, gotPod); diff != "" {
				t.Errorf("createPods() mismatch (-want +got):\n%s", diff)
			}
		})
	}
}

func TestResourceApplier_updatePods(t *testing.T) {
	t.Parallel()

	tests := []struct {
		name           string
		originalPod    *corev1.Pod
		updatePod      func(pod *corev1.Pod)
		podAfterUpdate *corev1.Pod
		wantErr        bool
	}{
		{
			name: "update an unscheduled Pod",
			originalPod: &corev1.Pod{
				TypeMeta: metav1.TypeMeta{
					Kind:       "Pod",
					APIVersion: "v1",
				},
				ObjectMeta: metav1.ObjectMeta{
					Name:      "pod-1",
					Namespace: "default",
				},
				Spec: corev1.PodSpec{
					Containers: []corev1.Container{
						{
							Name:  "container-1",
							Image: "image-1",
						},
					},
				},
			},
			updatePod: func(pod *corev1.Pod) {
				pod.Spec.Containers[0].Image = "image-2"
			},
			podAfterUpdate: &corev1.Pod{
				TypeMeta: metav1.TypeMeta{
					Kind:       "Pod",
					APIVersion: "v1",
				},
				ObjectMeta: metav1.ObjectMeta{
					Name:      "pod-1",
					Namespace: "default",
				},
				Spec: corev1.PodSpec{
					Containers: []corev1.Container{
						{
							Name:  "container-1",
							Image: "image-2",
						},
					},
				},
			},
			wantErr: false,
		},
		{
			name: "update an unscheduled Pod to be scheduled but it should not be updated",
			originalPod: &corev1.Pod{
				TypeMeta: metav1.TypeMeta{
					Kind:       "Pod",
					APIVersion: "v1",
				},
				ObjectMeta: metav1.ObjectMeta{
					Name:      "pod-1",
					Namespace: "default",
				},
				Spec: corev1.PodSpec{
					Containers: []corev1.Container{
						{
							Name:  "container-1",
							Image: "image-1",
						},
					},
				},
			},
			updatePod: func(pod *corev1.Pod) {
				pod.Spec.NodeName = "node-1"
			},
			podAfterUpdate: &corev1.Pod{
				TypeMeta: metav1.TypeMeta{
					Kind:       "Pod",
					APIVersion: "v1",
				},
				ObjectMeta: metav1.ObjectMeta{
					Name:      "pod-1",
					Namespace: "default",
				},
				Spec: corev1.PodSpec{
					Containers: []corev1.Container{
						{
							Name:  "container-1",
							Image: "image-1",
						},
					},
				},
			},
		},
		{
			name: "update a scheduled Pod but it should not be updated",
			originalPod: &corev1.Pod{
				TypeMeta: metav1.TypeMeta{
					Kind:       "Pod",
					APIVersion: "v1",
				},
				ObjectMeta: metav1.ObjectMeta{
					Name:      "pod-1",
					Namespace: "default",
				},
				Spec: corev1.PodSpec{
					NodeName: "node-1",
					Containers: []corev1.Container{
						{
							Name:  "container-1",
							Image: "image-1",
						},
					},
				},
			},
			updatePod: func(pod *corev1.Pod) {
				pod.Spec.Containers[0].Image = "image-2"
			},
			podAfterUpdate: &corev1.Pod{
				TypeMeta: metav1.TypeMeta{
					Kind:       "Pod",
					APIVersion: "v1",
				},
				ObjectMeta: metav1.ObjectMeta{
					Name:      "pod-1",
					Namespace: "default",
				},
				Spec: corev1.PodSpec{
					NodeName: "node-1",
					Containers: []corev1.Container{
						{
							Name:  "container-1",
							Image: "image-1",
						},
					},
				},
			},
			wantErr: false,
		},
	}

	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()

			client, mapper := prepare()
			service := New(client, mapper, Options{})

			p, err := runtime.DefaultUnstructuredConverter.ToUnstructured(tt.originalPod)
			if err != nil {
				t.Fatalf("failed to convert pod to unstructured: %v", err)
			}
			unstructedPod := &unstructured.Unstructured{Object: p}
			err = service.Create(context.Background(), unstructedPod)
			if (err != nil) != tt.wantErr {
				t.Errorf("updatePods() error = %v, wantErr %v", err, tt.wantErr)
				return
			}

			tt.updatePod(tt.originalPod)
			p, err = runtime.DefaultUnstructuredConverter.ToUnstructured(tt.originalPod)
			if err != nil {
				t.Fatalf("failed to convert pod to unstructured: %v", err)
			}
			unstructedPod = &unstructured.Unstructured{Object: p}

			err = service.Update(context.Background(), unstructedPod)
			if (err != nil) != tt.wantErr {
				t.Errorf("updatePods() error = %v, wantErr %v", err, tt.wantErr)
				return
			}

			got, err := getResource(tt.originalPod.GroupVersionKind(), tt.originalPod.Name, tt.originalPod.Namespace, mapper, client)
			if err != nil {
				t.Fatalf("failed to get pod when comparing: %v", err)
			}
			var gotPod corev1.Pod
			err = runtime.DefaultUnstructuredConverter.FromUnstructured(got.UnstructuredContent(), &gotPod)
			if err != nil {
				t.Fatalf("failed to convert got unstructured to pod: %v", err)
			}

			if diff := cmp.Diff(*tt.podAfterUpdate, gotPod); diff != "" {
				t.Errorf("updatePods() mismatch (-want +got):\n%s", diff)
			}
		})
	}
}

func TestResourceApplier_deletePods(t *testing.T) {
	t.Parallel()

	tests := []struct {
		name    string
		pod     *corev1.Pod
		wantErr bool
	}{
		{
			name: "delete a Pod",
			pod: &corev1.Pod{
				TypeMeta: metav1.TypeMeta{
					Kind:       "Pod",
					APIVersion: "v1",
				},
				ObjectMeta: metav1.ObjectMeta{
					Name:      "pod-1",
					Namespace: "default",
				},
				Spec: corev1.PodSpec{
					Containers: []corev1.Container{
						{
							Name:  "container-1",
							Image: "image-1",
						},
					},
				},
			},
			wantErr: false,
		},
	}

	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()

			client, mapper := prepare()
			service := New(client, mapper, Options{})

			p, err := runtime.DefaultUnstructuredConverter.ToUnstructured(tt.pod)
			if err != nil {
				t.Fatalf("failed to convert pod to unstructured: %v", err)
			}
			unstructedPod := &unstructured.Unstructured{Object: p}
			err = service.Create(context.Background(), unstructedPod)
			if (err != nil) != tt.wantErr {
				t.Errorf("deletePods() error = %v, wantErr %v", err, tt.wantErr)
				return
			}

			err = service.Delete(context.Background(), unstructedPod)
			if (err != nil) != tt.wantErr {
				t.Errorf("deletePods() error = %v, wantErr %v", err, tt.wantErr)
				return
			}

			_, err = getResource(tt.pod.GroupVersionKind(), tt.pod.Name, tt.pod.Namespace, mapper, client)
			if err == nil {
				t.Fatalf("pod should be deleted but it still exists")
			}

			if !errors.IsNotFound(err) && !tt.wantErr {
				t.Fatalf("failed to check if pod is deleted: %v", err)
			}
		})
	}
}

func TestResourceApplier_createNodes(t *testing.T) {
	t.Parallel()

	tests := []struct {
		name           string
		nodeToApply    *corev1.Node
		nodeAfterApply *corev1.Node
		wantErr        bool
	}{
		{
			name: "create a Node",
			nodeToApply: &corev1.Node{
				TypeMeta: metav1.TypeMeta{
					Kind:       "Node",
					APIVersion: "v1",
				},
				ObjectMeta: metav1.ObjectMeta{
					Name: "node-1",
				},
			},
			nodeAfterApply: &corev1.Node{
				TypeMeta: metav1.TypeMeta{
					Kind:       "Node",
					APIVersion: "v1",
				},
				ObjectMeta: metav1.ObjectMeta{
					Name: "node-1",
				},
			},
			wantErr: false,
		},
	}

	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()

			client, mapper := prepare()
			service := New(client, mapper, Options{})

			n, err := runtime.DefaultUnstructuredConverter.ToUnstructured(tt.nodeToApply)
			if err != nil {
				t.Fatalf("failed to convert node to unstructured: %v", err)
			}
			unstructedNode := &unstructured.Unstructured{Object: n}
			err = service.Create(context.Background(), unstructedNode)
			if (err != nil) != tt.wantErr {
				t.Errorf("createNode() error = %v, wantErr %v", err, tt.wantErr)
				return
			}

			got, err := getResource(tt.nodeToApply.GroupVersionKind(), tt.nodeToApply.Name, tt.nodeToApply.Namespace, mapper, client)
			if err != nil {
				t.Fatalf("failed to get node when comparing: %v", err)
			}
			var gotNode corev1.Node
			err = runtime.DefaultUnstructuredConverter.FromUnstructured(got.UnstructuredContent(), &gotNode)
			if err != nil {
				t.Fatalf("failed to convert got unstructured to node: %v", err)
			}

			if diff := cmp.Diff(*tt.nodeAfterApply, gotNode); diff != "" {
				t.Errorf("createNode() mismatch (-want +got):\n %s", diff)
				return
			}
		})
	}
}

func prepare() (*dynamicFake.FakeDynamicClient, meta.RESTMapper) {
	s := runtime.NewScheme()
	v1.AddToScheme(s)
	scheduling.AddToScheme(s)
	storage.AddToScheme(s)
	client := dynamicFake.NewSimpleDynamicClient(s)
	resources := []*restmapper.APIGroupResources{
		{
			Group: metav1.APIGroup{
				Versions: []metav1.GroupVersionForDiscovery{
					{Version: "v1"},
				},
			},
			VersionedResources: map[string][]metav1.APIResource{
				"v1": {
					{Name: "pods", Namespaced: true, Kind: "Pod"},
				},
			},
		},
		{
			Group: metav1.APIGroup{
				Versions: []metav1.GroupVersionForDiscovery{
					{Version: "v1"},
				},
			},
			VersionedResources: map[string][]metav1.APIResource{
				"v1": {
					{Name: "nodes", Namespaced: false, Kind: "Node"},
				},
			},
		},
	}

	mapper := restmapper.NewDiscoveryRESTMapper(resources)
	return client, mapper
}

func getResource(gvk schema.GroupVersionKind, name, namespace string, mapper meta.RESTMapper, client *dynamicFake.FakeDynamicClient) (*unstructured.Unstructured, error) {
	mapping, err := mapper.RESTMapping(gvk.GroupKind(), gvk.Version)
	if err != nil {
		return nil, err
	}

	resource := client.Resource(mapping.Resource).Namespace(namespace)
	return resource.Get(context.Background(), name, metav1.GetOptions{})
}

func setFilter(gvk schema.GroupVersionKind, filter FilteringFunction, mapper meta.RESTMapper) Options {
	if filter == nil {
		return Options{}
	}

	m, err := mapper.RESTMapping(gvk.GroupKind(), gvk.Version)
	if err != nil {
		panic(err)
	}

	return Options{
		FilterBeforeCreating: map[schema.GroupVersionResource][]FilteringFunction{
			m.Resource: {filter},
		},
	}
}
</file>

<file path="kube-scheduler-simulator/simulator/resourceapplier/resourceapplier.go">
package resourceapplier

import (
	"context"

	"golang.org/x/xerrors"
	"k8s.io/apimachinery/pkg/api/meta"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
	"k8s.io/apimachinery/pkg/runtime/schema"
	"k8s.io/client-go/dynamic"
)

// FilteringFunction is a function that filters a resource.
// If it returns false, the resource will not be imported.
type FilteringFunction func(ctx context.Context, resource *unstructured.Unstructured, clients *Clients) (bool, error)

// MutatingFunction is a function that mutates a resource before importing it.
type MutatingFunction func(ctx context.Context, resource *unstructured.Unstructured, clients *Clients) (*unstructured.Unstructured, error)

// Note: Clients and its fields are exposed intentionally so that users can use it in MutatingFunction and FilteringFunction.
type Clients struct {
	// DynamicClient is the dynamic client for the destination cluster, which the resource is supposed to be copied to.
	DynamicClient dynamic.Interface
	RestMapper    meta.RESTMapper
}

type Options struct {
	GVRsToSync           []schema.GroupVersionResource
	FilterBeforeCreating map[schema.GroupVersionResource][]FilteringFunction
	MutateBeforeCreating map[schema.GroupVersionResource][]MutatingFunction
	FilterBeforeUpdating map[schema.GroupVersionResource][]FilteringFunction
	MutateBeforeUpdating map[schema.GroupVersionResource][]MutatingFunction
}

type Service struct {
	clients *Clients

	mutateBeforeCreating map[schema.GroupVersionResource][]MutatingFunction
	filterBeforeCreating map[schema.GroupVersionResource][]FilteringFunction
	mutateBeforeUpdating map[schema.GroupVersionResource][]MutatingFunction
	filterBeforeUpdating map[schema.GroupVersionResource][]FilteringFunction

	GVRsToSync []schema.GroupVersionResource
}

func New(dynamicClient dynamic.Interface, restMapper meta.RESTMapper, options Options) *Service {
	s := &Service{
		clients: &Clients{
			DynamicClient: dynamicClient,
			RestMapper:    restMapper,
		},

		filterBeforeCreating: map[schema.GroupVersionResource][]FilteringFunction{},
		mutateBeforeCreating: map[schema.GroupVersionResource][]MutatingFunction{},
		filterBeforeUpdating: map[schema.GroupVersionResource][]FilteringFunction{},
		mutateBeforeUpdating: map[schema.GroupVersionResource][]MutatingFunction{},

		GVRsToSync: options.GVRsToSync,
	}

	for gvr, fn := range mandatoryFilterForCreating {
		s.addFilterBeforeCreating(gvr, []FilteringFunction{fn})
	}
	for gvr, fn := range mandatoryMutateForCreating {
		s.addMutateBeforeCreating(gvr, []MutatingFunction{fn})
	}
	for gvr, fn := range mandatoryFilterForUpdating {
		s.addFilterBeforeUpdating(gvr, []FilteringFunction{fn})
	}
	for gvr, fn := range mandatoryMutateForUpdating {
		s.addMutateBeforeUpdating(gvr, []MutatingFunction{fn})
	}

	for gvr, fns := range options.FilterBeforeCreating {
		s.addFilterBeforeCreating(gvr, fns)
	}
	for gvr, fns := range options.MutateBeforeCreating {
		s.addMutateBeforeCreating(gvr, fns)
	}
	for gvr, fns := range options.FilterBeforeUpdating {
		s.addFilterBeforeUpdating(gvr, fns)
	}
	for gvr, fns := range options.MutateBeforeUpdating {
		s.addMutateBeforeUpdating(gvr, fns)
	}

	return s
}

func (s *Service) Create(ctx context.Context, resource *unstructured.Unstructured) error {
	// Extract the GroupVersionResource from the Unstructured object
	gvk := resource.GroupVersionKind()
	gvr, err := s.findGVRForGVK(gvk)
	if err != nil {
		return err
	}

	// Namespaces resources should be created within the namespace defined in the Unstructured object
	namespace := resource.GetNamespace()

	// Run the filtering function for the resource.
	if ok, err := s.filterResourceForCreating(ctx, gvr, resource, s.clients); !ok || err != nil {
		return err
	}

	// When creating a resource on the destination cluster, we must remove the metadata such as UID and Generation.
	// It's done for all resources.
	resource = removeUnnecessaryMetadata(resource)

	// Run the mutating function for the resource.
	resource, err = s.mutateResourceForCreating(ctx, gvr, resource, s.clients)
	if err != nil {
		return xerrors.Errorf("failed to mutate resource: %w", err)
	}

	// Create the resource on the destination cluster using the dynamic client
	_, err = s.clients.DynamicClient.Resource(gvr).Namespace(namespace).Create(
		ctx,
		resource,
		metav1.CreateOptions{},
	)
	if err != nil {
		return xerrors.Errorf("failed to create resource: %w", err)
	}

	return nil
}

func (s *Service) Update(ctx context.Context, resource *unstructured.Unstructured) error {
	// Extract the GroupVersionResource from the Unstructured object
	gvk := resource.GroupVersionKind()
	gvr, err := s.findGVRForGVK(gvk)
	if err != nil {
		return err
	}

	// Namespaces resources should be created within the namespace defined in the Unstructured object
	namespace := resource.GetNamespace()

	// Run the filtering function for the resource.
	if ok, err := s.filterResourceForUpdating(ctx, gvr, resource, s.clients); !ok || err != nil {
		return err
	}

	// When updating a resource on the destination cluster, we must remove the metadata such as UID and Generation.
	// It's done for all resources.
	resource = removeUnnecessaryMetadata(resource)

	// Run the mutating function for the resource.
	resource, err = s.mutateResourceForUpdating(ctx, gvr, resource, s.clients)
	if err != nil {
		return xerrors.Errorf("failed to mutate resource: %w", err)
	}

	// Update the resource on the destination cluster using the dynamic client
	_, err = s.clients.DynamicClient.Resource(gvr).Namespace(namespace).Update(
		ctx,
		resource,
		metav1.UpdateOptions{},
	)
	if err != nil {
		return xerrors.Errorf("failed to update resource: %w", err)
	}

	return nil
}

func (s *Service) Delete(
	ctx context.Context,
	resource *unstructured.Unstructured,
) error {
	// Extract the GroupVersionResource from the Unstructured object
	gvk := resource.GroupVersionKind()
	gvr, err := s.findGVRForGVK(gvk)
	if err != nil {
		return err
	}

	// Namespaces resources should be created within the namespace defined in the Unstructured object
	namespace := resource.GetNamespace()

	// Create the resource on the destination cluster using the dynamic client
	err = s.clients.DynamicClient.Resource(gvr).Namespace(namespace).Delete(
		ctx,
		resource.GetName(),
		metav1.DeleteOptions{},
	)
	if err != nil {
		return xerrors.Errorf("failed to delete resource: %w", err)
	}

	return nil
}

func (s *Service) filterResourceForCreating(ctx context.Context, gvr schema.GroupVersionResource, resource *unstructured.Unstructured, clients *Clients) (bool, error) {
	filteringFns, ok := s.filterBeforeCreating[gvr]
	if !ok {
		return true, nil
	}

	for _, filteringFn := range filteringFns {
		ok, err := filteringFn(ctx, resource, clients)
		if err != nil {
			return false, err
		}
		if !ok {
			return false, nil
		}
	}

	return true, nil
}

func (s *Service) mutateResourceForCreating(ctx context.Context, gvr schema.GroupVersionResource, resource *unstructured.Unstructured, clients *Clients) (*unstructured.Unstructured, error) {
	mutatingFns, ok := s.mutateBeforeCreating[gvr]
	if !ok {
		return resource, nil
	}

	for _, mutatingFn := range mutatingFns {
		modifiedResource, err := mutatingFn(ctx, resource, clients)
		if err != nil {
			return nil, err
		}
		resource = modifiedResource
	}

	return resource, nil
}

func (s *Service) filterResourceForUpdating(ctx context.Context, gvr schema.GroupVersionResource, resource *unstructured.Unstructured, clients *Clients) (bool, error) {
	filteringFns, ok := s.filterBeforeUpdating[gvr]
	if !ok {
		return true, nil
	}

	for _, filteringFn := range filteringFns {
		ok, err := filteringFn(ctx, resource, clients)
		if err != nil {
			return false, err
		}
		if !ok {
			return false, nil
		}
	}

	return true, nil
}

func (s *Service) mutateResourceForUpdating(ctx context.Context, gvr schema.GroupVersionResource, resource *unstructured.Unstructured, clients *Clients) (*unstructured.Unstructured, error) {
	mutatingFns, ok := s.mutateBeforeUpdating[gvr]
	if !ok {
		return resource, nil
	}

	for _, mutatingFn := range mutatingFns {
		modifiedResource, err := mutatingFn(ctx, resource, clients)
		if err != nil {
			return nil, err
		}
		resource = modifiedResource
	}

	return resource, nil
}

// findGVRForGVK uses the discovery client to get the GroupVersionResource for a given GroupVersionKind.
func (s *Service) findGVRForGVK(gvk schema.GroupVersionKind) (schema.GroupVersionResource, error) {
	m, err := s.clients.RestMapper.RESTMapping(gvk.GroupKind(), gvk.Version)
	if err != nil {
		return schema.GroupVersionResource{}, err
	}

	return m.Resource, nil
}

// removeUnnecessaryMetadata removes the metadata from the resource.
func removeUnnecessaryMetadata(resource *unstructured.Unstructured) *unstructured.Unstructured {
	resource.SetUID("")
	resource.SetGeneration(0)
	resource.SetResourceVersion("")

	return resource
}

func (s *Service) addFilterBeforeCreating(gvr schema.GroupVersionResource, fn []FilteringFunction) {
	if _, ok := s.filterBeforeCreating[gvr]; !ok {
		s.filterBeforeCreating[gvr] = []FilteringFunction{}
	}

	s.filterBeforeCreating[gvr] = append(s.filterBeforeCreating[gvr], fn...)
}

func (s *Service) addMutateBeforeCreating(gvr schema.GroupVersionResource, fn []MutatingFunction) {
	if _, ok := s.mutateBeforeCreating[gvr]; !ok {
		s.mutateBeforeCreating[gvr] = []MutatingFunction{}
	}

	s.mutateBeforeCreating[gvr] = append(s.mutateBeforeCreating[gvr], fn...)
}

func (s *Service) addFilterBeforeUpdating(gvr schema.GroupVersionResource, fn []FilteringFunction) {
	if _, ok := s.filterBeforeUpdating[gvr]; !ok {
		s.filterBeforeUpdating[gvr] = []FilteringFunction{}
	}

	s.filterBeforeUpdating[gvr] = append(s.filterBeforeUpdating[gvr], fn...)
}

func (s *Service) addMutateBeforeUpdating(gvr schema.GroupVersionResource, fn []MutatingFunction) {
	if _, ok := s.mutateBeforeUpdating[gvr]; !ok {
		s.mutateBeforeUpdating[gvr] = []MutatingFunction{}
	}

	s.mutateBeforeUpdating[gvr] = append(s.mutateBeforeUpdating[gvr], fn...)
}
</file>

<file path="kube-scheduler-simulator/simulator/resourcewatcher/mock_resourcewatcher/lister.go">
// Code generated by MockGen. DO NOT EDIT.
// Source: k8s.io/client-go/tools/cache (interfaces: Lister)
//
// Generated by this command:
//
//	mockgen -destination=./mock_resourcewatcher/lister.go -package=mock_resourcewatcher -mock_names Interface=MockListerInterface k8s.io/client-go/tools/cache Lister
//

// Package mock_resourcewatcher is a generated GoMock package.
package mock_resourcewatcher

import (
	reflect "reflect"

	gomock "go.uber.org/mock/gomock"
	v1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	runtime "k8s.io/apimachinery/pkg/runtime"
)

// MockLister is a mock of Lister interface.
type MockLister struct {
	ctrl     *gomock.Controller
	recorder *MockListerMockRecorder
	isgomock struct{}
}

// MockListerMockRecorder is the mock recorder for MockLister.
type MockListerMockRecorder struct {
	mock *MockLister
}

// NewMockLister creates a new mock instance.
func NewMockLister(ctrl *gomock.Controller) *MockLister {
	mock := &MockLister{ctrl: ctrl}
	mock.recorder = &MockListerMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockLister) EXPECT() *MockListerMockRecorder {
	return m.recorder
}

// List mocks base method.
func (m *MockLister) List(options v1.ListOptions) (runtime.Object, error) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "List", options)
	ret0, _ := ret[0].(runtime.Object)
	ret1, _ := ret[1].(error)
	return ret0, ret1
}

// List indicates an expected call of List.
func (mr *MockListerMockRecorder) List(options any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "List", reflect.TypeOf((*MockLister)(nil).List), options)
}
</file>

<file path="kube-scheduler-simulator/simulator/resourcewatcher/mock_resourcewatcher/streamWriter.go">
// Code generated by MockGen. DO NOT EDIT.
// Source: sigs.k8s.io/kube-scheduler-simulator/simulator/resourcewatcher (interfaces: StreamWriter)
//
// Generated by this command:
//
//	mockgen -destination=./mock_resourcewatcher/streamWriter.go . StreamWriter
//

// Package mock_resourcewatcher is a generated GoMock package.
package mock_resourcewatcher

import (
	reflect "reflect"

	gomock "go.uber.org/mock/gomock"

	streamwriter "sigs.k8s.io/kube-scheduler-simulator/simulator/resourcewatcher/streamwriter"
)

// MockStreamWriter is a mock of StreamWriter interface.
type MockStreamWriter struct {
	ctrl     *gomock.Controller
	recorder *MockStreamWriterMockRecorder
	isgomock struct{}
}

// MockStreamWriterMockRecorder is the mock recorder for MockStreamWriter.
type MockStreamWriterMockRecorder struct {
	mock *MockStreamWriter
}

// NewMockStreamWriter creates a new mock instance.
func NewMockStreamWriter(ctrl *gomock.Controller) *MockStreamWriter {
	mock := &MockStreamWriter{ctrl: ctrl}
	mock.recorder = &MockStreamWriterMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockStreamWriter) EXPECT() *MockStreamWriterMockRecorder {
	return m.recorder
}

// Write mocks base method.
func (m *MockStreamWriter) Write(we *streamwriter.WatchEvent) error {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Write", we)
	ret0, _ := ret[0].(error)
	return ret0
}

// Write indicates an expected call of Write.
func (mr *MockStreamWriterMockRecorder) Write(we any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Write", reflect.TypeOf((*MockStreamWriter)(nil).Write), we)
}
</file>

<file path="kube-scheduler-simulator/simulator/resourcewatcher/mock_resourcewatcher/watchInterface.go">
// Code generated by MockGen. DO NOT EDIT.
// Source: k8s.io/apimachinery/pkg/watch (interfaces: Interface)
//
// Generated by this command:
//
//	mockgen -destination=./mock_resourcewatcher/watchInterface.go -package=mock_resourcewatcher -mock_names Interface=MockWatchInterface k8s.io/apimachinery/pkg/watch Interface
//

// Package mock_resourcewatcher is a generated GoMock package.
package mock_resourcewatcher

import (
	reflect "reflect"

	gomock "go.uber.org/mock/gomock"
	watch "k8s.io/apimachinery/pkg/watch"
)

// MockWatchInterface is a mock of Interface interface.
type MockWatchInterface struct {
	ctrl     *gomock.Controller
	recorder *MockWatchInterfaceMockRecorder
	isgomock struct{}
}

// MockWatchInterfaceMockRecorder is the mock recorder for MockWatchInterface.
type MockWatchInterfaceMockRecorder struct {
	mock *MockWatchInterface
}

// NewMockWatchInterface creates a new mock instance.
func NewMockWatchInterface(ctrl *gomock.Controller) *MockWatchInterface {
	mock := &MockWatchInterface{ctrl: ctrl}
	mock.recorder = &MockWatchInterfaceMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockWatchInterface) EXPECT() *MockWatchInterfaceMockRecorder {
	return m.recorder
}

// ResultChan mocks base method.
func (m *MockWatchInterface) ResultChan() <-chan watch.Event {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "ResultChan")
	ret0, _ := ret[0].(<-chan watch.Event)
	return ret0
}

// ResultChan indicates an expected call of ResultChan.
func (mr *MockWatchInterfaceMockRecorder) ResultChan() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "ResultChan", reflect.TypeOf((*MockWatchInterface)(nil).ResultChan))
}

// Stop mocks base method.
func (m *MockWatchInterface) Stop() {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "Stop")
}

// Stop indicates an expected call of Stop.
func (mr *MockWatchInterfaceMockRecorder) Stop() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Stop", reflect.TypeOf((*MockWatchInterface)(nil).Stop))
}
</file>

<file path="kube-scheduler-simulator/simulator/resourcewatcher/streamwriter/mock_streamwriter/responseStream.go">
// Code generated by MockGen. DO NOT EDIT.
// Source: sigs.k8s.io/kube-scheduler-simulator/simulator/resourcewatcher/streamwriter (interfaces: ResponseStream)
//
// Generated by this command:
//
//	mockgen -destination=./mock_streamwriter/responseStream.go . ResponseStream
//

// Package mock_streamwriter is a generated GoMock package.
package mock_streamwriter

import (
	reflect "reflect"

	gomock "go.uber.org/mock/gomock"
)

// MockResponseStream is a mock of ResponseStream interface.
type MockResponseStream struct {
	ctrl     *gomock.Controller
	recorder *MockResponseStreamMockRecorder
	isgomock struct{}
}

// MockResponseStreamMockRecorder is the mock recorder for MockResponseStream.
type MockResponseStreamMockRecorder struct {
	mock *MockResponseStream
}

// NewMockResponseStream creates a new mock instance.
func NewMockResponseStream(ctrl *gomock.Controller) *MockResponseStream {
	mock := &MockResponseStream{ctrl: ctrl}
	mock.recorder = &MockResponseStreamMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockResponseStream) EXPECT() *MockResponseStreamMockRecorder {
	return m.recorder
}

// Flush mocks base method.
func (m *MockResponseStream) Flush() {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "Flush")
}

// Flush indicates an expected call of Flush.
func (mr *MockResponseStreamMockRecorder) Flush() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Flush", reflect.TypeOf((*MockResponseStream)(nil).Flush))
}

// Write mocks base method.
func (m *MockResponseStream) Write(p []byte) (int, error) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Write", p)
	ret0, _ := ret[0].(int)
	ret1, _ := ret[1].(error)
	return ret0, ret1
}

// Write indicates an expected call of Write.
func (mr *MockResponseStreamMockRecorder) Write(p any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Write", reflect.TypeOf((*MockResponseStream)(nil).Write), p)
}
</file>

<file path="kube-scheduler-simulator/simulator/resourcewatcher/streamwriter/streamwriter_test.go">
package streamwriter

import (
	"testing"

	"go.uber.org/mock/gomock"
	"golang.org/x/xerrors"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/watch"

	"sigs.k8s.io/kube-scheduler-simulator/simulator/resourcewatcher/streamwriter/mock_streamwriter"
)

const (
	Pods ResourceKind = "pods"
)

var (
	dummyWatchEvent1 = WatchEvent{
		Kind:      Pods,
		EventType: watch.Added,
		Obj:       Pod1,
	}
	Pod1 = corev1.Pod{
		ObjectMeta: metav1.ObjectMeta{
			Name: "pod1",
		},
	}
)

func TestStreamWriter_Writer(t *testing.T) {
	t.Parallel()
	tests := []struct {
		name                        string
		prepareResponseStreamMockFn func(ws *mock_streamwriter.MockResponseStream)
		wantErr                     bool
	}{
		{
			name: "should success when ResponseWriter's Write method returns no error",
			prepareResponseStreamMockFn: func(ws *mock_streamwriter.MockResponseStream) {
				ws.EXPECT().Flush()
				ws.EXPECT().Write(gomock.Any()).Return(0, nil)
			},
			wantErr: false,
		},
		{
			name: "should failed when ResponseWriter's Write method returns an error",
			prepareResponseStreamMockFn: func(ws *mock_streamwriter.MockResponseStream) {
				ws.EXPECT().Write(gomock.Any()).Return(0, xerrors.Errorf("call write"))
			},
			wantErr: true,
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			ctrl := gomock.NewController(t)
			mockResponseStream := mock_streamwriter.NewMockResponseStream(ctrl)

			sw := NewStreamWriter(mockResponseStream)
			tt.prepareResponseStreamMockFn(mockResponseStream)

			if err := sw.Write(&dummyWatchEvent1); (err != nil) != tt.wantErr {
				t.Fatalf("Writer %v test, \nerror = %v", tt.name, err)
			}
		})
	}
}
</file>

<file path="kube-scheduler-simulator/simulator/resourcewatcher/streamwriter/streamwriter.go">
package streamwriter

//go:generate mockgen -destination=./mock_$GOPACKAGE/responseStream.go . ResponseStream
import (
	"encoding/json"
	"io"
	"net/http"
	"sync"

	"golang.org/x/xerrors"
	"k8s.io/apimachinery/pkg/watch"
)

// ResourceKind represents k8s resource name.
type ResourceKind string

// WatchEvent represents an event notified by the watched apiserver.
type WatchEvent struct {
	Kind      ResourceKind
	EventType watch.EventType
	// Obj is an object included in the event notified by the watched apiserver.
	Obj interface{}
}

// StreamWriter operates a given stream to send a received WatchEvent to the frontend.
type StreamWriter struct {
	sync.Mutex
	// stream knows where to write a received WatchEvent and how to send it.
	stream ResponseStream
	// encoder is a json encoder and the result will be written to the above stream via io.Writer.
	encoder *json.Encoder
}

func NewStreamWriter(stream ResponseStream) *StreamWriter {
	return &StreamWriter{
		stream:  stream,
		encoder: json.NewEncoder(stream),
	}
}

// Write encodes the an received WatchEvent and push it to the frontend.
func (sw *StreamWriter) Write(we *WatchEvent) error {
	sw.Lock()
	defer sw.Unlock()
	if err := sw.encoder.Encode(we); err != nil {
		return xerrors.Errorf("encode a WatchEvent: %w", err)
	}
	sw.stream.Flush()
	return nil
}

// ResponseStream is an interface that allows Server Push to a Service.
type ResponseStream interface {
	io.Writer
	http.Flusher
}
</file>

<file path="kube-scheduler-simulator/simulator/resourcewatcher/eventproxy_test.go">
package resourcewatcher

import (
	"context"
	"testing"
	"time"

	"github.com/stretchr/testify/assert"
	"go.uber.org/mock/gomock"
	"golang.org/x/xerrors"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/runtime/schema"
	"k8s.io/apimachinery/pkg/watch"
	"k8s.io/client-go/kubernetes/fake"
	typedcorev1 "k8s.io/client-go/kubernetes/typed/core/v1"
	restfake "k8s.io/client-go/rest/fake"

	"sigs.k8s.io/kube-scheduler-simulator/simulator/resourcewatcher/mock_resourcewatcher"
	sw "sigs.k8s.io/kube-scheduler-simulator/simulator/resourcewatcher/streamwriter"
)

var (
	fakenode1 = &corev1.Node{
		ObjectMeta: metav1.ObjectMeta{
			Name:            "node1",
			ResourceVersion: "100",
		},
		TypeMeta: metav1.TypeMeta{
			Kind: "Node",
		},
	}
	fakenode2 = &corev1.Node{
		ObjectMeta: metav1.ObjectMeta{
			Name:            "node2",
			ResourceVersion: "200",
		},
		TypeMeta: metav1.TypeMeta{
			Kind: "Node",
		},
	}
	fakenode3 = &corev1.Node{
		ObjectMeta: metav1.ObjectMeta{
			Name:            "node3",
			ResourceVersion: "300",
		},
		TypeMeta: metav1.TypeMeta{
			Kind: "Node",
		},
	}
)

type fakePod struct{}

func (obj *fakePod) GetObjectKind() schema.ObjectKind { return schema.EmptyObjectKind }
func (obj *fakePod) DeepCopyObject() runtime.Object   { panic("DeepCopyObject not supported by fakePod") }

func TestService_listAndHandleItems(t *testing.T) {
	t.Parallel()
	tests := []struct {
		name                      string
		prepareFakeClientSetFn    func() *fake.Clientset
		prepareListerMockFn       func(l *mock_resourcewatcher.MockLister, nodes typedcorev1.NodeInterface)
		prepareStreamWriterMockFn func(w *mock_resourcewatcher.MockStreamWriter)
		prepareFakeRestClientFn   func() *restfake.RESTClient
		wantErr                   bool
		wantlastResourceVersion   string
	}{
		{
			name: "should list the resource and update the lastResourceVersion",
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				c.CoreV1().Nodes().Create(context.Background(), &corev1.Node{
					ObjectMeta: metav1.ObjectMeta{
						Name: "node1",
					},
				}, metav1.CreateOptions{})
				return c
			},
			prepareListerMockFn: func(l *mock_resourcewatcher.MockLister, nodes typedcorev1.NodeInterface) {
				list, _ := nodes.List(context.Background(), metav1.ListOptions{})
				list.ResourceVersion = "100"
				l.EXPECT().List(metav1.ListOptions{}).Return(list, nil)
			},
			prepareStreamWriterMockFn: func(w *mock_resourcewatcher.MockStreamWriter) {
				w.EXPECT().Write(gomock.Any()).Return(nil)
			},
			prepareFakeRestClientFn: func() *restfake.RESTClient {
				return &restfake.RESTClient{}
			},
			wantErr:                 false,
			wantlastResourceVersion: "100",
		},
		{
			name: "should return an error when HandleListItems return an error",
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				c.CoreV1().Nodes().Create(context.Background(), &corev1.Node{
					ObjectMeta: metav1.ObjectMeta{
						Name: "node1",
					},
				}, metav1.CreateOptions{})
				return c
			},
			prepareListerMockFn: func(l *mock_resourcewatcher.MockLister, nodes typedcorev1.NodeInterface) {
				get, _ := nodes.Get(context.Background(), "node1", metav1.GetOptions{})
				l.EXPECT().List(metav1.ListOptions{}).Return(get, nil)
			},
			prepareStreamWriterMockFn: func(_ *mock_resourcewatcher.MockStreamWriter) {
			},
			prepareFakeRestClientFn: func() *restfake.RESTClient {
				return &restfake.RESTClient{}
			},
			wantErr:                 true,
			wantlastResourceVersion: "1",
		},
		{
			name: "should returns an error and shouldn't changes the lastResourceVersion  when sendListedItems return an error",
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				c.CoreV1().Nodes().Create(context.Background(), &corev1.Node{
					ObjectMeta: metav1.ObjectMeta{
						Name: "node1",
					},
				}, metav1.CreateOptions{})
				return c
			},
			prepareListerMockFn: func(l *mock_resourcewatcher.MockLister, nodes typedcorev1.NodeInterface) {
				list, _ := nodes.List(context.Background(), metav1.ListOptions{})
				list.ResourceVersion = "100"
				l.EXPECT().List(metav1.ListOptions{}).Return(list, nil)
			},
			prepareStreamWriterMockFn: func(w *mock_resourcewatcher.MockStreamWriter) {
				w.EXPECT().Write(gomock.Any()).Return(xerrors.Errorf("failed"))
			},
			prepareFakeRestClientFn: func() *restfake.RESTClient {
				return &restfake.RESTClient{}
			},
			wantErr:                 true,
			wantlastResourceVersion: "1",
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			ctrl := gomock.NewController(t)
			fakeclientset := tt.prepareFakeClientSetFn()
			lister := mock_resourcewatcher.NewMockLister(ctrl)
			tt.prepareListerMockFn(lister, fakeclientset.CoreV1().Nodes())
			sw := mock_resourcewatcher.NewMockStreamWriter(ctrl)
			tt.prepareStreamWriterMockFn(sw)
			fakeRestClient := tt.prepareFakeRestClientFn()
			proxy := neweventProxy(sw, fakeRestClient, Nodes, &corev1.Node{}, "1")

			if err := proxy.listAndHandleItems(lister); (err != nil) != tt.wantErr {
				t.Fatalf("listAndHandleItems %v test, \nerror = %v", tt.name, err)
			}
			v := proxy.lastResourceVersion()
			if v != tt.wantlastResourceVersion {
				t.Fatalf("listAndHandleItems %v test, \nlastResourceVersion = %s, want = %v", tt.name, v, tt.wantlastResourceVersion)
			}
		})
	}
}

func TestEventProxyer_sendListedItems(t *testing.T) {
	t.Parallel()
	tests := []struct {
		name                      string
		prepareeventProxyerMockFn func(p *MockeventProxyer)
		prepareFakeClientSetFn    func() *fake.Clientset
		prepareStreamWriterMockFn func(w *mock_resourcewatcher.MockStreamWriter)
		prepareFakeRestClientFn   func() *restfake.RESTClient
		prepareItems              func() []runtime.Object
		wantErr                   bool
	}{
		{
			name: "should call Write method",
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				return c
			},
			prepareStreamWriterMockFn: func(w *mock_resourcewatcher.MockStreamWriter) {
				w.EXPECT().Write(gomock.Any()).Return(nil)
			},
			prepareFakeRestClientFn: func() *restfake.RESTClient {
				return &restfake.RESTClient{}
			},
			prepareItems: func() []runtime.Object {
				return []runtime.Object{
					fakenode1,
				}
			},
			wantErr: false,
		},
		{
			name: "should call Write method twice",
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				return c
			},
			prepareStreamWriterMockFn: func(w *mock_resourcewatcher.MockStreamWriter) {
				w.EXPECT().Write(gomock.Any()).Return(nil).Times(2)
			},
			prepareFakeRestClientFn: func() *restfake.RESTClient {
				return &restfake.RESTClient{}
			},
			prepareItems: func() []runtime.Object {
				return []runtime.Object{
					fakenode1,
					fakenode2,
				}
			},
			wantErr: false,
		},
		{
			name: "should return an error when the Write method returns an error",
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				return c
			},
			prepareStreamWriterMockFn: func(w *mock_resourcewatcher.MockStreamWriter) {
				w.EXPECT().Write(gomock.Any()).Return(xerrors.Errorf("failed"))
			},
			prepareFakeRestClientFn: func() *restfake.RESTClient {
				return &restfake.RESTClient{}
			},
			prepareItems: func() []runtime.Object {
				return []runtime.Object{
					fakenode1,
				}
			},
			wantErr: true,
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			ctrl := gomock.NewController(t)
			sw := mock_resourcewatcher.NewMockStreamWriter(ctrl)
			tt.prepareStreamWriterMockFn(sw)
			fakeRestClient := tt.prepareFakeRestClientFn()
			proxy := neweventProxy(sw, fakeRestClient, Nodes, &corev1.Node{}, "1")
			items := tt.prepareItems()
			if err := proxy.sendListedItems(items); (err != nil) != tt.wantErr {
				t.Fatalf("listAndHandleItems %v test, \nerror = %v", tt.name, err)
			}
		})
	}
}

func TestEventProxyer_watchHandlerFunc(t *testing.T) {
	t.Parallel()
	tests := []struct {
		name                      string
		prepareStreamWriterMockFn func(sw *mock_resourcewatcher.MockStreamWriter)
		prepareFakeRestClientFn   func() *restfake.RESTClient
		doEvent                   func(fw *watch.FakeWatcher)
		wantErr                   bool
		wantlastResourceVersion   string
	}{
		{
			name: "should call the Write method (with ADDED event)",
			prepareStreamWriterMockFn: func(w *mock_resourcewatcher.MockStreamWriter) {
				w.EXPECT().Write(gomock.Any()).Return(nil).Do(func(e *sw.WatchEvent) {
					assert.Equal(t, Nodes, e.Kind)
					assert.Equal(t, watch.Added, e.EventType)
					obj, ok := e.Obj.(metav1.Object)
					assert.True(t, ok)
					assert.Equal(t, "node1", obj.GetName())
				})
			},
			prepareFakeRestClientFn: func() *restfake.RESTClient {
				return &restfake.RESTClient{}
			},
			doEvent: func(fw *watch.FakeWatcher) {
				go func() {
					fw.Add(fakenode1)
				}()
			},
			wantErr:                 false,
			wantlastResourceVersion: "100",
		},
		{
			name: "should call the Write method (with twice ADDED event)",
			prepareStreamWriterMockFn: func(w *mock_resourcewatcher.MockStreamWriter) {
				w.EXPECT().Write(gomock.Any()).Return(nil).Do(func(e *sw.WatchEvent) {
					assert.Equal(t, Nodes, e.Kind)
					assert.Equal(t, watch.Added, e.EventType)
					obj, ok := e.Obj.(metav1.Object)
					assert.True(t, ok)
					assert.Equal(t, "node1", obj.GetName())
				})
				w.EXPECT().Write(gomock.Any()).Return(nil).Do(func(e *sw.WatchEvent) {
					assert.Equal(t, Nodes, e.Kind)
					assert.Equal(t, watch.Added, e.EventType)
					obj, ok := e.Obj.(metav1.Object)
					assert.True(t, ok)
					assert.Equal(t, "node2", obj.GetName())
				})
			},
			prepareFakeRestClientFn: func() *restfake.RESTClient {
				return &restfake.RESTClient{}
			},
			doEvent: func(fw *watch.FakeWatcher) {
				go func() {
					fw.Add(fakenode1)
					fw.Add(fakenode2)
				}()
			},
			wantErr:                 false,
			wantlastResourceVersion: "200",
		},
		{
			name: "should call the Write method (with MODIFIED event)",
			prepareStreamWriterMockFn: func(w *mock_resourcewatcher.MockStreamWriter) {
				w.EXPECT().Write(gomock.Any()).Return(nil).Do(func(e *sw.WatchEvent) {
					assert.Equal(t, Nodes, e.Kind)
					assert.Equal(t, watch.Modified, e.EventType)
					obj, ok := e.Obj.(metav1.Object)
					assert.True(t, ok)
					assert.Equal(t, "node1", obj.GetName())
				})
			},
			prepareFakeRestClientFn: func() *restfake.RESTClient {
				return &restfake.RESTClient{}
			},
			doEvent: func(fw *watch.FakeWatcher) {
				go func() {
					fw.Modify(fakenode1)
				}()
			},
			wantErr:                 false,
			wantlastResourceVersion: "100",
		},
		{
			name: "should call the Write method (with twice MODIFIED event)",
			prepareStreamWriterMockFn: func(w *mock_resourcewatcher.MockStreamWriter) {
				w.EXPECT().Write(gomock.Any()).Return(nil).Do(func(e *sw.WatchEvent) {
					assert.Equal(t, Nodes, e.Kind)
					assert.Equal(t, watch.Modified, e.EventType)
					obj, ok := e.Obj.(metav1.Object)
					assert.True(t, ok)
					assert.Equal(t, "node1", obj.GetName())
				})
				w.EXPECT().Write(gomock.Any()).Return(nil).Do(func(e *sw.WatchEvent) {
					assert.Equal(t, Nodes, e.Kind)
					assert.Equal(t, watch.Modified, e.EventType)
					obj, ok := e.Obj.(metav1.Object)
					assert.True(t, ok)
					assert.Equal(t, "node2", obj.GetName())
				})
			},
			prepareFakeRestClientFn: func() *restfake.RESTClient {
				return &restfake.RESTClient{}
			},
			doEvent: func(fw *watch.FakeWatcher) {
				go func() {
					fw.Modify(fakenode1)
					fw.Modify(fakenode2)
				}()
			},
			wantErr:                 false,
			wantlastResourceVersion: "200",
		},
		{
			name: "should call the Write method (with DELETED event)",
			prepareStreamWriterMockFn: func(w *mock_resourcewatcher.MockStreamWriter) {
				w.EXPECT().Write(gomock.Any()).Return(nil).Do(func(e *sw.WatchEvent) {
					assert.Equal(t, Nodes, e.Kind)
					assert.Equal(t, watch.Deleted, e.EventType)
					obj, ok := e.Obj.(metav1.Object)
					assert.True(t, ok)
					assert.Equal(t, "node1", obj.GetName())
				})
			},
			prepareFakeRestClientFn: func() *restfake.RESTClient {
				return &restfake.RESTClient{}
			},
			doEvent: func(fw *watch.FakeWatcher) {
				go func() {
					fw.Delete(fakenode1)
				}()
			},
			wantErr:                 false,
			wantlastResourceVersion: "100",
		},
		{
			name: "should call the Write method (with twice DELETED event)",
			prepareStreamWriterMockFn: func(w *mock_resourcewatcher.MockStreamWriter) {
				w.EXPECT().Write(gomock.Any()).Return(nil).Do(func(e *sw.WatchEvent) {
					assert.Equal(t, Nodes, e.Kind)
					assert.Equal(t, watch.Deleted, e.EventType)
					obj, ok := e.Obj.(metav1.Object)
					assert.True(t, ok)
					assert.Equal(t, "node1", obj.GetName())
				})
				w.EXPECT().Write(gomock.Any()).Return(nil).Do(func(e *sw.WatchEvent) {
					assert.Equal(t, Nodes, e.Kind)
					assert.Equal(t, watch.Deleted, e.EventType)
					obj, ok := e.Obj.(metav1.Object)
					assert.True(t, ok)
					assert.Equal(t, "node2", obj.GetName())
				})
			},
			prepareFakeRestClientFn: func() *restfake.RESTClient {
				return &restfake.RESTClient{}
			},
			doEvent: func(fw *watch.FakeWatcher) {
				go func() {
					fw.Delete(fakenode1)
					fw.Delete(fakenode2)
				}()
			},
			wantErr:                 false,
			wantlastResourceVersion: "200",
		},
		{
			name: "should call the Write method (with ADDED and MODIFIED event)",
			prepareStreamWriterMockFn: func(w *mock_resourcewatcher.MockStreamWriter) {
				w.EXPECT().Write(gomock.Any()).Return(nil).Do(func(e *sw.WatchEvent) {
					assert.Equal(t, Nodes, e.Kind)
					assert.Equal(t, watch.Added, e.EventType)
					obj, ok := e.Obj.(metav1.Object)
					assert.True(t, ok)
					assert.Equal(t, "node1", obj.GetName())
				})
				w.EXPECT().Write(gomock.Any()).Return(nil).Do(func(e *sw.WatchEvent) {
					assert.Equal(t, Nodes, e.Kind)
					assert.Equal(t, watch.Modified, e.EventType)
					obj, ok := e.Obj.(metav1.Object)
					assert.True(t, ok)
					assert.Equal(t, "node2", obj.GetName())
				})
			},
			prepareFakeRestClientFn: func() *restfake.RESTClient {
				return &restfake.RESTClient{}
			},
			doEvent: func(fw *watch.FakeWatcher) {
				go func() {
					fw.Add(fakenode1)
					fw.Modify(fakenode2)
				}()
			},
			wantErr:                 false,
			wantlastResourceVersion: "200",
		},
		{
			name: "should call the Write method (with ADDED and twice MODIFIED event)",
			prepareStreamWriterMockFn: func(w *mock_resourcewatcher.MockStreamWriter) {
				w.EXPECT().Write(gomock.Any()).Return(nil).Do(func(e *sw.WatchEvent) {
					assert.Equal(t, Nodes, e.Kind)
					assert.Equal(t, watch.Added, e.EventType)
					obj, ok := e.Obj.(metav1.Object)
					assert.True(t, ok)
					assert.Equal(t, "node1", obj.GetName())
				})
				w.EXPECT().Write(gomock.Any()).Return(nil).Do(func(e *sw.WatchEvent) {
					assert.Equal(t, Nodes, e.Kind)
					assert.Equal(t, watch.Modified, e.EventType)
					obj, ok := e.Obj.(metav1.Object)
					assert.True(t, ok)
					assert.Equal(t, "node2", obj.GetName())
				})
				w.EXPECT().Write(gomock.Any()).Return(nil).Do(func(e *sw.WatchEvent) {
					assert.Equal(t, Nodes, e.Kind)
					assert.Equal(t, watch.Modified, e.EventType)
					obj, ok := e.Obj.(metav1.Object)
					assert.True(t, ok)
					assert.Equal(t, "node3", obj.GetName())
				})
			},
			prepareFakeRestClientFn: func() *restfake.RESTClient {
				return &restfake.RESTClient{}
			},
			doEvent: func(fw *watch.FakeWatcher) {
				go func() {
					fw.Add(fakenode1)
					fw.Modify(fakenode2)
					fw.Modify(fakenode3)
				}()
			},
			wantErr:                 false,
			wantlastResourceVersion: "300",
		},
		{
			name: "should call the Write method (with ADDED, MODIFIED and DELETED event)",
			prepareStreamWriterMockFn: func(w *mock_resourcewatcher.MockStreamWriter) {
				w.EXPECT().Write(gomock.Any()).Return(nil).Do(func(e *sw.WatchEvent) {
					assert.Equal(t, Nodes, e.Kind)
					assert.Equal(t, watch.Added, e.EventType)
					obj, ok := e.Obj.(metav1.Object)
					assert.True(t, ok)
					assert.Equal(t, "node1", obj.GetName())
				})
				w.EXPECT().Write(gomock.Any()).Return(nil).Do(func(e *sw.WatchEvent) {
					assert.Equal(t, Nodes, e.Kind)
					assert.Equal(t, watch.Modified, e.EventType)
					obj, ok := e.Obj.(metav1.Object)
					assert.True(t, ok)
					assert.Equal(t, "node2", obj.GetName())
				})
				w.EXPECT().Write(gomock.Any()).Return(nil).Do(func(e *sw.WatchEvent) {
					assert.Equal(t, Nodes, e.Kind)
					assert.Equal(t, watch.Deleted, e.EventType)
					obj, ok := e.Obj.(metav1.Object)
					assert.True(t, ok)
					assert.Equal(t, "node3", obj.GetName())
				})
			},
			prepareFakeRestClientFn: func() *restfake.RESTClient {
				return &restfake.RESTClient{}
			},
			doEvent: func(fw *watch.FakeWatcher) {
				go func() {
					fw.Add(fakenode1)
					fw.Modify(fakenode2)
					fw.Delete(fakenode3)
				}()
			},
			wantErr:                 false,
			wantlastResourceVersion: "300",
		},
		{
			name: "should return an error if the passed object is failed to cast to a metav1.Object",
			prepareStreamWriterMockFn: func(_ *mock_resourcewatcher.MockStreamWriter) {
			},
			prepareFakeRestClientFn: func() *restfake.RESTClient {
				return &restfake.RESTClient{}
			},
			doEvent: func(fw *watch.FakeWatcher) {
				go func() {
					var obj *fakePod
					fw.Add(obj)
				}()
			},
			wantErr:                 true,
			wantlastResourceVersion: "1",
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			ctrl := gomock.NewController(t)
			fakeRestClient := tt.prepareFakeRestClientFn()
			mockStreamWriter := mock_resourcewatcher.NewMockStreamWriter(ctrl)
			tt.prepareStreamWriterMockFn(mockStreamWriter)
			fw := watch.NewFake()

			proxy := neweventProxy(mockStreamWriter, fakeRestClient, Nodes, &corev1.Node{}, "1")

			testFunc := proxy.watchHandlerFunc(fw)
			ctx, cancel := context.WithTimeout(context.Background(), time.Second)
			defer cancel()
			tt.doEvent(fw)
			err := testFunc(ctx.Done())

			if (err != nil) != tt.wantErr {
				t.Fatalf("watchHandlerFunc %v test, \nerror = %v", tt.name, err)
			}
			v := proxy.lastResourceVersion()
			if v != tt.wantlastResourceVersion {
				t.Fatalf("watchHandlerFunc %v test, \nlastResourceVersion = %v, want = %v", tt.name, v, tt.wantlastResourceVersion)
			}
		})
	}
}

func TestEventProxyer_watchHandlerFuncFails(t *testing.T) {
	t.Parallel()
	tests := []struct {
		name                        string
		prepareWatchInterfaceMockFn func(rs *mock_resourcewatcher.MockWatchInterface)
		prepareFakeRestClientFn     func() *restfake.RESTClient
		wantErr                     bool
	}{
		{
			name: "should return an error if the channel of ResultChan is closed",
			prepareWatchInterfaceMockFn: func(w *mock_resourcewatcher.MockWatchInterface) {
				ch := make(chan watch.Event)
				close(ch)
				w.EXPECT().ResultChan().Return(ch)
			},
			prepareFakeRestClientFn: func() *restfake.RESTClient {
				return &restfake.RESTClient{}
			},
			wantErr: true,
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			ctrl := gomock.NewController(t)
			fakeRestClient := tt.prepareFakeRestClientFn()
			mockStreamWriter := mock_resourcewatcher.NewMockStreamWriter(ctrl)
			mockWatcher := mock_resourcewatcher.NewMockWatchInterface(ctrl)
			tt.prepareWatchInterfaceMockFn(mockWatcher)

			proxy := neweventProxy(mockStreamWriter, fakeRestClient, Nodes, &corev1.Node{}, "1")

			testFunc := proxy.watchHandlerFunc(mockWatcher)

			ctx := context.Background()
			err := testFunc(ctx.Done())

			if (err != nil) != tt.wantErr {
				t.Fatalf("watchHandlerFunc %v test, \nerror = %v", tt.name, err)
			}
		})
	}
}
</file>

<file path="kube-scheduler-simulator/simulator/resourcewatcher/eventproxy.go">
package resourcewatcher

//go:generate mockgen -destination=./mock_$GOPACKAGE/watchInterface.go -package=mock_resourcewatcher -mock_names Interface=MockWatchInterface k8s.io/apimachinery/pkg/watch Interface
//go:generate mockgen -source=eventproxy.go -destination=./mock_eventproxy_test.go -package=resourcewatcher
//go:generate mockgen -destination=./mock_$GOPACKAGE/lister.go -package=mock_resourcewatcher -mock_names Interface=MockListerInterface k8s.io/client-go/tools/cache Lister

import (
	"errors"
	"fmt"
	"io"

	"golang.org/x/xerrors"
	"k8s.io/apimachinery/pkg/api/meta"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	utilruntime "k8s.io/apimachinery/pkg/util/runtime"
	"k8s.io/apimachinery/pkg/util/wait"
	"k8s.io/apimachinery/pkg/watch"
	"k8s.io/client-go/tools/cache"
	"k8s.io/klog/v2"

	sw "sigs.k8s.io/kube-scheduler-simulator/simulator/resourcewatcher/streamwriter"
)

// eventProxyer is an interface that allows handle events and errors.
type eventProxyer interface {
	listAndHandleItems(lw cache.Lister) error
	watchAndHandleEvent(watcher watch.Interface, stopCh <-chan struct{})
	lastResourceVersion() string
	resourceKind() sw.ResourceKind
	restClient() cache.Getter
}

// eventProxy implements event handler for the specified resource
// and knows where to send the event.
type eventProxy struct {
	// writer knows where to send the event.
	writer StreamWriter
	// The RESTClient to watch the specified.
	c cache.Getter
	// The kind of resource to watch.
	r sw.ResourceKind
	// The Object of resource to watch.
	o runtime.Object
	// The last value of ResourceVersion. This is used to RetryWatcher.
	// First, this value is specified by a user.
	// After that this is updated when received an event everytime.
	//
	// The RetryWatcher will be reconnect to the apiserver if it is disconnected.
	// lrv can be used to ensure that only events
	// that have not yet been received are received when reconnecting.
	lrv string
}

func neweventProxy(sw StreamWriter, c cache.Getter, r sw.ResourceKind, o runtime.Object, lrv string) *eventProxy {
	return &eventProxy{
		writer: sw,
		c:      c,
		r:      r,
		o:      o,
		lrv:    lrv,
	}
}

// listAndHandleItems calls the list for the resource and the results is sent to the client by sendListedItems method.
func (p *eventProxy) listAndHandleItems(lw cache.Lister) error {
	list, err := lw.List(metav1.ListOptions{})
	if err != nil {
		return xerrors.Errorf("failed to list: %w", err)
	}
	items, lrv, err := extractListItem(list)
	if err != nil {
		return xerrors.Errorf("call HandleListItems: %w", err)
	}
	if err := p.sendListedItems(items); err != nil {
		return xerrors.Errorf("call ListItemsHandle: %w", err)
	}
	p.lrv = lrv
	return nil
}

// extractListItem validates whether the object is a list object and returns these items and lastResourceVersion.
func extractListItem(list runtime.Object) ([]runtime.Object, string, error) {
	listMetaInterface, err := meta.ListAccessor(list)
	if err != nil {
		return nil, "", xerrors.Errorf("failed to ListAccessor, unable to understand list result %#v: %w", list, err)
	}
	items, err := meta.ExtractList(list)
	if err != nil {
		return nil, "", xerrors.Errorf("failed to ExtractList, unable to understand list result %#v: %w", list, err)
	}
	return items, listMetaInterface.GetResourceVersion(), nil
}

// watchAndHandleEvent prepares a handler for the wacher and runs the handler
// until the stopCh is closed.
func (p *eventProxy) watchAndHandleEvent(watcher watch.Interface, stopCh <-chan struct{}) {
	defer utilruntime.HandleCrash()
	handleFunc := p.watchHandlerFunc(watcher)
	run := func(stopCh <-chan struct{}) {
		if err := handleFunc(stopCh); err != nil {
			p.watchErrorHandler(err)
		}
	}
	var wg wait.Group
	wg.StartWithChannel(stopCh, run)
	wg.Wait()
}

// sendListedItems sends results of list as "ADDED" event to the client.
// This method will be expected to call before starting the watch.
func (p *eventProxy) sendListedItems(items []runtime.Object) error {
	for _, item := range items {
		if err := p.writer.Write(&sw.WatchEvent{Kind: p.r, EventType: watch.Added, Obj: item}); err != nil {
			return xerrors.Errorf("call Write to return list item %#v: %w", item, err)
		}
	}
	return nil
}

// watchHandlerFunc watches the specified resource's event.
// When it receives the event from watcher, it sends the event to stream
// and updates the lastResourceVersion in the eventProxy.
//
//nolint:cyclop // For readability.
func (p *eventProxy) watchHandlerFunc(watcher watch.Interface) func(stopCh <-chan struct{}) error {
	return func(stopCh <-chan struct{}) error {
		for {
			// give the stopCh a chance to stop the loop, even in case of continue statements further down on errors
			select {
			case <-stopCh:
				return nil
			case event, ok := <-watcher.ResultChan():
				// grab the event object
				if !ok {
					return xerrors.New("closed channel")
				}
				obj, ok := event.Object.(metav1.Object)
				if !ok {
					return xerrors.Errorf("failed to cast type from %T to metav1.Object", event.Object)
				}
				var writingErr error
				switch event.Type {
				case watch.Added:
					writingErr = p.writer.Write(&sw.WatchEvent{Kind: p.r, EventType: watch.Added, Obj: obj})
				case watch.Modified:
					writingErr = p.writer.Write(&sw.WatchEvent{Kind: p.r, EventType: watch.Modified, Obj: obj})
				case watch.Deleted:
					writingErr = p.writer.Write(&sw.WatchEvent{Kind: p.r, EventType: watch.Deleted, Obj: obj})
				case watch.Bookmark:
					// A `Bookmark` means watch has synced here, just update the resourceVersion
				case watch.Error:
					return xerrors.Errorf("%s: get an error watch event %#v", p.resourceKind(), obj)
				default:
					return xerrors.Errorf("%s: unsupported event type %v, object %#v", p.resourceKind(), event.Type, obj)
				}

				if writingErr != nil {
					return xerrors.Errorf("call Write to watch event: %w", writingErr)
				}
				p.lrv = obj.GetResourceVersion()
			}
		}
	}
}

func (p *eventProxy) resourceKind() sw.ResourceKind {
	return p.r
}

func (p *eventProxy) restClient() cache.Getter {
	return p.c
}

// lastResourceVersion returns the lastResourceVersion value that is kept in the proxy.
func (p *eventProxy) lastResourceVersion() string {
	return p.lrv
}

// WatchErrorHandler handles some errors.
func (p *eventProxy) watchErrorHandler(err error) {
	switch {
	case errors.Is(err, io.EOF):
		// watch closed normally
	case errors.Is(err, io.ErrUnexpectedEOF):
		klog.Infof("watch for %v closed with unexpected EOF: %v", p.r, err)
	default:
		utilruntime.HandleError(fmt.Errorf("failed to watch %v: %w", p.r, err))
	}
}
</file>

<file path="kube-scheduler-simulator/simulator/resourcewatcher/mock_eventproxy_test.go">
// Code generated by MockGen. DO NOT EDIT.
// Source: eventproxy.go
//
// Generated by this command:
//
//	mockgen -source=eventproxy.go -destination=./mock_eventproxy_test.go -package=resourcewatcher
//

// Package resourcewatcher is a generated GoMock package.
package resourcewatcher

import (
	reflect "reflect"

	gomock "go.uber.org/mock/gomock"
	watch "k8s.io/apimachinery/pkg/watch"
	cache "k8s.io/client-go/tools/cache"

	streamwriter "sigs.k8s.io/kube-scheduler-simulator/simulator/resourcewatcher/streamwriter"
)

// MockeventProxyer is a mock of eventProxyer interface.
type MockeventProxyer struct {
	ctrl     *gomock.Controller
	recorder *MockeventProxyerMockRecorder
	isgomock struct{}
}

// MockeventProxyerMockRecorder is the mock recorder for MockeventProxyer.
type MockeventProxyerMockRecorder struct {
	mock *MockeventProxyer
}

// NewMockeventProxyer creates a new mock instance.
func NewMockeventProxyer(ctrl *gomock.Controller) *MockeventProxyer {
	mock := &MockeventProxyer{ctrl: ctrl}
	mock.recorder = &MockeventProxyerMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockeventProxyer) EXPECT() *MockeventProxyerMockRecorder {
	return m.recorder
}

// lastResourceVersion mocks base method.
func (m *MockeventProxyer) lastResourceVersion() string {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "lastResourceVersion")
	ret0, _ := ret[0].(string)
	return ret0
}

// lastResourceVersion indicates an expected call of lastResourceVersion.
func (mr *MockeventProxyerMockRecorder) lastResourceVersion() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "lastResourceVersion", reflect.TypeOf((*MockeventProxyer)(nil).lastResourceVersion))
}

// listAndHandleItems mocks base method.
func (m *MockeventProxyer) listAndHandleItems(lw cache.Lister) error {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "listAndHandleItems", lw)
	ret0, _ := ret[0].(error)
	return ret0
}

// listAndHandleItems indicates an expected call of listAndHandleItems.
func (mr *MockeventProxyerMockRecorder) listAndHandleItems(lw any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "listAndHandleItems", reflect.TypeOf((*MockeventProxyer)(nil).listAndHandleItems), lw)
}

// resourceKind mocks base method.
func (m *MockeventProxyer) resourceKind() streamwriter.ResourceKind {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "resourceKind")
	ret0, _ := ret[0].(streamwriter.ResourceKind)
	return ret0
}

// resourceKind indicates an expected call of resourceKind.
func (mr *MockeventProxyerMockRecorder) resourceKind() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "resourceKind", reflect.TypeOf((*MockeventProxyer)(nil).resourceKind))
}

// restClient mocks base method.
func (m *MockeventProxyer) restClient() cache.Getter {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "restClient")
	ret0, _ := ret[0].(cache.Getter)
	return ret0
}

// restClient indicates an expected call of restClient.
func (mr *MockeventProxyerMockRecorder) restClient() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "restClient", reflect.TypeOf((*MockeventProxyer)(nil).restClient))
}

// watchAndHandleEvent mocks base method.
func (m *MockeventProxyer) watchAndHandleEvent(watcher watch.Interface, stopCh <-chan struct{}) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "watchAndHandleEvent", watcher, stopCh)
}

// watchAndHandleEvent indicates an expected call of watchAndHandleEvent.
func (mr *MockeventProxyerMockRecorder) watchAndHandleEvent(watcher, stopCh any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "watchAndHandleEvent", reflect.TypeOf((*MockeventProxyer)(nil).watchAndHandleEvent), watcher, stopCh)
}
</file>

<file path="kube-scheduler-simulator/simulator/resourcewatcher/resourcewatcher_test.go">
package resourcewatcher

import (
	"context"
	"testing"

	"go.uber.org/mock/gomock"
	"golang.org/x/xerrors"
	corev1 "k8s.io/api/core/v1"
	"k8s.io/client-go/kubernetes/fake"
	restfake "k8s.io/client-go/rest/fake"
	"k8s.io/client-go/tools/cache"

	sw "sigs.k8s.io/kube-scheduler-simulator/simulator/resourcewatcher/streamwriter"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/resourcewatcher/streamwriter/mock_streamwriter"
)

func TestEventProxyer_createWatcher(t *testing.T) {
	t.Parallel()
	tests := []struct {
		name                    string
		prepareFakeRestClientFn func() *restfake.RESTClient
		resourceversion         string
		wantErr                 bool
	}{
		{
			name: "should success",
			prepareFakeRestClientFn: func() *restfake.RESTClient {
				return &restfake.RESTClient{}
			},
			resourceversion: "1",
			wantErr:         false,
		},
		{
			name: "should returns an error when NewRetryWatcher is failed(when resourceversion is 0)",
			prepareFakeRestClientFn: func() *restfake.RESTClient {
				return &restfake.RESTClient{}
			},
			resourceversion: "0",
			wantErr:         true,
		},
		{
			name: "should returns an error when NewRetryWatcher is failed(when resourceversion is empty)",
			prepareFakeRestClientFn: func() *restfake.RESTClient {
				return &restfake.RESTClient{}
			},
			resourceversion: "",
			wantErr:         true,
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			ctrl := gomock.NewController(t)
			restclient := tt.prepareFakeRestClientFn()
			mockResponseStream := mock_streamwriter.NewMockResponseStream(ctrl)

			sw := sw.NewStreamWriter(mockResponseStream)
			proxy := neweventProxy(sw, restclient, Pods, &corev1.Pod{}, tt.resourceversion)

			lw := createListWatch(proxy)
			_, err := createWatcher(proxy, lw)
			if (err != nil) != tt.wantErr {
				t.Fatalf("createWatcher %v test, \nerror = %v", tt.name, err)
			}
		})
	}
}

func TestService_doListAndWatch(t *testing.T) {
	t.Parallel()
	tests := []struct {
		name                      string
		prepareFakeClientSetFn    func() *fake.Clientset
		prepareFakeRestClientFn   func() *restfake.RESTClient
		prepareeventProxyerMockFn func(p *MockeventProxyer, getter cache.Getter)
		wantErr                   bool
	}{
		{
			name: "should call watchAndHandleEvent method",
			prepareFakeClientSetFn: func() *fake.Clientset {
				return fake.NewSimpleClientset()
			},
			prepareFakeRestClientFn: func() *restfake.RESTClient {
				return &restfake.RESTClient{}
			},
			prepareeventProxyerMockFn: func(p *MockeventProxyer, getter cache.Getter) {
				p.EXPECT().watchAndHandleEvent(gomock.Any(), gomock.Any())
				p.EXPECT().lastResourceVersion().Return("1").Times(2)
				p.EXPECT().restClient().Return(getter)
				p.EXPECT().resourceKind().Return(Pods)
			},
			wantErr: false,
		},
		{
			name: "should return an error when the lastResourceVersion is 0",
			prepareFakeClientSetFn: func() *fake.Clientset {
				return fake.NewSimpleClientset()
			},
			prepareFakeRestClientFn: func() *restfake.RESTClient {
				return &restfake.RESTClient{}
			},
			prepareeventProxyerMockFn: func(p *MockeventProxyer, getter cache.Getter) {
				p.EXPECT().lastResourceVersion().Return("0").Times(2)
				p.EXPECT().restClient().Return(getter)
				p.EXPECT().resourceKind().Return(Pods).Times(2)
			},
			wantErr: true,
		},
		{
			name: "should call listAndHandleItems method when the lastResourceVersion is empty",
			prepareFakeClientSetFn: func() *fake.Clientset {
				return fake.NewSimpleClientset()
			},
			prepareFakeRestClientFn: func() *restfake.RESTClient {
				return &restfake.RESTClient{}
			},
			prepareeventProxyerMockFn: func(p *MockeventProxyer, getter cache.Getter) {
				p.EXPECT().watchAndHandleEvent(gomock.Any(), gomock.Any())
				p.EXPECT().lastResourceVersion().Do(func() {
					p.EXPECT().lastResourceVersion().Return("1")
				}).Return("")
				p.EXPECT().restClient().Return(getter)
				p.EXPECT().resourceKind().Return(Pods).Times(1)
				p.EXPECT().listAndHandleItems(gomock.Any()).Return(nil)
			},
			wantErr: false,
		},
		{
			name: "should return an error when listAndHandleItems method return an error",
			prepareFakeClientSetFn: func() *fake.Clientset {
				return fake.NewSimpleClientset()
			},
			prepareFakeRestClientFn: func() *restfake.RESTClient {
				return &restfake.RESTClient{}
			},
			prepareeventProxyerMockFn: func(p *MockeventProxyer, getter cache.Getter) {
				p.EXPECT().lastResourceVersion().Return("")
				p.EXPECT().restClient().Return(getter)
				p.EXPECT().resourceKind().Return(Pods).Times(2)
				p.EXPECT().listAndHandleItems(gomock.Any()).Return(xerrors.Errorf("failed"))
			},
			wantErr: true,
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			ctrl := gomock.NewController(t)
			mockProxy := NewMockeventProxyer(ctrl)
			fakeClientSet := tt.prepareFakeClientSetFn()
			s := NewService(fakeClientSet)
			fakeRestClient := tt.prepareFakeRestClientFn()
			tt.prepareeventProxyerMockFn(mockProxy, fakeRestClient)

			ctx, cancel := context.WithCancel(context.Background())
			defer cancel()

			if err := s.doListAndWatch(mockProxy, ctx.Done()); (err != nil) != tt.wantErr {
				t.Fatalf("doListAndWatch %v test, \nerror = %v", tt.name, err)
			}
		})
	}
}
</file>

<file path="kube-scheduler-simulator/simulator/resourcewatcher/resourcewatcher.go">
package resourcewatcher

//go:generate mockgen -destination=./mock_$GOPACKAGE/streamWriter.go . StreamWriter

import (
	"context"

	"golang.org/x/xerrors"
	corev1 "k8s.io/api/core/v1"
	schedulingv1 "k8s.io/api/scheduling/v1"
	storagev1 "k8s.io/api/storage/v1"
	"k8s.io/apimachinery/pkg/fields"
	"k8s.io/apimachinery/pkg/watch"
	clientset "k8s.io/client-go/kubernetes"
	"k8s.io/client-go/tools/cache"
	watchtools "k8s.io/client-go/tools/watch"
	"k8s.io/klog/v2"

	sw "sigs.k8s.io/kube-scheduler-simulator/simulator/resourcewatcher/streamwriter"
)

const (
	Pods       sw.ResourceKind = "pods"
	Nodes      sw.ResourceKind = "nodes"
	Pvs        sw.ResourceKind = "persistentvolumes"
	Pvcs       sw.ResourceKind = "persistentvolumeclaims"
	Scs        sw.ResourceKind = "storageclasses"
	Pcs        sw.ResourceKind = "priorityclasses"
	Namespaces sw.ResourceKind = "namespaces"
)

// LastResourceVersions includes each resource's LastResourceVersions.
type LastResourceVersions struct {
	Pods       string
	Nodes      string
	Pvs        string
	Pvcs       string
	Scs        string
	Pcs        string
	Namespaces string
}

// StreamWriter is an interface that allows send a received WatchEvent to the frontend.
type StreamWriter interface {
	Write(we *sw.WatchEvent) error
}

// Service watches simulator's resources.
type Service struct {
	client clientset.Interface
}

// NewService initializes Service.
func NewService(client clientset.Interface) *Service {
	return &Service{
		client: client,
	}
}

// ListWatch watches each simulator's resources and send notified events to the frontend continuously.
func (s *Service) ListWatch(ctx context.Context, stream sw.ResponseStream, lrVersions *LastResourceVersions) error {
	sw := sw.NewStreamWriter(stream)
	proxies := []*eventProxy{
		neweventProxy(sw, s.client.CoreV1().RESTClient(), Pods, &corev1.Pod{}, lrVersions.Pods),
		neweventProxy(sw, s.client.CoreV1().RESTClient(), Nodes, &corev1.Node{}, lrVersions.Nodes),
		neweventProxy(sw, s.client.CoreV1().RESTClient(), Pvs, &corev1.PersistentVolume{}, lrVersions.Pvs),
		neweventProxy(sw, s.client.CoreV1().RESTClient(), Pvcs, &corev1.PersistentVolumeClaim{}, lrVersions.Pvcs),
		neweventProxy(sw, s.client.StorageV1().RESTClient(), Scs, &storagev1.StorageClass{}, lrVersions.Scs),
		neweventProxy(sw, s.client.SchedulingV1().RESTClient(), Pcs, &schedulingv1.PriorityClass{}, lrVersions.Pcs),
		neweventProxy(sw, s.client.CoreV1().RESTClient(), Namespaces, &corev1.Namespace{}, lrVersions.Namespaces),
	}
	runctx, cancel := context.WithCancel(ctx)
	defer cancel()
	for _, p := range proxies {
		go s.run(p, runctx.Done(), cancel)
	}

	select {
	case <-runctx.Done():
		// ruuctx monitors s.Run (ListAndWatch) for each resource.
		// If some error occurs in the process before starting the watch,
		// ths error is returned.
		return xerrors.Errorf("failed to run ListAndWatch: %w", runctx.Err())
	case <-ctx.Done():
		// This method will return an error and finish to event send when the connection from a client is closed.
		// It includes browser reload, ReadableStream.cancel() calling and so on.
		// This is to allow the front end to handle stream connection disconnections.
		return nil
	}
}

// run runs doListAndWatch method.
// If an error is returned, call cancel to abort ListAndWatch of other resources being processed in parallel.
func (s *Service) run(p *eventProxy, stopCh <-chan struct{}, cancel context.CancelFunc) {
	defer cancel()
	// ListAndWatch usually continues to wait for WATCH to end and does not return any value.
	if err := s.doListAndWatch(p, stopCh); err != nil {
		cancel()
		klog.Errorf("call ListAndWatch: %v", err)
	}
}

// ListAndWatch runs list and watch on the target resource. The list is not always ran
// This method returns error unless an error occurs in the watch. If an error occurs in the watch,
// it outputs a log and re-run the watch.
func (s *Service) doListAndWatch(p eventProxyer, stopCh <-chan struct{}) error {
	lw := createListWatch(p)
	// If the lastResourceVersion isn't specified by client, call the list and return the result as ADDED event first.
	if p.lastResourceVersion() == "" {
		if err := p.listAndHandleItems(lw); err != nil {
			return xerrors.Errorf("call listAndHandleItems for %s: %w", p.resourceKind(), err)
		}
	}
	watcher, err := createWatcher(p, lw)
	if err != nil {
		return xerrors.Errorf("call createWatcher for %s: %w", p.resourceKind(), err)
	}
	p.watchAndHandleEvent(watcher, stopCh)
	return nil
}

// createListWatch creates and returns ListWatch.
func createListWatch(p eventProxyer) cache.ListerWatcher {
	return cache.NewListWatchFromClient(p.restClient(), string(p.resourceKind()), corev1.NamespaceAll, fields.Everything())
}

// createWatcher creates and returns RetryWatcher.
func createWatcher(p eventProxyer, lw cache.ListerWatcher) (watch.Interface, error) {
	rWatcher, err := watchtools.NewRetryWatcher(p.lastResourceVersion(), lw)
	if err != nil {
		return nil, xerrors.Errorf("call NewRetryWatcher: %w", err)
	}
	return rWatcher, nil
}
</file>

<file path="kube-scheduler-simulator/simulator/scheduler/config/config.go">
package config

import (
	"bytes"
	"encoding/json"
	"fmt"
	"os"

	"golang.org/x/xerrors"
	"gopkg.in/yaml.v2"
	v1 "k8s.io/kube-scheduler/config/v1"
	"k8s.io/kubernetes/pkg/scheduler/apis/config/scheme"
)

// kubeSchedulerConfigPath represents the file path to the scheduler configuration.
// It should be initialized when loading the simulator config.
var kubeSchedulerConfigPath string

// DefaultSchedulerConfig creates KubeSchedulerConfiguration default configuration.
func DefaultSchedulerConfig() (*v1.KubeSchedulerConfiguration, error) {
	var versionedCfg v1.KubeSchedulerConfiguration
	scheme.Scheme.Default(&versionedCfg)
	versionedCfg.SetGroupVersionKind(v1.SchemeGroupVersion.WithKind("KubeSchedulerConfiguration"))

	return &versionedCfg, nil
}

// SetKubeSchedulerCfgPath set Scheduler config path.
func SetKubeSchedulerCfgPath(path string) {
	kubeSchedulerConfigPath = path
}

// UpdateSchedulerConfig writes the given scheduler config to kubeSchedulerConfigPath.
func UpdateSchedulerConfig(cfg *v1.KubeSchedulerConfiguration) error {
	if kubeSchedulerConfigPath == "" {
		return xerrors.New("kubeSchedulerConfigPath isn't initialized, which is likely a bug in the simulator")
	}
	jsonData, err := json.Marshal(cfg)
	if err != nil {
		return fmt.Errorf("failed to marshal jsonData: %w", err)
	}

	var yamlData map[string]interface{}
	decoder := yaml.NewDecoder((bytes.NewReader(jsonData)))
	err = decoder.Decode(&yamlData)
	if err != nil {
		return fmt.Errorf("failed to decode jsonData: %w", err)
	}

	data, err := yaml.Marshal(yamlData)
	if err != nil {
		return fmt.Errorf("failed to marshal yaml: %w", err)
	}

	if err := os.WriteFile(kubeSchedulerConfigPath, data, 0o600); err != nil {
		return fmt.Errorf("failed to write file: %w", err)
	}

	return nil
}
</file>

<file path="kube-scheduler-simulator/simulator/scheduler/config/plugin_test.go">
package config

import (
	"sort"
	"testing"

	"github.com/stretchr/testify/assert"
	"k8s.io/kubernetes/pkg/scheduler/framework/runtime"
	"k8s.io/utils/strings/slices"
)

func TestInTreeMultiPointPluginSet(t *testing.T) {
	t.Parallel()
	t.Run("", func(t *testing.T) {
		wantEnabled := []string{
			"PrioritySort",
			"NodeUnschedulable",
			"NodeName",
			"TaintToleration",
			"NodeAffinity",
			"NodePorts",
			"NodeResourcesFit",
			"VolumeRestrictions",
			"EBSLimits",
			"GCEPDLimits",
			"NodeVolumeLimits",
			"AzureDiskLimits",
			"VolumeBinding",
			"VolumeZone",
			"PodTopologySpread",
			"InterPodAffinity",
			"DefaultPreemption",
			"NodeResourcesBalancedAllocation",
			"ImageLocality",
			"DefaultBinder",
			"SchedulingGates",
		}
		wantDisabled := []string{}

		mp, err := InTreeMultiPointPluginSet()
		assert.NoError(t, err, "check error")

		var count int
		for _, p := range mp.Enabled {
			if !slices.Contains(wantEnabled, p.Name) {
				t.Errorf("unexpected enabled plugin name is contained: name=%s", p.Name)
			}
			count++
		}
		assert.Equal(t, len(wantEnabled), count, "check sum of default enabled plugin")

		count = 0
		for _, p := range mp.Disabled {
			if !slices.Contains(wantDisabled, p.Name) {
				t.Errorf("unexpected disabled plugin name is contained: name=%s", p.Name)
			}
			count++
		}
		assert.Equal(t, len(wantDisabled), count, "check sum of default disabled plugin")
	})
}

//nolint:paralleltest // cannot use t.Parallel because SetOutOfTreeRegistries affects other test cases.
func TestRegisteredMultiPointPluginNames(t *testing.T) {
	tests := []struct {
		name              string
		outOfTreeRegistry runtime.Registry
		want              []string
		wantErr           bool
	}{
		{
			name: "success",
			want: []string{
				"PrioritySort",
				"SchedulingGates",
				"NodeName",
				"TaintToleration",
				"NodeAffinity",
				"NodeUnschedulable",
				"NodeResourcesBalancedAllocation",
				"ImageLocality",
				"InterPodAffinity",
				"NodeResourcesFit",
				"PodTopologySpread",
				"DefaultBinder",
				"VolumeBinding",
				"NodePorts",
				"VolumeRestrictions",
				"EBSLimits",
				"GCEPDLimits",
				"NodeVolumeLimits",
				"AzureDiskLimits",
				"VolumeZone",
				"DefaultPreemption",
			},
			wantErr: false,
		},
		{
			name: "success with out of tree",
			want: []string{
				"PrioritySort",
				"SchedulingGates",
				"NodeName",
				"TaintToleration",
				"NodeAffinity",
				"NodeUnschedulable",
				"NodeResourcesBalancedAllocation",
				"ImageLocality",
				"InterPodAffinity",
				"NodeResourcesFit",
				"PodTopologySpread",
				"DefaultBinder",
				"VolumeBinding",
				"NodePorts",
				"VolumeRestrictions",
				"EBSLimits",
				"GCEPDLimits",
				"NodeVolumeLimits",
				"AzureDiskLimits",
				"VolumeZone",
				"DefaultPreemption",
				"custom", // added.
			},
			outOfTreeRegistry: map[string]runtime.PluginFactory{
				"custom": nil,
			},
			wantErr: false,
		},
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			SetOutOfTreeRegistries(tt.outOfTreeRegistry)
			got, err := RegisteredMultiPointPluginNames()
			if (err != nil) != tt.wantErr {
				t.Errorf("RegisteredPluginNames() error = %v, wantErr %v", err, tt.wantErr)
				return
			}
			sort.SliceStable(got, func(i, j int) bool {
				return got[i] < got[j]
			})
			sort.SliceStable(tt.want, func(i, j int) bool {
				return tt.want[i] < tt.want[j]
			})
			assert.Equal(t, tt.want, got)
		})
	}
}
</file>

<file path="kube-scheduler-simulator/simulator/scheduler/config/plugin.go">
package config

import (
	"golang.org/x/xerrors"
	configv1 "k8s.io/kube-scheduler/config/v1"
	"k8s.io/kubernetes/pkg/scheduler/framework/plugins"
	"k8s.io/kubernetes/pkg/scheduler/framework/runtime"
)

var outOfTreeRegistries = runtime.Registry{
	// TODO(user): add your plugins registries here.
}

// RegisteredMultiPointPluginNames returns all registered multipoint plugin names.
// in-tree plugins and your original plugins listed in outOfTreeRegistries above.
func RegisteredMultiPointPluginNames() ([]string, error) {
	def, err := InTreeMultiPointPluginSet()
	if err != nil {
		return nil, xerrors.Errorf("get default multi point plugins: %w", err)
	}

	enabledPls := make([]string, 0, len(def.Enabled))
	for _, e := range def.Enabled {
		enabledPls = append(enabledPls, e.Name)
	}

	return append(enabledPls, OutOfTreeMultiPointPluginNames()...), nil
}

// InTreeMultiPointPluginSet returns default multipoint plugins.
// See also: https://github.com/kubernetes/kubernetes/blob/475f9010f5faa7bdd439944a6f5f1ec206297602/pkg/scheduler/apis/config/v1/default_plugins.go#L30https://github.com/kubernetes/kubernetes/blob/475f9010f5faa7bdd439944a6f5f1ec206297602/pkg/scheduler/apis/config/v1/default_plugins.go#L30
func InTreeMultiPointPluginSet() (configv1.PluginSet, error) {
	defaultConfig, err := DefaultSchedulerConfig()
	if err != nil || len(defaultConfig.Profiles) != 1 {
		// default Config should only have default-scheduler configuration.
		return configv1.PluginSet{}, xerrors.Errorf("get default scheduler configuration: %w", err)
	}
	return defaultConfig.Profiles[0].Plugins.MultiPoint, nil
}

func OutOfTreeMultiPointPluginNames() []string {
	registeredOutOfTreeMultiPointName := make([]string, 0, len(outOfTreeRegistries))
	for k := range outOfTreeRegistries {
		registeredOutOfTreeMultiPointName = append(registeredOutOfTreeMultiPointName, k)
	}
	return registeredOutOfTreeMultiPointName
}

func InTreeRegistries() runtime.Registry {
	return plugins.NewInTreeRegistry()
}

func OutOfTreeRegistries() runtime.Registry {
	return outOfTreeRegistries
}

func SetOutOfTreeRegistries(r runtime.Registry) {
	for k, v := range r {
		outOfTreeRegistries[k] = v
	}
}
</file>

<file path="kube-scheduler-simulator/simulator/scheduler/config/wasm_test.go">
package config

import (
	"testing"

	"github.com/stretchr/testify/require"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/kubernetes/pkg/scheduler/apis/config"
)

func TestGetWasmRegistryFromUnversionedConfig(t *testing.T) {
	t.Parallel()

	tests := []struct {
		name     string
		cfg      *config.KubeSchedulerConfiguration
		expected int
	}{
		{
			name:     "no profiles",
			cfg:      &config.KubeSchedulerConfiguration{},
			expected: 0,
		},
		{
			name: "no wasm plugins",
			cfg: &config.KubeSchedulerConfiguration{
				Profiles: []config.KubeSchedulerProfile{
					{
						PluginConfig: []config.PluginConfig{
							{
								Name: "DefaultPreemption",
								Args: &config.DefaultPreemptionArgs{},
							},
						},
						Plugins: &config.Plugins{
							MultiPoint: config.PluginSet{
								Enabled: []config.Plugin{
									{Name: "DefaultPreemption"},
								},
							},
						},
					},
				},
			},
			expected: 0,
		},
		{
			name: "one wasm plugin",
			cfg: &config.KubeSchedulerConfiguration{
				Profiles: []config.KubeSchedulerProfile{
					{
						PluginConfig: []config.PluginConfig{
							{
								Name: "DefaultPreemption",
								Args: &config.DefaultPreemptionArgs{},
							},
							{Name: "wasmPlugin", Args: &runtime.Unknown{
								ContentType: runtime.ContentTypeJSON,
								Raw:         []byte(`{"guestURL":"http://example.com/plugin.wasm"}`),
							}},
						},
						Plugins: &config.Plugins{
							MultiPoint: config.PluginSet{
								Enabled: []config.Plugin{
									{Name: "DefaultPreemption"},
									{Name: "wasmPlugin"},
								},
							},
						},
					},
				},
			},
			expected: 1,
		},
		{
			name: "multiple wasm plugins",
			cfg: &config.KubeSchedulerConfiguration{
				Profiles: []config.KubeSchedulerProfile{
					{
						PluginConfig: []config.PluginConfig{
							{
								Name: "DefaultPreemption",
								Args: &config.DefaultPreemptionArgs{},
							},
							{Name: "wasmPlugin1", Args: &runtime.Unknown{
								ContentType: runtime.ContentTypeJSON,
								Raw:         []byte(`{"guestURL":"http://example.com/plugin1.wasm"}`),
							}},
							{Name: "wasmPlugin2", Args: &runtime.Unknown{
								ContentType: runtime.ContentTypeJSON,
								Raw:         []byte(`{"guestURL":"http://example.com/plugin2.wasm"}`),
							}},
						},
						Plugins: &config.Plugins{
							MultiPoint: config.PluginSet{
								Enabled: []config.Plugin{
									{Name: "DefaultPreemption"},
									{Name: "wasmPlugin1"},
									{Name: "wasmPlugin2"},
								},
							},
						},
					},
				},
			},
			expected: 2,
		},
	}

	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			registry, err := getWasmRegistryFromUnversionedConfig(tt.cfg)
			require.NoError(t, err, "check error")
			if len(registry) != tt.expected {
				t.Errorf("expected %d plugins, got %d", tt.expected, len(registry))
			}
		})
	}
}
</file>

<file path="kube-scheduler-simulator/simulator/scheduler/config/wasm.go">
package config

import (
	"golang.org/x/xerrors"
	"k8s.io/apimachinery/pkg/util/sets"
	configv1 "k8s.io/kube-scheduler/config/v1"
	"k8s.io/kubernetes/pkg/scheduler/apis/config"
	"k8s.io/kubernetes/pkg/scheduler/apis/config/scheme"
	"k8s.io/kubernetes/pkg/scheduler/framework/runtime"
	wasm "sigs.k8s.io/kube-scheduler-wasm-extension/scheduler/plugin"
)

// RegisterWasmPlugins registers wasm plugins from the given configuration.
func RegisterWasmPlugins(versionedCfg *configv1.KubeSchedulerConfiguration) error {
	cfg := config.KubeSchedulerConfiguration{}
	if err := scheme.Scheme.Convert(versionedCfg, &cfg, nil); err != nil {
		return xerrors.Errorf("convert configuration: %w", err)
	}

	registry, err := getWasmRegistryFromUnversionedConfig(&cfg)
	if err != nil {
		return err
	}

	SetOutOfTreeRegistries(registry)

	return nil
}

// getWasmRegistryFromUnversionedConfig registers wasm plugins from the given unversioned configuration.
func getWasmRegistryFromUnversionedConfig(cfg *config.KubeSchedulerConfiguration) (runtime.Registry, error) {
	registry := runtime.Registry{}

	for _, profile := range cfg.Profiles {
		wasmplugins := sets.New[string]()
		// look for the wasm plugin in the plugin config.
		for _, config := range profile.PluginConfig {
			if err := runtime.DecodeInto(config.Args, &wasm.PluginConfig{}); err != nil {
				// not wasm plugin.
				continue
			}

			wasmplugins.Insert(config.Name)
		}

		// look for the wasm plugin in the enabled plugins.
		// (assuming that the wasm plugin is specified as a multi-point plugin.)
		for _, plugin := range profile.Plugins.MultiPoint.Enabled {
			if wasmplugins.Has(plugin.Name) {
				if err := registry.Register(plugin.Name, wasm.PluginFactory(plugin.Name)); err != nil {
					return nil, xerrors.Errorf("register plugin %s: %w", plugin.Name, err)
				}
			}
		}
	}

	return registry, nil
}
</file>

<file path="kube-scheduler-simulator/simulator/scheduler/extender/annotation/annotation.go">
package annotation

const (
	// ExtenderFilterResultAnnotationKey has the filtering result of extender.
	ExtenderFilterResultAnnotationKey = "kube-scheduler-simulator.sigs.k8s.io/extender-filter-result"
	// ExtenderPrioritizeResultAnnotationKey has the prioritizing result of extender.
	ExtenderPrioritizeResultAnnotationKey = "kube-scheduler-simulator.sigs.k8s.io/extender-prioritize-result"
	// ExtenderPreemptResultAnnotationKey has the preemption result of extender.
	ExtenderPreemptResultAnnotationKey = "kube-scheduler-simulator.sigs.k8s.io/extender-preempt-result"
	// ExtenderBindResultAnnotationKey has the binding result of extender.
	ExtenderBindResultAnnotationKey = "kube-scheduler-simulator.sigs.k8s.io/extender-bind-result"
)
</file>

<file path="kube-scheduler-simulator/simulator/scheduler/extender/mock_extender/extender.go">
// Code generated by MockGen. DO NOT EDIT.
// Source: sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler/extender (interfaces: Extender)
//
// Generated by this command:
//
//	mockgen -destination=./mock_extender/extender.go . Extender
//

// Package mock_extender is a generated GoMock package.
package mock_extender

import (
	reflect "reflect"

	gomock "go.uber.org/mock/gomock"
	v1 "k8s.io/kube-scheduler/extender/v1"
)

// MockExtender is a mock of Extender interface.
type MockExtender struct {
	ctrl     *gomock.Controller
	recorder *MockExtenderMockRecorder
	isgomock struct{}
}

// MockExtenderMockRecorder is the mock recorder for MockExtender.
type MockExtenderMockRecorder struct {
	mock *MockExtender
}

// NewMockExtender creates a new mock instance.
func NewMockExtender(ctrl *gomock.Controller) *MockExtender {
	mock := &MockExtender{ctrl: ctrl}
	mock.recorder = &MockExtenderMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockExtender) EXPECT() *MockExtenderMockRecorder {
	return m.recorder
}

// Bind mocks base method.
func (m *MockExtender) Bind(args v1.ExtenderBindingArgs) (*v1.ExtenderBindingResult, error) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Bind", args)
	ret0, _ := ret[0].(*v1.ExtenderBindingResult)
	ret1, _ := ret[1].(error)
	return ret0, ret1
}

// Bind indicates an expected call of Bind.
func (mr *MockExtenderMockRecorder) Bind(args any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Bind", reflect.TypeOf((*MockExtender)(nil).Bind), args)
}

// Filter mocks base method.
func (m *MockExtender) Filter(args v1.ExtenderArgs) (*v1.ExtenderFilterResult, error) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Filter", args)
	ret0, _ := ret[0].(*v1.ExtenderFilterResult)
	ret1, _ := ret[1].(error)
	return ret0, ret1
}

// Filter indicates an expected call of Filter.
func (mr *MockExtenderMockRecorder) Filter(args any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Filter", reflect.TypeOf((*MockExtender)(nil).Filter), args)
}

// Name mocks base method.
func (m *MockExtender) Name() string {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Name")
	ret0, _ := ret[0].(string)
	return ret0
}

// Name indicates an expected call of Name.
func (mr *MockExtenderMockRecorder) Name() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Name", reflect.TypeOf((*MockExtender)(nil).Name))
}

// Preempt mocks base method.
func (m *MockExtender) Preempt(args v1.ExtenderPreemptionArgs) (*v1.ExtenderPreemptionResult, error) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Preempt", args)
	ret0, _ := ret[0].(*v1.ExtenderPreemptionResult)
	ret1, _ := ret[1].(error)
	return ret0, ret1
}

// Preempt indicates an expected call of Preempt.
func (mr *MockExtenderMockRecorder) Preempt(args any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Preempt", reflect.TypeOf((*MockExtender)(nil).Preempt), args)
}

// Prioritize mocks base method.
func (m *MockExtender) Prioritize(args v1.ExtenderArgs) (*v1.HostPriorityList, error) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Prioritize", args)
	ret0, _ := ret[0].(*v1.HostPriorityList)
	ret1, _ := ret[1].(error)
	return ret0, ret1
}

// Prioritize indicates an expected call of Prioritize.
func (mr *MockExtenderMockRecorder) Prioritize(args any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Prioritize", reflect.TypeOf((*MockExtender)(nil).Prioritize), args)
}
</file>

<file path="kube-scheduler-simulator/simulator/scheduler/extender/mock_extender/resultstore.go">
// Code generated by MockGen. DO NOT EDIT.
// Source: ./resultstore/resultstore.go
//
// Generated by this command:
//
//	mockgen -package=mock_extender -source=./resultstore/resultstore.go -destination=./mock_extender/resultstore.go
//

// Package mock_extender is a generated GoMock package.
package mock_extender

import (
	reflect "reflect"

	gomock "go.uber.org/mock/gomock"
	v1 "k8s.io/api/core/v1"
	v10 "k8s.io/kube-scheduler/extender/v1"
)

// MockStore is a mock of Store interface.
type MockStore struct {
	ctrl     *gomock.Controller
	recorder *MockStoreMockRecorder
	isgomock struct{}
}

// MockStoreMockRecorder is the mock recorder for MockStore.
type MockStoreMockRecorder struct {
	mock *MockStore
}

// NewMockStore creates a new mock instance.
func NewMockStore(ctrl *gomock.Controller) *MockStore {
	mock := &MockStore{ctrl: ctrl}
	mock.recorder = &MockStoreMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockStore) EXPECT() *MockStoreMockRecorder {
	return m.recorder
}

// AddBindResult mocks base method.
func (m *MockStore) AddBindResult(args v10.ExtenderBindingArgs, result v10.ExtenderBindingResult, hostName string) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "AddBindResult", args, result, hostName)
}

// AddBindResult indicates an expected call of AddBindResult.
func (mr *MockStoreMockRecorder) AddBindResult(args, result, hostName any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "AddBindResult", reflect.TypeOf((*MockStore)(nil).AddBindResult), args, result, hostName)
}

// AddFilterResult mocks base method.
func (m *MockStore) AddFilterResult(args v10.ExtenderArgs, result v10.ExtenderFilterResult, hostName string) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "AddFilterResult", args, result, hostName)
}

// AddFilterResult indicates an expected call of AddFilterResult.
func (mr *MockStoreMockRecorder) AddFilterResult(args, result, hostName any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "AddFilterResult", reflect.TypeOf((*MockStore)(nil).AddFilterResult), args, result, hostName)
}

// AddPreemptResult mocks base method.
func (m *MockStore) AddPreemptResult(args v10.ExtenderPreemptionArgs, result v10.ExtenderPreemptionResult, hostName string) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "AddPreemptResult", args, result, hostName)
}

// AddPreemptResult indicates an expected call of AddPreemptResult.
func (mr *MockStoreMockRecorder) AddPreemptResult(args, result, hostName any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "AddPreemptResult", reflect.TypeOf((*MockStore)(nil).AddPreemptResult), args, result, hostName)
}

// AddPrioritizeResult mocks base method.
func (m *MockStore) AddPrioritizeResult(args v10.ExtenderArgs, result v10.HostPriorityList, hostName string) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "AddPrioritizeResult", args, result, hostName)
}

// AddPrioritizeResult indicates an expected call of AddPrioritizeResult.
func (mr *MockStoreMockRecorder) AddPrioritizeResult(args, result, hostName any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "AddPrioritizeResult", reflect.TypeOf((*MockStore)(nil).AddPrioritizeResult), args, result, hostName)
}

// DeleteData mocks base method.
func (m *MockStore) DeleteData(pod v1.Pod) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "DeleteData", pod)
}

// DeleteData indicates an expected call of DeleteData.
func (mr *MockStoreMockRecorder) DeleteData(pod any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "DeleteData", reflect.TypeOf((*MockStore)(nil).DeleteData), pod)
}

// GetStoredResult mocks base method.
func (m *MockStore) GetStoredResult(pod *v1.Pod) map[string]string {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "GetStoredResult", pod)
	ret0, _ := ret[0].(map[string]string)
	return ret0
}

// GetStoredResult indicates an expected call of GetStoredResult.
func (mr *MockStoreMockRecorder) GetStoredResult(pod any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "GetStoredResult", reflect.TypeOf((*MockStore)(nil).GetStoredResult), pod)
}
</file>

<file path="kube-scheduler-simulator/simulator/scheduler/extender/resultstore/resultstore_test.go">
package resultstore

import (
	"encoding/json"
	"sync"
	"testing"

	"github.com/stretchr/testify/assert"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	extenderv1 "k8s.io/kube-scheduler/extender/v1"

	"sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler/extender/annotation"
)

func TestStore_GetStoredResult(t *testing.T) {
	t.Parallel()
	podName := "pod1"
	namespace := "default"
	tests := []struct {
		name           string
		result         map[key]*result
		newObj         *corev1.Pod
		wantAnnotation map[string]string
	}{
		{
			name: "success",
			result: map[key]*result{
				"default/pod1": {
					filter: map[string]extenderv1.ExtenderFilterResult{
						"node0": {
							Nodes:                      &corev1.NodeList{Items: []corev1.Node{{ObjectMeta: metav1.ObjectMeta{Name: "nodename"}}}},
							NodeNames:                  &[]string{"node1"},
							FailedNodes:                map[string]string{"foo": "bar"},
							FailedAndUnresolvableNodes: map[string]string{"baz": "qux"},
							Error:                      "myerror",
						},
					},
					prioritize: map[string]extenderv1.HostPriorityList{
						"node0": {
							{
								Host:  "node1",
								Score: 1.0,
							},
						},
					},
					preempt: map[string]extenderv1.ExtenderPreemptionResult{
						"node0": {
							NodeNameToMetaVictims: map[string]*extenderv1.MetaVictims{"foo": {Pods: []*extenderv1.MetaPod{{UID: "myuid"}}, NumPDBViolations: 1}},
						},
					},
					bind: map[string]extenderv1.ExtenderBindingResult{
						"node0": {
							Error: "myerror",
						},
					},
				},
			},
			newObj: &corev1.Pod{
				ObjectMeta: metav1.ObjectMeta{
					Name:      podName,
					Namespace: namespace,
				},
			},
			wantAnnotation: map[string]string{
				annotation.ExtenderFilterResultAnnotationKey: func() string {
					r := map[string]extenderv1.ExtenderFilterResult{
						"node0": {
							Nodes:                      &corev1.NodeList{Items: []corev1.Node{{ObjectMeta: metav1.ObjectMeta{Name: "nodename"}}}},
							NodeNames:                  &[]string{"node1"},
							FailedNodes:                map[string]string{"foo": "bar"},
							FailedAndUnresolvableNodes: map[string]string{"baz": "qux"},
							Error:                      "myerror",
						},
					}
					d, _ := json.Marshal(r)
					return string(d)
				}(),
				annotation.ExtenderPrioritizeResultAnnotationKey: func() string {
					r := map[string]extenderv1.HostPriorityList{
						"node0": {
							{
								Host:  "node1",
								Score: 1.0,
							},
						},
					}
					d, _ := json.Marshal(r)
					return string(d)
				}(),
				annotation.ExtenderPreemptResultAnnotationKey: func() string {
					r := map[string]extenderv1.ExtenderPreemptionResult{
						"node0": {
							NodeNameToMetaVictims: map[string]*extenderv1.MetaVictims{"foo": {Pods: []*extenderv1.MetaPod{{UID: "myuid"}}, NumPDBViolations: 1}},
						},
					}
					d, _ := json.Marshal(r)
					return string(d)
				}(),
				annotation.ExtenderBindResultAnnotationKey: func() string {
					r := map[string]extenderv1.ExtenderBindingResult{
						"node0": {
							Error: "myerror",
						},
					}
					d, _ := json.Marshal(r)
					return string(d)
				}(),
			},
		},
		{
			name:   "do nothing if store doesn't have data",
			result: map[key]*result{},
			newObj: &corev1.Pod{
				ObjectMeta: metav1.ObjectMeta{
					Name:      podName,
					Namespace: namespace,
				},
			},
		},
		{
			name: "success without some data on store",
			result: map[key]*result{
				"default/pod1": {
					filter: map[string]extenderv1.ExtenderFilterResult{
						"node0": {
							Nodes:                      &corev1.NodeList{Items: []corev1.Node{{ObjectMeta: metav1.ObjectMeta{Name: "nodename"}}}},
							NodeNames:                  &[]string{"node1"},
							FailedNodes:                map[string]string{"foo": "bar"},
							FailedAndUnresolvableNodes: map[string]string{"baz": "qux"},
							Error:                      "myerror",
						},
					},
					prioritize: map[string]extenderv1.HostPriorityList{},
					preempt:    map[string]extenderv1.ExtenderPreemptionResult{},
					bind:       map[string]extenderv1.ExtenderBindingResult{},
				},
			},
			newObj: &corev1.Pod{
				ObjectMeta: metav1.ObjectMeta{
					Name:      podName,
					Namespace: namespace,
				},
			},
			wantAnnotation: map[string]string{
				annotation.ExtenderFilterResultAnnotationKey: func() string {
					r := map[string]extenderv1.ExtenderFilterResult{
						"node0": {
							Nodes:                      &corev1.NodeList{Items: []corev1.Node{{ObjectMeta: metav1.ObjectMeta{Name: "nodename"}}}},
							NodeNames:                  &[]string{"node1"},
							FailedNodes:                map[string]string{"foo": "bar"},
							FailedAndUnresolvableNodes: map[string]string{"baz": "qux"},
							Error:                      "myerror",
						},
					}
					d, _ := json.Marshal(r)
					return string(d)
				}(),
				annotation.ExtenderPrioritizeResultAnnotationKey: "{}",
				annotation.ExtenderPreemptResultAnnotationKey:    "{}",
				annotation.ExtenderBindResultAnnotationKey:       "{}",
			},
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			s := &store{
				mu:      new(sync.Mutex),
				results: tt.result,
			}
			p := tt.newObj
			result := s.GetStoredResult(p)

			assert.Equal(t, tt.wantAnnotation, result)
		})
	}
}

func TestStore_AddFilterResult(t *testing.T) {
	t.Parallel()
	podName := "pod1"
	namespace := "default"
	tests := []struct {
		name          string
		hostname      string
		args          extenderv1.ExtenderArgs
		filterResult  extenderv1.ExtenderFilterResult
		prepareResult map[key]*result
		wantResult    map[key]*result
	}{
		{
			name:     "success to add the result",
			hostname: "extenderserver",
			args: extenderv1.ExtenderArgs{
				Pod: &corev1.Pod{
					ObjectMeta: metav1.ObjectMeta{
						Name:      podName,
						Namespace: namespace,
					},
				},
			},
			filterResult: extenderv1.ExtenderFilterResult{
				Nodes:                      &corev1.NodeList{Items: []corev1.Node{{ObjectMeta: metav1.ObjectMeta{Name: "nodename"}}}},
				NodeNames:                  &[]string{"node1"},
				FailedNodes:                map[string]string{"foo": "bar"},
				FailedAndUnresolvableNodes: map[string]string{"baz": "qux"},
				Error:                      "myerror",
			},
			prepareResult: map[key]*result{},
			wantResult: map[key]*result{
				"default/pod1": {
					filter: map[string]extenderv1.ExtenderFilterResult{
						"extenderserver": {
							Nodes:                      &corev1.NodeList{Items: []corev1.Node{{ObjectMeta: metav1.ObjectMeta{Name: "nodename"}}}},
							NodeNames:                  &[]string{"node1"},
							FailedNodes:                map[string]string{"foo": "bar"},
							FailedAndUnresolvableNodes: map[string]string{"baz": "qux"},
							Error:                      "myerror",
						},
					},
					prioritize: map[string]extenderv1.HostPriorityList{},
					preempt:    map[string]extenderv1.ExtenderPreemptionResult{},
					bind:       map[string]extenderv1.ExtenderBindingResult{},
				},
			},
		},
		{
			name:     "overwrite to the already stored data which has the same key and hostname",
			hostname: "extenderserver",
			args: extenderv1.ExtenderArgs{
				Pod: &corev1.Pod{
					ObjectMeta: metav1.ObjectMeta{
						Name:      podName,
						Namespace: namespace,
					},
				},
			},
			filterResult: extenderv1.ExtenderFilterResult{
				Nodes:                      &corev1.NodeList{Items: []corev1.Node{{ObjectMeta: metav1.ObjectMeta{Name: "nodename"}}}},
				NodeNames:                  &[]string{"node1"},
				FailedNodes:                map[string]string{"foo": "bar"},
				FailedAndUnresolvableNodes: map[string]string{"baz": "qux"},
				Error:                      "myerror",
			},
			prepareResult: map[key]*result{
				"default/pod1": {
					filter: map[string]extenderv1.ExtenderFilterResult{
						"extenderserver": {
							Nodes:                      &corev1.NodeList{Items: []corev1.Node{{ObjectMeta: metav1.ObjectMeta{Name: "nodename0"}}}},
							NodeNames:                  &[]string{"node0"},
							FailedNodes:                map[string]string{"foo": "foo"},
							FailedAndUnresolvableNodes: map[string]string{"baz": "foo"},
							Error:                      "",
						},
					},
					prioritize: map[string]extenderv1.HostPriorityList{},
					preempt:    map[string]extenderv1.ExtenderPreemptionResult{},
					bind:       map[string]extenderv1.ExtenderBindingResult{},
				},
			},
			wantResult: map[key]*result{
				"default/pod1": {
					filter: map[string]extenderv1.ExtenderFilterResult{
						"extenderserver": {
							Nodes:                      &corev1.NodeList{Items: []corev1.Node{{ObjectMeta: metav1.ObjectMeta{Name: "nodename"}}}},
							NodeNames:                  &[]string{"node1"},
							FailedNodes:                map[string]string{"foo": "bar"},
							FailedAndUnresolvableNodes: map[string]string{"baz": "qux"},
							Error:                      "myerror",
						},
					},
					prioritize: map[string]extenderv1.HostPriorityList{},
					preempt:    map[string]extenderv1.ExtenderPreemptionResult{},
					bind:       map[string]extenderv1.ExtenderBindingResult{},
				},
			},
		},
		{
			name:     "shouldn't overwrite to the already stored data which has the same key and different hostname",
			hostname: "extenderserver",
			args: extenderv1.ExtenderArgs{
				Pod: &corev1.Pod{
					ObjectMeta: metav1.ObjectMeta{
						Name:      podName,
						Namespace: namespace,
					},
				},
			},
			filterResult: extenderv1.ExtenderFilterResult{
				Nodes:                      &corev1.NodeList{Items: []corev1.Node{{ObjectMeta: metav1.ObjectMeta{Name: "nodename"}}}},
				NodeNames:                  &[]string{"node1"},
				FailedNodes:                map[string]string{"foo": "bar"},
				FailedAndUnresolvableNodes: map[string]string{"baz": "qux"},
				Error:                      "myerror",
			},
			prepareResult: map[key]*result{
				"default/pod1": {
					filter: map[string]extenderv1.ExtenderFilterResult{
						"different-extenderserver": {
							Nodes:                      &corev1.NodeList{Items: []corev1.Node{{ObjectMeta: metav1.ObjectMeta{Name: "nodename0"}}}},
							NodeNames:                  &[]string{"node0"},
							FailedNodes:                map[string]string{"foo": "foo"},
							FailedAndUnresolvableNodes: map[string]string{"baz": "foo"},
							Error:                      "",
						},
					},
					prioritize: map[string]extenderv1.HostPriorityList{},
					preempt:    map[string]extenderv1.ExtenderPreemptionResult{},
					bind:       map[string]extenderv1.ExtenderBindingResult{},
				},
			},
			wantResult: map[key]*result{
				"default/pod1": {
					filter: map[string]extenderv1.ExtenderFilterResult{
						"extenderserver": {
							Nodes:                      &corev1.NodeList{Items: []corev1.Node{{ObjectMeta: metav1.ObjectMeta{Name: "nodename"}}}},
							NodeNames:                  &[]string{"node1"},
							FailedNodes:                map[string]string{"foo": "bar"},
							FailedAndUnresolvableNodes: map[string]string{"baz": "qux"},
							Error:                      "myerror",
						},
						"different-extenderserver": {
							Nodes:                      &corev1.NodeList{Items: []corev1.Node{{ObjectMeta: metav1.ObjectMeta{Name: "nodename0"}}}},
							NodeNames:                  &[]string{"node0"},
							FailedNodes:                map[string]string{"foo": "foo"},
							FailedAndUnresolvableNodes: map[string]string{"baz": "foo"},
							Error:                      "",
						},
					},
					prioritize: map[string]extenderv1.HostPriorityList{},
					preempt:    map[string]extenderv1.ExtenderPreemptionResult{},
					bind:       map[string]extenderv1.ExtenderBindingResult{},
				},
			},
		},
		{
			name:     "overwrite to the already stored data which has the different key and same hostname",
			hostname: "extenderserver",
			args: extenderv1.ExtenderArgs{
				Pod: &corev1.Pod{
					ObjectMeta: metav1.ObjectMeta{
						Name:      podName,
						Namespace: namespace,
					},
				},
			},
			filterResult: extenderv1.ExtenderFilterResult{
				Nodes:                      &corev1.NodeList{Items: []corev1.Node{{ObjectMeta: metav1.ObjectMeta{Name: "nodename"}}}},
				NodeNames:                  &[]string{"node1"},
				FailedNodes:                map[string]string{"foo": "bar"},
				FailedAndUnresolvableNodes: map[string]string{"baz": "qux"},
				Error:                      "myerror",
			},
			prepareResult: map[key]*result{
				"default/pod2": {
					filter: map[string]extenderv1.ExtenderFilterResult{
						"extenderserver": {
							Nodes:                      &corev1.NodeList{Items: []corev1.Node{{ObjectMeta: metav1.ObjectMeta{Name: "nodename0"}}}},
							NodeNames:                  &[]string{"node0"},
							FailedNodes:                map[string]string{"foo": "foo"},
							FailedAndUnresolvableNodes: map[string]string{"baz": "foo"},
							Error:                      "",
						},
					},
					prioritize: map[string]extenderv1.HostPriorityList{},
					preempt:    map[string]extenderv1.ExtenderPreemptionResult{},
					bind:       map[string]extenderv1.ExtenderBindingResult{},
				},
			},
			wantResult: map[key]*result{
				"default/pod1": {
					filter: map[string]extenderv1.ExtenderFilterResult{
						"extenderserver": {
							Nodes:                      &corev1.NodeList{Items: []corev1.Node{{ObjectMeta: metav1.ObjectMeta{Name: "nodename"}}}},
							NodeNames:                  &[]string{"node1"},
							FailedNodes:                map[string]string{"foo": "bar"},
							FailedAndUnresolvableNodes: map[string]string{"baz": "qux"},
							Error:                      "myerror",
						},
					},
					prioritize: map[string]extenderv1.HostPriorityList{},
					preempt:    map[string]extenderv1.ExtenderPreemptionResult{},
					bind:       map[string]extenderv1.ExtenderBindingResult{},
				},
				"default/pod2": {
					filter: map[string]extenderv1.ExtenderFilterResult{
						"extenderserver": {
							Nodes:                      &corev1.NodeList{Items: []corev1.Node{{ObjectMeta: metav1.ObjectMeta{Name: "nodename0"}}}},
							NodeNames:                  &[]string{"node0"},
							FailedNodes:                map[string]string{"foo": "foo"},
							FailedAndUnresolvableNodes: map[string]string{"baz": "foo"},
							Error:                      "",
						},
					},
					prioritize: map[string]extenderv1.HostPriorityList{},
					preempt:    map[string]extenderv1.ExtenderPreemptionResult{},
					bind:       map[string]extenderv1.ExtenderBindingResult{},
				},
			},
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			s := &store{
				mu:      new(sync.Mutex),
				results: tt.prepareResult,
			}
			s.AddFilterResult(tt.args, tt.filterResult, tt.hostname)

			assert.Equal(t, tt.wantResult, s.results)
		})
	}
}

func TestStore_AddPrioritizeResult(t *testing.T) {
	t.Parallel()
	podName := "pod1"
	namespace := "default"
	tests := []struct {
		name          string
		hostname      string
		args          extenderv1.ExtenderArgs
		prioritize    extenderv1.HostPriorityList
		prepareResult map[key]*result
		wantResult    map[key]*result
	}{
		{
			name:     "success to add the result",
			hostname: "extenderserver",
			args: extenderv1.ExtenderArgs{
				Pod: &corev1.Pod{
					ObjectMeta: metav1.ObjectMeta{
						Name:      podName,
						Namespace: namespace,
					},
				},
			},
			prioritize: extenderv1.HostPriorityList{
				{
					Host:  "node1",
					Score: 1.0,
				},
			},
			prepareResult: map[key]*result{},
			wantResult: map[key]*result{
				"default/pod1": {
					filter: map[string]extenderv1.ExtenderFilterResult{},
					prioritize: map[string]extenderv1.HostPriorityList{
						"extenderserver": {
							{
								Host:  "node1",
								Score: 1.0,
							},
						},
					},
					preempt: map[string]extenderv1.ExtenderPreemptionResult{},
					bind:    map[string]extenderv1.ExtenderBindingResult{},
				},
			},
		},
		{
			name:     "overwrite to the already stored data which has the same key and hostname",
			hostname: "extenderserver",
			args: extenderv1.ExtenderArgs{
				Pod: &corev1.Pod{
					ObjectMeta: metav1.ObjectMeta{
						Name:      podName,
						Namespace: namespace,
					},
				},
			},
			prioritize: extenderv1.HostPriorityList{
				{
					Host:  "node1",
					Score: 1.0,
				},
			},
			prepareResult: map[key]*result{
				"default/pod1": {
					filter: map[string]extenderv1.ExtenderFilterResult{},
					prioritize: map[string]extenderv1.HostPriorityList{
						"extenderserver": {
							{
								Host:  "node0",
								Score: 1.0,
							},
						},
					},
					preempt: map[string]extenderv1.ExtenderPreemptionResult{},
					bind:    map[string]extenderv1.ExtenderBindingResult{},
				},
			},
			wantResult: map[key]*result{
				"default/pod1": {
					filter: map[string]extenderv1.ExtenderFilterResult{},
					prioritize: map[string]extenderv1.HostPriorityList{
						"extenderserver": {
							{
								Host:  "node1",
								Score: 1.0,
							},
						},
					},
					preempt: map[string]extenderv1.ExtenderPreemptionResult{},
					bind:    map[string]extenderv1.ExtenderBindingResult{},
				},
			},
		},
		{
			name:     "shouldn't overwrite to the already stored data which has the same key and different hostname",
			hostname: "extenderserver",
			args: extenderv1.ExtenderArgs{
				Pod: &corev1.Pod{
					ObjectMeta: metav1.ObjectMeta{
						Name:      podName,
						Namespace: namespace,
					},
				},
			},
			prioritize: extenderv1.HostPriorityList{
				{
					Host:  "node1",
					Score: 1.0,
				},
			},
			prepareResult: map[key]*result{
				"default/pod1": {
					filter: map[string]extenderv1.ExtenderFilterResult{},
					prioritize: map[string]extenderv1.HostPriorityList{
						"different-extenderserver": {
							{
								Host:  "node0",
								Score: 1.0,
							},
						},
					},
					preempt: map[string]extenderv1.ExtenderPreemptionResult{},
					bind:    map[string]extenderv1.ExtenderBindingResult{},
				},
			},
			wantResult: map[key]*result{
				"default/pod1": {
					filter: map[string]extenderv1.ExtenderFilterResult{},
					prioritize: map[string]extenderv1.HostPriorityList{
						"extenderserver": {
							{
								Host:  "node1",
								Score: 1.0,
							},
						},
						"different-extenderserver": {
							{
								Host:  "node0",
								Score: 1.0,
							},
						},
					},
					preempt: map[string]extenderv1.ExtenderPreemptionResult{},
					bind:    map[string]extenderv1.ExtenderBindingResult{},
				},
			},
		},
		{
			name:     "overwrite to the already stored data which has the different key and same hostname",
			hostname: "extenderserver",
			args: extenderv1.ExtenderArgs{
				Pod: &corev1.Pod{
					ObjectMeta: metav1.ObjectMeta{
						Name:      podName,
						Namespace: namespace,
					},
				},
			},
			prioritize: extenderv1.HostPriorityList{
				{
					Host:  "node1",
					Score: 1.0,
				},
			},
			prepareResult: map[key]*result{
				"default/pod2": {
					filter: map[string]extenderv1.ExtenderFilterResult{},
					prioritize: map[string]extenderv1.HostPriorityList{
						"extenderserver": {
							{
								Host:  "node0",
								Score: 1.0,
							},
						},
					},
					preempt: map[string]extenderv1.ExtenderPreemptionResult{},
					bind:    map[string]extenderv1.ExtenderBindingResult{},
				},
			},
			wantResult: map[key]*result{
				"default/pod1": {
					filter: map[string]extenderv1.ExtenderFilterResult{},
					prioritize: map[string]extenderv1.HostPriorityList{
						"extenderserver": {
							{
								Host:  "node1",
								Score: 1.0,
							},
						},
					},
					preempt: map[string]extenderv1.ExtenderPreemptionResult{},
					bind:    map[string]extenderv1.ExtenderBindingResult{},
				},
				"default/pod2": {
					filter: map[string]extenderv1.ExtenderFilterResult{},
					prioritize: map[string]extenderv1.HostPriorityList{
						"extenderserver": {
							{
								Host:  "node0",
								Score: 1.0,
							},
						},
					},
					preempt: map[string]extenderv1.ExtenderPreemptionResult{},
					bind:    map[string]extenderv1.ExtenderBindingResult{},
				},
			},
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			s := &store{
				mu:      new(sync.Mutex),
				results: tt.prepareResult,
			}
			s.AddPrioritizeResult(tt.args, tt.prioritize, tt.hostname)

			assert.Equal(t, tt.wantResult, s.results)
		})
	}
}

func TestStore_AddPreemptResult(t *testing.T) {
	t.Parallel()
	podName := "pod1"
	namespace := "default"
	tests := []struct {
		name             string
		hostname         string
		args             extenderv1.ExtenderPreemptionArgs
		preemptionResult extenderv1.ExtenderPreemptionResult
		prepareResult    map[key]*result
		wantResult       map[key]*result
	}{
		{
			name:     "success to add the result",
			hostname: "extenderserver",
			args: extenderv1.ExtenderPreemptionArgs{
				Pod: &corev1.Pod{
					ObjectMeta: metav1.ObjectMeta{
						Name:      podName,
						Namespace: namespace,
					},
				},
			},
			preemptionResult: extenderv1.ExtenderPreemptionResult{
				NodeNameToMetaVictims: map[string]*extenderv1.MetaVictims{"foo": {Pods: []*extenderv1.MetaPod{{UID: "myuid"}}, NumPDBViolations: 1}},
			},
			prepareResult: map[key]*result{},
			wantResult: map[key]*result{
				"default/pod1": {
					filter:     map[string]extenderv1.ExtenderFilterResult{},
					prioritize: map[string]extenderv1.HostPriorityList{},
					preempt: map[string]extenderv1.ExtenderPreemptionResult{
						"extenderserver": {
							NodeNameToMetaVictims: map[string]*extenderv1.MetaVictims{"foo": {Pods: []*extenderv1.MetaPod{{UID: "myuid"}}, NumPDBViolations: 1}},
						},
					},
					bind: map[string]extenderv1.ExtenderBindingResult{},
				},
			},
		},
		{
			name:     "overwrite to the already stored data which has the same key and hostname",
			hostname: "extenderserver",
			args: extenderv1.ExtenderPreemptionArgs{
				Pod: &corev1.Pod{
					ObjectMeta: metav1.ObjectMeta{
						Name:      podName,
						Namespace: namespace,
					},
				},
			},
			preemptionResult: extenderv1.ExtenderPreemptionResult{
				NodeNameToMetaVictims: map[string]*extenderv1.MetaVictims{"foo": {Pods: []*extenderv1.MetaPod{{UID: "myuid"}}, NumPDBViolations: 1}},
			},
			prepareResult: map[key]*result{
				"default/pod1": {
					filter:     map[string]extenderv1.ExtenderFilterResult{},
					prioritize: map[string]extenderv1.HostPriorityList{},
					preempt: map[string]extenderv1.ExtenderPreemptionResult{
						"extenderserver": {
							NodeNameToMetaVictims: map[string]*extenderv1.MetaVictims{"bar": {Pods: []*extenderv1.MetaPod{{UID: "myuid"}}, NumPDBViolations: 1}},
						},
					},
					bind: map[string]extenderv1.ExtenderBindingResult{},
				},
			},
			wantResult: map[key]*result{
				"default/pod1": {
					filter:     map[string]extenderv1.ExtenderFilterResult{},
					prioritize: map[string]extenderv1.HostPriorityList{},
					preempt: map[string]extenderv1.ExtenderPreemptionResult{
						"extenderserver": {
							NodeNameToMetaVictims: map[string]*extenderv1.MetaVictims{"foo": {Pods: []*extenderv1.MetaPod{{UID: "myuid"}}, NumPDBViolations: 1}},
						},
					},
					bind: map[string]extenderv1.ExtenderBindingResult{},
				},
			},
		},
		{
			name:     "shouldn't overwrite to the already stored data which has the same key and different hostname",
			hostname: "extenderserver",
			args: extenderv1.ExtenderPreemptionArgs{
				Pod: &corev1.Pod{
					ObjectMeta: metav1.ObjectMeta{
						Name:      podName,
						Namespace: namespace,
					},
				},
			},
			preemptionResult: extenderv1.ExtenderPreemptionResult{
				NodeNameToMetaVictims: map[string]*extenderv1.MetaVictims{"foo": {Pods: []*extenderv1.MetaPod{{UID: "myuid"}}, NumPDBViolations: 1}},
			},
			prepareResult: map[key]*result{
				"default/pod1": {
					filter:     map[string]extenderv1.ExtenderFilterResult{},
					prioritize: map[string]extenderv1.HostPriorityList{},
					preempt: map[string]extenderv1.ExtenderPreemptionResult{
						"different-extenderserver": {
							NodeNameToMetaVictims: map[string]*extenderv1.MetaVictims{"bar": {Pods: []*extenderv1.MetaPod{{UID: "myuid"}}, NumPDBViolations: 1}},
						},
					},
					bind: map[string]extenderv1.ExtenderBindingResult{},
				},
			},
			wantResult: map[key]*result{
				"default/pod1": {
					filter:     map[string]extenderv1.ExtenderFilterResult{},
					prioritize: map[string]extenderv1.HostPriorityList{},
					preempt: map[string]extenderv1.ExtenderPreemptionResult{
						"extenderserver": {
							NodeNameToMetaVictims: map[string]*extenderv1.MetaVictims{"foo": {Pods: []*extenderv1.MetaPod{{UID: "myuid"}}, NumPDBViolations: 1}},
						},
						"different-extenderserver": {
							NodeNameToMetaVictims: map[string]*extenderv1.MetaVictims{"bar": {Pods: []*extenderv1.MetaPod{{UID: "myuid"}}, NumPDBViolations: 1}},
						},
					},
					bind: map[string]extenderv1.ExtenderBindingResult{},
				},
			},
		},
		{
			name:     "overwrite to the already stored data which has the different key and same hostname",
			hostname: "extenderserver",
			args: extenderv1.ExtenderPreemptionArgs{
				Pod: &corev1.Pod{
					ObjectMeta: metav1.ObjectMeta{
						Name:      podName,
						Namespace: namespace,
					},
				},
			},
			preemptionResult: extenderv1.ExtenderPreemptionResult{
				NodeNameToMetaVictims: map[string]*extenderv1.MetaVictims{"foo": {Pods: []*extenderv1.MetaPod{{UID: "myuid"}}, NumPDBViolations: 1}},
			},
			prepareResult: map[key]*result{
				"default/pod2": {
					filter:     map[string]extenderv1.ExtenderFilterResult{},
					prioritize: map[string]extenderv1.HostPriorityList{},
					preempt: map[string]extenderv1.ExtenderPreemptionResult{
						"extenderserver": {
							NodeNameToMetaVictims: map[string]*extenderv1.MetaVictims{"bar": {Pods: []*extenderv1.MetaPod{{UID: "myuid"}}, NumPDBViolations: 1}},
						},
					},
					bind: map[string]extenderv1.ExtenderBindingResult{},
				},
			},
			wantResult: map[key]*result{
				"default/pod1": {
					filter:     map[string]extenderv1.ExtenderFilterResult{},
					prioritize: map[string]extenderv1.HostPriorityList{},
					preempt: map[string]extenderv1.ExtenderPreemptionResult{
						"extenderserver": {
							NodeNameToMetaVictims: map[string]*extenderv1.MetaVictims{"foo": {Pods: []*extenderv1.MetaPod{{UID: "myuid"}}, NumPDBViolations: 1}},
						},
					},
					bind: map[string]extenderv1.ExtenderBindingResult{},
				},
				"default/pod2": {
					filter:     map[string]extenderv1.ExtenderFilterResult{},
					prioritize: map[string]extenderv1.HostPriorityList{},
					preempt: map[string]extenderv1.ExtenderPreemptionResult{
						"extenderserver": {
							NodeNameToMetaVictims: map[string]*extenderv1.MetaVictims{"bar": {Pods: []*extenderv1.MetaPod{{UID: "myuid"}}, NumPDBViolations: 1}},
						},
					},
					bind: map[string]extenderv1.ExtenderBindingResult{},
				},
			},
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			s := &store{
				mu:      new(sync.Mutex),
				results: tt.prepareResult,
			}
			s.AddPreemptResult(tt.args, tt.preemptionResult, tt.hostname)

			assert.Equal(t, tt.wantResult, s.results)
		})
	}
}

func TestStore_AddBindResult(t *testing.T) {
	t.Parallel()
	podName := "pod1"
	namespace := "default"
	tests := []struct {
		name          string
		hostname      string
		args          extenderv1.ExtenderBindingArgs
		bindingResult extenderv1.ExtenderBindingResult
		prepareResult map[key]*result
		wantResult    map[key]*result
	}{
		{
			name:     "success to add the result",
			hostname: "extenderserver",
			args: extenderv1.ExtenderBindingArgs{
				PodName:      podName,
				PodNamespace: namespace,
			},
			bindingResult: extenderv1.ExtenderBindingResult{
				Error: "myerror",
			},
			prepareResult: map[key]*result{},
			wantResult: map[key]*result{
				"default/pod1": {
					filter:     map[string]extenderv1.ExtenderFilterResult{},
					prioritize: map[string]extenderv1.HostPriorityList{},
					preempt:    map[string]extenderv1.ExtenderPreemptionResult{},
					bind: map[string]extenderv1.ExtenderBindingResult{
						"extenderserver": {
							Error: "myerror",
						},
					},
				},
			},
		},
		{
			name:     "overwrite to the already stored data which has the same key and hostname",
			hostname: "extenderserver",
			args: extenderv1.ExtenderBindingArgs{
				PodName:      podName,
				PodNamespace: namespace,
			},
			bindingResult: extenderv1.ExtenderBindingResult{
				Error: "myerror",
			},
			prepareResult: map[key]*result{
				"default/pod1": {
					filter:     map[string]extenderv1.ExtenderFilterResult{},
					prioritize: map[string]extenderv1.HostPriorityList{},
					preempt:    map[string]extenderv1.ExtenderPreemptionResult{},
					bind: map[string]extenderv1.ExtenderBindingResult{
						"extenderserver": {
							Error: "myerror1",
						},
					},
				},
			},
			wantResult: map[key]*result{
				"default/pod1": {
					filter:     map[string]extenderv1.ExtenderFilterResult{},
					prioritize: map[string]extenderv1.HostPriorityList{},
					preempt:    map[string]extenderv1.ExtenderPreemptionResult{},
					bind: map[string]extenderv1.ExtenderBindingResult{
						"extenderserver": {
							Error: "myerror",
						},
					},
				},
			},
		},
		{
			name:     "shouldn't overwrite to the already stored data which has the same key and different hostname",
			hostname: "extenderserver",
			args: extenderv1.ExtenderBindingArgs{
				PodName:      podName,
				PodNamespace: namespace,
			},
			bindingResult: extenderv1.ExtenderBindingResult{
				Error: "myerror",
			},
			prepareResult: map[key]*result{
				"default/pod1": {
					filter:     map[string]extenderv1.ExtenderFilterResult{},
					prioritize: map[string]extenderv1.HostPriorityList{},
					preempt:    map[string]extenderv1.ExtenderPreemptionResult{},
					bind: map[string]extenderv1.ExtenderBindingResult{
						"different-extenderserver": {
							Error: "myerror1",
						},
					},
				},
			},
			wantResult: map[key]*result{
				"default/pod1": {
					filter:     map[string]extenderv1.ExtenderFilterResult{},
					prioritize: map[string]extenderv1.HostPriorityList{},
					preempt:    map[string]extenderv1.ExtenderPreemptionResult{},
					bind: map[string]extenderv1.ExtenderBindingResult{
						"extenderserver": {
							Error: "myerror",
						},
						"different-extenderserver": {
							Error: "myerror1",
						},
					},
				},
			},
		},
		{
			name:     "overwrite to the already stored data which has the different key and same hostname",
			hostname: "extenderserver",
			args: extenderv1.ExtenderBindingArgs{
				PodName:      podName,
				PodNamespace: namespace,
			},
			bindingResult: extenderv1.ExtenderBindingResult{
				Error: "myerror",
			},
			prepareResult: map[key]*result{
				"default/pod2": {
					filter:     map[string]extenderv1.ExtenderFilterResult{},
					prioritize: map[string]extenderv1.HostPriorityList{},
					preempt:    map[string]extenderv1.ExtenderPreemptionResult{},
					bind: map[string]extenderv1.ExtenderBindingResult{
						"extenderserver": {
							Error: "myerror",
						},
					},
				},
			},
			wantResult: map[key]*result{
				"default/pod1": {
					filter:     map[string]extenderv1.ExtenderFilterResult{},
					prioritize: map[string]extenderv1.HostPriorityList{},
					preempt:    map[string]extenderv1.ExtenderPreemptionResult{},
					bind: map[string]extenderv1.ExtenderBindingResult{
						"extenderserver": {
							Error: "myerror",
						},
					},
				},
				"default/pod2": {
					filter:     map[string]extenderv1.ExtenderFilterResult{},
					prioritize: map[string]extenderv1.HostPriorityList{},
					preempt:    map[string]extenderv1.ExtenderPreemptionResult{},
					bind: map[string]extenderv1.ExtenderBindingResult{
						"extenderserver": {
							Error: "myerror",
						},
					},
				},
			},
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			s := &store{
				mu:      new(sync.Mutex),
				results: tt.prepareResult,
			}
			s.AddBindResult(tt.args, tt.bindingResult, tt.hostname)

			assert.Equal(t, tt.wantResult, s.results)
		})
	}
}

func TestStore_DeleteData(t *testing.T) {
	t.Parallel()
	podName := "pod1"
	namespace := "default"
	tests := []struct {
		name       string
		target     corev1.Pod
		result     map[key]*result
		wantResult map[key]*result
	}{
		{
			name: "success to delete the stored data which has the specified key.",
			target: corev1.Pod{
				ObjectMeta: metav1.ObjectMeta{
					Name:      podName,
					Namespace: namespace,
				},
			},
			result: map[key]*result{
				"default/pod1": {
					filter: map[string]extenderv1.ExtenderFilterResult{
						"node0": {
							Nodes:                      &corev1.NodeList{Items: []corev1.Node{{ObjectMeta: metav1.ObjectMeta{Name: "nodename"}}}},
							NodeNames:                  &[]string{"node1"},
							FailedNodes:                map[string]string{"foo": "bar"},
							FailedAndUnresolvableNodes: map[string]string{"baz": "qux"},
							Error:                      "myerror",
						},
					},
					prioritize: map[string]extenderv1.HostPriorityList{
						"node0": {
							{
								Host:  "node1",
								Score: 1.0,
							},
						},
					},
					preempt: map[string]extenderv1.ExtenderPreemptionResult{
						"node0": {
							NodeNameToMetaVictims: map[string]*extenderv1.MetaVictims{"foo": {Pods: []*extenderv1.MetaPod{{UID: "myuid"}}, NumPDBViolations: 1}},
						},
					},
					bind: map[string]extenderv1.ExtenderBindingResult{
						"node0": {
							Error: "myerror",
						},
					},
				},
				"default/pod2": {
					filter: map[string]extenderv1.ExtenderFilterResult{
						"node0": {
							Nodes:                      &corev1.NodeList{Items: []corev1.Node{{ObjectMeta: metav1.ObjectMeta{Name: "nodename"}}}},
							NodeNames:                  &[]string{"node1"},
							FailedNodes:                map[string]string{"foo": "bar"},
							FailedAndUnresolvableNodes: map[string]string{"baz": "qux"},
							Error:                      "myerror",
						},
					},
					prioritize: map[string]extenderv1.HostPriorityList{
						"node0": {
							{
								Host:  "node1",
								Score: 1.0,
							},
						},
					},
					preempt: map[string]extenderv1.ExtenderPreemptionResult{
						"node0": {
							NodeNameToMetaVictims: map[string]*extenderv1.MetaVictims{"foo": {Pods: []*extenderv1.MetaPod{{UID: "myuid"}}, NumPDBViolations: 1}},
						},
					},
					bind: map[string]extenderv1.ExtenderBindingResult{
						"node0": {
							Error: "myerror",
						},
					},
				},
			},
			wantResult: map[key]*result{
				"default/pod2": {
					filter: map[string]extenderv1.ExtenderFilterResult{
						"node0": {
							Nodes:                      &corev1.NodeList{Items: []corev1.Node{{ObjectMeta: metav1.ObjectMeta{Name: "nodename"}}}},
							NodeNames:                  &[]string{"node1"},
							FailedNodes:                map[string]string{"foo": "bar"},
							FailedAndUnresolvableNodes: map[string]string{"baz": "qux"},
							Error:                      "myerror",
						},
					},
					prioritize: map[string]extenderv1.HostPriorityList{
						"node0": {
							{
								Host:  "node1",
								Score: 1.0,
							},
						},
					},
					preempt: map[string]extenderv1.ExtenderPreemptionResult{
						"node0": {
							NodeNameToMetaVictims: map[string]*extenderv1.MetaVictims{"foo": {Pods: []*extenderv1.MetaPod{{UID: "myuid"}}, NumPDBViolations: 1}},
						},
					},
					bind: map[string]extenderv1.ExtenderBindingResult{
						"node0": {
							Error: "myerror",
						},
					},
				},
			},
		},
		{
			name: "do nothing if store doesn't have the data.",
			target: corev1.Pod{
				ObjectMeta: metav1.ObjectMeta{
					Name:      podName,
					Namespace: namespace,
				},
			},
			result: map[key]*result{
				"default/pod2": {
					filter: map[string]extenderv1.ExtenderFilterResult{
						"node0": {
							Nodes:                      &corev1.NodeList{Items: []corev1.Node{{ObjectMeta: metav1.ObjectMeta{Name: "nodename"}}}},
							NodeNames:                  &[]string{"node1"},
							FailedNodes:                map[string]string{"foo": "bar"},
							FailedAndUnresolvableNodes: map[string]string{"baz": "qux"},
							Error:                      "myerror",
						},
					},
					prioritize: map[string]extenderv1.HostPriorityList{
						"node0": {
							{
								Host:  "node1",
								Score: 1.0,
							},
						},
					},
					preempt: map[string]extenderv1.ExtenderPreemptionResult{
						"node0": {
							NodeNameToMetaVictims: map[string]*extenderv1.MetaVictims{"foo": {Pods: []*extenderv1.MetaPod{{UID: "myuid"}}, NumPDBViolations: 1}},
						},
					},
					bind: map[string]extenderv1.ExtenderBindingResult{
						"node0": {
							Error: "myerror",
						},
					},
				},
			},
			wantResult: map[key]*result{
				"default/pod2": {
					filter: map[string]extenderv1.ExtenderFilterResult{
						"node0": {
							Nodes:                      &corev1.NodeList{Items: []corev1.Node{{ObjectMeta: metav1.ObjectMeta{Name: "nodename"}}}},
							NodeNames:                  &[]string{"node1"},
							FailedNodes:                map[string]string{"foo": "bar"},
							FailedAndUnresolvableNodes: map[string]string{"baz": "qux"},
							Error:                      "myerror",
						},
					},
					prioritize: map[string]extenderv1.HostPriorityList{
						"node0": {
							{
								Host:  "node1",
								Score: 1.0,
							},
						},
					},
					preempt: map[string]extenderv1.ExtenderPreemptionResult{
						"node0": {
							NodeNameToMetaVictims: map[string]*extenderv1.MetaVictims{"foo": {Pods: []*extenderv1.MetaPod{{UID: "myuid"}}, NumPDBViolations: 1}},
						},
					},
					bind: map[string]extenderv1.ExtenderBindingResult{
						"node0": {
							Error: "myerror",
						},
					},
				},
			},
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			s := &store{
				mu:      new(sync.Mutex),
				results: tt.result,
			}
			s.DeleteData(tt.target)

			assert.Equal(t, tt.wantResult, s.results)
		})
	}
}
</file>

<file path="kube-scheduler-simulator/simulator/scheduler/extender/resultstore/resultstore.go">
package resultstore

import (
	"encoding/json"
	"sync"

	"golang.org/x/xerrors"
	v1 "k8s.io/api/core/v1"
	"k8s.io/klog/v2"
	extenderv1 "k8s.io/kube-scheduler/extender/v1"

	"sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler/extender/annotation"
)

type Store interface {
	GetStoredResult(pod *v1.Pod) map[string]string
	DeleteData(pod v1.Pod)
	AddFilterResult(args extenderv1.ExtenderArgs, result extenderv1.ExtenderFilterResult, hostName string)
	AddPrioritizeResult(args extenderv1.ExtenderArgs, result extenderv1.HostPriorityList, hostName string)
	AddPreemptResult(args extenderv1.ExtenderPreemptionArgs, result extenderv1.ExtenderPreemptionResult, hostName string)
	AddBindResult(args extenderv1.ExtenderBindingArgs, result extenderv1.ExtenderBindingResult, hostName string)
}

// store has results of all extenders.
// It manages all extenders results.
type store struct {
	mu *sync.Mutex

	results map[key]*result
}

// key is the key of result map on Store.
// key is created from namespace and podName.
type key string

type result struct {
	filter map[string]extenderv1.ExtenderFilterResult

	prioritize map[string]extenderv1.HostPriorityList

	preempt map[string]extenderv1.ExtenderPreemptionResult

	bind map[string]extenderv1.ExtenderBindingResult
}

func New() Store {
	s := &store{
		mu:      new(sync.Mutex),
		results: map[key]*result{},
	}
	return s
}

// newKey creates key with namespace and podName.
func newKey(namespace, podName string) key {
	k := namespace + "/" + podName
	return key(k)
}

func newData() *result {
	return &result{
		filter:     map[string]extenderv1.ExtenderFilterResult{},
		prioritize: map[string]extenderv1.HostPriorityList{},
		preempt:    map[string]extenderv1.ExtenderPreemptionResult{},
		bind:       map[string]extenderv1.ExtenderBindingResult{},
	}
}

// GetStoredResult get all stored result of a given Pod.
func (s *store) GetStoredResult(pod *v1.Pod) map[string]string {
	s.mu.Lock()
	defer s.mu.Unlock()

	k := newKey(pod.Namespace, pod.Name)
	if _, ok := s.results[k]; !ok {
		// Store doesn't have any scheduling result of the Pod.
		return nil
	}

	annotation := map[string]string{}
	if err := s.addFilterResultToMap(annotation, k); err != nil {
		klog.Errorf("failed to add filtering result to the pod: %+v", err)
		return nil
	}

	if err := s.addPrioritizeResultToMap(annotation, k); err != nil {
		klog.Errorf("failed to add prioritize result to the pod: %+v", err)
		return nil
	}

	if err := s.addPreemptResultToMap(annotation, k); err != nil {
		klog.Errorf("failed to add preempt result to the pod: %+v", err)
		return nil
	}

	if err := s.addBindResultToMap(annotation, k); err != nil {
		klog.Errorf("failed to add bind result to the pod: %+v", err)
		return nil
	}

	return annotation
}

func (s *store) addFilterResultToMap(anno map[string]string, k key) error {
	results, err := json.Marshal(s.results[k].filter)
	if err != nil {
		return xerrors.Errorf("encode Filter results to json: %w", err)
	}
	anno[annotation.ExtenderFilterResultAnnotationKey] = string(results)
	return nil
}

func (s *store) addPrioritizeResultToMap(anno map[string]string, k key) error {
	results, err := json.Marshal(s.results[k].prioritize)
	if err != nil {
		return xerrors.Errorf("encode Prioritize results to json: %w", err)
	}
	anno[annotation.ExtenderPrioritizeResultAnnotationKey] = string(results)
	return nil
}

func (s *store) addPreemptResultToMap(anno map[string]string, k key) error {
	results, err := json.Marshal(s.results[k].preempt)
	if err != nil {
		return xerrors.Errorf("encode Preempt results to json: %w", err)
	}
	anno[annotation.ExtenderPreemptResultAnnotationKey] = string(results)
	return nil
}

func (s *store) addBindResultToMap(anno map[string]string, k key) error {
	results, err := json.Marshal(s.results[k].bind)
	if err != nil {
		return xerrors.Errorf("encode Bind results to json: %w", err)
	}
	anno[annotation.ExtenderBindResultAnnotationKey] = string(results)
	return nil
}

// AddFilterResult stores the filtering result.
func (s *store) AddFilterResult(args extenderv1.ExtenderArgs, result extenderv1.ExtenderFilterResult, hostName string) {
	s.mu.Lock()
	defer s.mu.Unlock()

	k := newKey(args.Pod.Namespace, args.Pod.Name)
	if _, ok := s.results[k]; !ok {
		s.results[k] = newData()
	}
	s.results[k].filter[hostName] = result
}

// AddPrioritizeResult stores the prioritizing result.
func (s *store) AddPrioritizeResult(args extenderv1.ExtenderArgs, result extenderv1.HostPriorityList, hostName string) {
	s.mu.Lock()
	defer s.mu.Unlock()

	k := newKey(args.Pod.Namespace, args.Pod.Name)
	if _, ok := s.results[k]; !ok {
		s.results[k] = newData()
	}
	s.results[k].prioritize[hostName] = result
}

// AddPreemptResult stores the preempting result.
func (s *store) AddPreemptResult(args extenderv1.ExtenderPreemptionArgs, result extenderv1.ExtenderPreemptionResult, hostName string) {
	s.mu.Lock()
	defer s.mu.Unlock()

	k := newKey(args.Pod.Namespace, args.Pod.Name)
	if _, ok := s.results[k]; !ok {
		s.results[k] = newData()
	}
	s.results[k].preempt[hostName] = result
}

// AddBindResult stores the binding result.
func (s *store) AddBindResult(args extenderv1.ExtenderBindingArgs, result extenderv1.ExtenderBindingResult, hostName string) {
	s.mu.Lock()
	defer s.mu.Unlock()

	k := newKey(args.PodNamespace, args.PodName)
	if _, ok := s.results[k]; !ok {
		s.results[k] = newData()
	}
	s.results[k].bind[hostName] = result
}

// DeleteData deletes the data corresponding to the specified Pod.
func (s *store) DeleteData(pod v1.Pod) {
	s.mu.Lock()
	defer s.mu.Unlock()
	s.deleteData(newKey(pod.Namespace, pod.Name))
}

// deleteData deletes the result stored with the given key.
func (s *store) deleteData(k key) {
	delete(s.results, k)
}
</file>

<file path="kube-scheduler-simulator/simulator/scheduler/extender/extender_test.go">
package extender

import (
	"io"
	"math"
	"net/http"
	"strings"
	"testing"

	"github.com/stretchr/testify/assert"
	"golang.org/x/xerrors"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/util/sets"
	extenderv1 "k8s.io/kube-scheduler/extender/v1"
)

type HTTPClient interface {
	Do(req *http.Request) (*http.Response, error)
}

type MockHTTPClient struct {
	DoFunc func(req *http.Request) (*http.Response, error)
}

func (m *MockHTTPClient) Do(req *http.Request) (*http.Response, error) {
	if m.DoFunc != nil {
		return m.DoFunc(req)
	}
	// just in case you want default correct return value
	return &http.Response{}, nil
}

func TestHttpExtender_send(t *testing.T) {
	t.Parallel()
	tests := []struct {
		name                    string
		verb                    string
		args                    interface{}
		prepareMockHTTPClientFn func() HTTPClient
		extenderFn              func(m HTTPClient) *extender
		wantErr                 bool
		wantResult              interface{}
	}{
		{
			name: "reflect response in the result",
			verb: "/PreemptVerb",
			args: extenderv1.ExtenderArgs{
				Pod: &corev1.Pod{
					ObjectMeta: metav1.ObjectMeta{
						Name:      "pod1",
						Namespace: "default",
					},
				},
			},
			prepareMockHTTPClientFn: func() HTTPClient {
				return &MockHTTPClient{
					DoFunc: func(_ *http.Request) (*http.Response, error) {
						return &http.Response{
							StatusCode: http.StatusOK,
							Body:       io.NopCloser(strings.NewReader(`{"Error":"myerror"}`)),
						}, nil
					},
				}
			},
			extenderFn: func(m HTTPClient) *extender {
				return &extender{
					extenderURL:      "http://example.com/extender",
					preemptVerb:      "/PreemptVerb",
					filterVerb:       "/FilterVerb",
					prioritizeVerb:   "/PrioritizeVerb",
					bindVerb:         "/BindVerb",
					weight:           1,
					client:           m,
					nodeCacheCapable: false,
					managedResources: sets.New[string](),
				}
			},
			wantErr: false,
			wantResult: extenderv1.ExtenderFilterResult{
				Nodes:                      nil,
				NodeNames:                  nil,
				FailedNodes:                nil,
				FailedAndUnresolvableNodes: nil,
				Error:                      "myerror",
			},
		},
		{
			name: "return an error if the args is nil",
			verb: "/PreemptVerb",
			args: math.Inf(1),
			prepareMockHTTPClientFn: func() HTTPClient {
				return &MockHTTPClient{
					DoFunc: func(_ *http.Request) (*http.Response, error) {
						return nil, nil
					},
				}
			},
			extenderFn: func(m HTTPClient) *extender {
				return &extender{
					extenderURL:      "http://example.com/extender",
					preemptVerb:      "/PreemptVerb",
					filterVerb:       "/FilterVerb",
					prioritizeVerb:   "/PrioritizeVerb",
					bindVerb:         "/BindVerb",
					weight:           1,
					client:           m,
					nodeCacheCapable: false,
					managedResources: sets.New[string](),
				}
			},
			wantErr: true,
		},
		{
			name: "return an error if NewRequest method returns an error",
			verb: "/PreemptVerb",
			args: extenderv1.ExtenderArgs{
				Pod: &corev1.Pod{
					ObjectMeta: metav1.ObjectMeta{
						Name:      "pod1",
						Namespace: "default",
					},
				},
			},
			prepareMockHTTPClientFn: func() HTTPClient {
				return &MockHTTPClient{
					DoFunc: func(_ *http.Request) (*http.Response, error) {
						return &http.Response{
							Body: io.NopCloser(strings.NewReader(`{"Error":"myerror"}`)),
						}, nil
					},
				}
			},
			extenderFn: func(m HTTPClient) *extender {
				return &extender{
					extenderURL:      "Unexpected URLs that cause errors",
					preemptVerb:      "/PreemptVerb",
					filterVerb:       "/FilterVerb",
					prioritizeVerb:   "/PrioritizeVerb",
					bindVerb:         "/BindVerb",
					weight:           1,
					client:           m,
					nodeCacheCapable: false,
					managedResources: sets.New[string](),
				}
			},
			wantErr: true,
		},
		{
			name: "return an error if Do method returns an error",
			verb: "/PreemptVerb",
			args: extenderv1.ExtenderArgs{
				Pod: &corev1.Pod{
					ObjectMeta: metav1.ObjectMeta{
						Name:      "pod1",
						Namespace: "default",
					},
				},
			},
			prepareMockHTTPClientFn: func() HTTPClient {
				return &MockHTTPClient{
					DoFunc: func(_ *http.Request) (*http.Response, error) {
						return nil, xerrors.New("Do returns an error")
					},
				}
			},
			extenderFn: func(m HTTPClient) *extender {
				return &extender{
					extenderURL:      "http://example.com/extender",
					preemptVerb:      "/PreemptVerb",
					filterVerb:       "/FilterVerb",
					prioritizeVerb:   "/PrioritizeVerb",
					bindVerb:         "/BindVerb",
					weight:           1,
					client:           m,
					nodeCacheCapable: false,
					managedResources: sets.New[string](),
				}
			},
			wantErr: true,
		},
		{
			name: "return an error if the status code of response is StatusInternalServerError",
			verb: "/PreemptVerb",
			args: extenderv1.ExtenderArgs{
				Pod: &corev1.Pod{
					ObjectMeta: metav1.ObjectMeta{
						Name:      "pod1",
						Namespace: "default",
					},
				},
			},
			prepareMockHTTPClientFn: func() HTTPClient {
				return &MockHTTPClient{
					DoFunc: func(_ *http.Request) (*http.Response, error) {
						return &http.Response{
							StatusCode: http.StatusInternalServerError,
							Body:       io.NopCloser(strings.NewReader(`{"Error":"myerror"}`)),
						}, nil
					},
				}
			},
			extenderFn: func(m HTTPClient) *extender {
				return &extender{
					extenderURL:      "http://example.com/extender",
					preemptVerb:      "/PreemptVerb",
					filterVerb:       "/FilterVerb",
					prioritizeVerb:   "/PrioritizeVerb",
					bindVerb:         "/BindVerb",
					weight:           1,
					client:           m,
					nodeCacheCapable: false,
					managedResources: sets.New[string](),
				}
			},
			wantErr: true,
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			mockClient := tt.prepareMockHTTPClientFn()
			e := tt.extenderFn(mockClient)
			var result extenderv1.ExtenderFilterResult
			err := e.send(tt.verb, tt.args, &result)
			if (err != nil) != tt.wantErr {
				t.Fatalf("send() error = %v, wantErr %v", err, tt.wantErr)
			}

			if !tt.wantErr {
				assert.Equal(t, tt.wantResult, result)
			}
		})
	}
}
</file>

<file path="kube-scheduler-simulator/simulator/scheduler/extender/extender.go">
package extender

//go:generate mockgen -destination=./mock_$GOPACKAGE/extender.go . Extender

import (
	"bytes"
	"encoding/json"
	"net/http"
	"strings"
	"time"

	"golang.org/x/xerrors"
	utilnet "k8s.io/apimachinery/pkg/util/net"
	"k8s.io/apimachinery/pkg/util/sets"
	restclient "k8s.io/client-go/rest"
	configv1 "k8s.io/kube-scheduler/config/v1"
	extenderv1 "k8s.io/kube-scheduler/extender/v1"
	"k8s.io/kubernetes/pkg/scheduler/framework"
)

const (
	// DefaultExtenderTimeout defines the default extender timeout in second.
	DefaultExtenderTimeout = 5 * time.Second
)

// Extender provides methods to call the actual extender's endpoint set by user.
type Extender interface {
	Name() string
	Filter(args extenderv1.ExtenderArgs) (*extenderv1.ExtenderFilterResult, error)
	Prioritize(args extenderv1.ExtenderArgs) (*extenderv1.HostPriorityList, error)
	Preempt(args extenderv1.ExtenderPreemptionArgs) (*extenderv1.ExtenderPreemptionResult, error)
	Bind(args extenderv1.ExtenderBindingArgs) (*extenderv1.ExtenderBindingResult, error)
}

type httpClient interface {
	Do(req *http.Request) (*http.Response, error)
}

// extender represents an extender server.
type extender struct {
	extenderURL      string
	preemptVerb      string
	filterVerb       string
	prioritizeVerb   string
	bindVerb         string
	weight           int64
	client           httpClient
	nodeCacheCapable bool

	// https://github.com/kubernetes/kubernetes/blob/fc04e732bb3e7198d2fa44efa5457c7c6f8c0f5b/pkg/scheduler/extender.go#L51
	managedResources sets.Set[string]
}

// makeTransport makes http.Transport from the extender config.
func makeTransport(config *configv1.Extender) (http.RoundTripper, error) {
	var cfg restclient.Config
	if config.TLSConfig != nil {
		cfg.TLSClientConfig.Insecure = config.TLSConfig.Insecure
		cfg.TLSClientConfig.ServerName = config.TLSConfig.ServerName
		cfg.TLSClientConfig.CertFile = config.TLSConfig.CertFile
		cfg.TLSClientConfig.KeyFile = config.TLSConfig.KeyFile
		cfg.TLSClientConfig.CAFile = config.TLSConfig.CAFile
		cfg.TLSClientConfig.CertData = config.TLSConfig.CertData
		cfg.TLSClientConfig.KeyData = config.TLSConfig.KeyData
		cfg.TLSClientConfig.CAData = config.TLSConfig.CAData
	}
	if config.EnableHTTPS {
		hasCA := len(cfg.CAFile) > 0 || len(cfg.CAData) > 0
		if !hasCA {
			cfg.Insecure = true
		}
	}
	tlsConfig, err := restclient.TLSConfigFor(&cfg)
	if err != nil {
		return nil, err
	}
	if tlsConfig != nil {
		return utilnet.SetTransportDefaults(&http.Transport{
			TLSClientConfig: tlsConfig,
		}), nil
	}
	return utilnet.SetTransportDefaults(&http.Transport{}), nil
}

// newExtender creates an Extender object.
func newExtender(config *configv1.Extender) (Extender, error) {
	if config.HTTPTimeout.Duration.Nanoseconds() == 0 {
		config.HTTPTimeout.Duration = DefaultExtenderTimeout
	}

	transport, err := makeTransport(config)
	if err != nil {
		return nil, err
	}
	client := &http.Client{
		Transport: transport,
		Timeout:   config.HTTPTimeout.Duration,
	}
	managedResources := sets.New[string]()
	for _, r := range config.ManagedResources {
		managedResources.Insert(r.Name)
	}
	return &extender{
		extenderURL:      config.URLPrefix,
		preemptVerb:      config.PreemptVerb,
		filterVerb:       config.FilterVerb,
		prioritizeVerb:   config.PrioritizeVerb,
		bindVerb:         config.BindVerb,
		weight:           config.Weight,
		client:           client,
		nodeCacheCapable: config.NodeCacheCapable,
		managedResources: managedResources,
	}, nil
}

// Name returns the extender URL as the server name.
func (e *extender) Name() string {
	return e.extenderURL
}

// Filter sends the request to the original extender server, and returns the response as is.
func (e *extender) Filter(args extenderv1.ExtenderArgs) (*extenderv1.ExtenderFilterResult, error) {
	var result extenderv1.ExtenderFilterResult
	if e.filterVerb == "" {
		return nil, xerrors.Errorf("filterVerb is empty")
	}
	if err := e.send(e.filterVerb, args, &result); err != nil {
		return nil, xerrors.Errorf("send filter request: %w", err)
	}
	return &result, nil
}

// Prioritize sends the request to the original extender server, and returns the response as is.
func (e *extender) Prioritize(args extenderv1.ExtenderArgs) (*extenderv1.HostPriorityList, error) {
	var result extenderv1.HostPriorityList
	if e.prioritizeVerb == "" {
		return nil, xerrors.Errorf("prioritizeVerb is empty")
	}
	if err := e.send(e.prioritizeVerb, args, &result); err != nil {
		return nil, xerrors.Errorf("send prioritize request: %w", err)
	}
	for i := range result {
		// MaxExtenderPriority may diverge from the max priority used in the scheduler and defined by MaxNodeScore,
		// therefore we need to scale the score returned by extenders to the score range used by the scheduler.
		result[i].Score = result[i].Score * e.weight * (framework.MaxNodeScore / extenderv1.MaxExtenderPriority)
	}
	return &result, nil
}

// Preempt sends the request to the original extender server, and returns the response as is.
func (e *extender) Preempt(args extenderv1.ExtenderPreemptionArgs) (*extenderv1.ExtenderPreemptionResult, error) {
	var result extenderv1.ExtenderPreemptionResult
	if e.preemptVerb == "" {
		return nil, xerrors.Errorf("preemptVerb is empty")
	}
	if err := e.send(e.preemptVerb, args, &result); err != nil {
		return nil, xerrors.Errorf("send preempt request: %w", err)
	}
	return &result, nil
}

// Bind sends the request to the original extender server, and returns the response as is.
func (e *extender) Bind(args extenderv1.ExtenderBindingArgs) (*extenderv1.ExtenderBindingResult, error) {
	var result extenderv1.ExtenderBindingResult
	if e.bindVerb == "" {
		return nil, xerrors.Errorf("bindVerb is empty")
	}
	if err := e.send(e.bindVerb, args, &result); err != nil {
		return nil, xerrors.Errorf("send bind request: %w", err)
	}
	return &result, nil
}

// Send is Helper function to send messages to the extender.
func (e *extender) send(action string, args interface{}, result interface{}) error {
	out, err := json.Marshal(args)
	if err != nil {
		return xerrors.Errorf("json Marshal: %w", err)
	}
	url := strings.TrimRight(e.extenderURL, "/") + "/" + action

	req, err := http.NewRequest("POST", url, bytes.NewReader(out))
	if err != nil {
		return xerrors.Errorf("http NewRequest: %w", err)
	}

	req.Header.Set("Content-Type", "application/json")

	resp, err := e.client.Do(req)
	if err != nil {
		return xerrors.Errorf("client Do: %w", err)
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		return xerrors.Errorf("failed %v with extender at URL %v, code %v", action, url, resp.StatusCode)
	}
	return json.NewDecoder(resp.Body).Decode(result)
}

// createExtenders creates Extender that represents actual extender's endpoint based on the config set by user.
func createExtenders(configs []configv1.Extender) ([]Extender, error) {
	if len(configs) == 0 {
		return nil, nil
	}
	extenders := make([]Extender, len(configs))
	for i := range configs {
		e, err := newExtender(&configs[i])
		if err != nil {
			return nil, xerrors.Errorf("failed newExtender: %w", err)
		}
		extenders[i] = e
	}
	return extenders, nil
}
</file>

<file path="kube-scheduler-simulator/simulator/scheduler/extender/service_test.go">
package extender

import (
	"strconv"
	"testing"

	"github.com/stretchr/testify/assert"
	"go.uber.org/mock/gomock"
	"golang.org/x/xerrors"
	"k8s.io/client-go/kubernetes/fake"
	configv1 "k8s.io/kube-scheduler/config/v1"
	extenderv1 "k8s.io/kube-scheduler/extender/v1"

	"sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler/extender/mock_extender"
)

func TestService_Filter(t *testing.T) {
	t.Parallel()
	tests := []struct {
		name                     string
		prepareFakeClientSetFn   func() *fake.Clientset
		prepareMockExtenderSetFn func(m *mock_extender.MockExtender)
		prepareMockStoreSetFn    func(m *mock_extender.MockStore)
		wantErr                  bool
	}{
		{
			name: "success",
			prepareFakeClientSetFn: func() *fake.Clientset {
				return fake.NewSimpleClientset()
			},
			prepareMockExtenderSetFn: func(m *mock_extender.MockExtender) {
				m.EXPECT().Filter(extenderv1.ExtenderArgs{}).Return(&extenderv1.ExtenderFilterResult{}, nil)
				m.EXPECT().Name().Return("ext1")
			},
			prepareMockStoreSetFn: func(m *mock_extender.MockStore) {
				m.EXPECT().AddFilterResult(extenderv1.ExtenderArgs{}, gomock.Any(), "ext1")
			},
			wantErr: false,
		},
		{
			name: "return an error if the extender return an error",
			prepareFakeClientSetFn: func() *fake.Clientset {
				return fake.NewSimpleClientset()
			},
			prepareMockExtenderSetFn: func(m *mock_extender.MockExtender) {
				m.EXPECT().Filter(extenderv1.ExtenderArgs{}).Return(nil, xerrors.New("failed"))
			},
			prepareMockStoreSetFn: func(_ *mock_extender.MockStore) {
			},
			wantErr: true,
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			c := tt.prepareFakeClientSetFn()
			ctrl := gomock.NewController(t)
			mStore := mock_extender.NewMockStore(ctrl)
			mExtender := mock_extender.NewMockExtender(ctrl)
			tt.prepareMockStoreSetFn(mStore)
			tt.prepareMockExtenderSetFn(mExtender)

			s := &Service{
				client:    c,
				extenders: []Extender{mExtender},
				store:     mStore,
			}
			args := extenderv1.ExtenderArgs{}
			_, err := s.Filter(0, args)

			if (err != nil) != tt.wantErr {
				t.Fatalf("New() error = %v, wantErr %v", err, tt.wantErr)
			}
		})
	}
}

func TestService_Prioritize(t *testing.T) {
	t.Parallel()
	tests := []struct {
		name                     string
		prepareFakeClientSetFn   func() *fake.Clientset
		prepareMockExtenderSetFn func(m *mock_extender.MockExtender)
		prepareMockStoreSetFn    func(m *mock_extender.MockStore)
		wantErr                  bool
	}{
		{
			name: "success",
			prepareFakeClientSetFn: func() *fake.Clientset {
				return fake.NewSimpleClientset()
			},
			prepareMockExtenderSetFn: func(m *mock_extender.MockExtender) {
				m.EXPECT().Prioritize(extenderv1.ExtenderArgs{}).Return(&extenderv1.HostPriorityList{}, nil)
				m.EXPECT().Name().Return("ext1")
			},
			prepareMockStoreSetFn: func(m *mock_extender.MockStore) {
				m.EXPECT().AddPrioritizeResult(extenderv1.ExtenderArgs{}, gomock.Any(), "ext1")
			},
			wantErr: false,
		},
		{
			name: "return an error if the extender return an error",
			prepareFakeClientSetFn: func() *fake.Clientset {
				return fake.NewSimpleClientset()
			},
			prepareMockExtenderSetFn: func(m *mock_extender.MockExtender) {
				m.EXPECT().Prioritize(extenderv1.ExtenderArgs{}).Return(nil, xerrors.New("failed"))
			},
			prepareMockStoreSetFn: func(_ *mock_extender.MockStore) {
			},
			wantErr: true,
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			c := tt.prepareFakeClientSetFn()
			ctrl := gomock.NewController(t)
			mStore := mock_extender.NewMockStore(ctrl)
			mExtender := mock_extender.NewMockExtender(ctrl)
			tt.prepareMockStoreSetFn(mStore)
			tt.prepareMockExtenderSetFn(mExtender)

			s := &Service{
				client:    c,
				extenders: []Extender{mExtender},
				store:     mStore,
			}
			args := extenderv1.ExtenderArgs{}
			_, err := s.Prioritize(0, args)

			if (err != nil) != tt.wantErr {
				t.Fatalf("New() error = %v, wantErr %v", err, tt.wantErr)
			}
		})
	}
}

func TestService_Preempt(t *testing.T) {
	t.Parallel()
	tests := []struct {
		name                     string
		prepareFakeClientSetFn   func() *fake.Clientset
		prepareMockExtenderSetFn func(m *mock_extender.MockExtender)
		prepareMockStoreSetFn    func(m *mock_extender.MockStore)
		wantErr                  bool
	}{
		{
			name: "success",
			prepareFakeClientSetFn: func() *fake.Clientset {
				return fake.NewSimpleClientset()
			},
			prepareMockExtenderSetFn: func(m *mock_extender.MockExtender) {
				m.EXPECT().Preempt(extenderv1.ExtenderPreemptionArgs{}).Return(&extenderv1.ExtenderPreemptionResult{}, nil)
				m.EXPECT().Name().Return("ext1")
			},
			prepareMockStoreSetFn: func(m *mock_extender.MockStore) {
				m.EXPECT().AddPreemptResult(extenderv1.ExtenderPreemptionArgs{}, gomock.Any(), "ext1")
			},
			wantErr: false,
		},
		{
			name: "return an error if the extender return an error",
			prepareFakeClientSetFn: func() *fake.Clientset {
				return fake.NewSimpleClientset()
			},
			prepareMockExtenderSetFn: func(m *mock_extender.MockExtender) {
				m.EXPECT().Preempt(extenderv1.ExtenderPreemptionArgs{}).Return(nil, xerrors.New("failed"))
			},
			prepareMockStoreSetFn: func(_ *mock_extender.MockStore) {
			},
			wantErr: true,
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			c := tt.prepareFakeClientSetFn()
			ctrl := gomock.NewController(t)
			mStore := mock_extender.NewMockStore(ctrl)
			mExtender := mock_extender.NewMockExtender(ctrl)
			tt.prepareMockStoreSetFn(mStore)
			tt.prepareMockExtenderSetFn(mExtender)

			s := &Service{
				client:    c,
				extenders: []Extender{mExtender},
				store:     mStore,
			}
			args := extenderv1.ExtenderPreemptionArgs{}
			_, err := s.Preempt(0, args)

			if (err != nil) != tt.wantErr {
				t.Fatalf("New() error = %v, wantErr %v", err, tt.wantErr)
			}
		})
	}
}

func TestService_Bind(t *testing.T) {
	t.Parallel()
	tests := []struct {
		name                     string
		prepareFakeClientSetFn   func() *fake.Clientset
		prepareMockExtenderSetFn func(m *mock_extender.MockExtender)
		prepareMockStoreSetFn    func(m *mock_extender.MockStore)
		wantErr                  bool
	}{
		{
			name: "success",
			prepareFakeClientSetFn: func() *fake.Clientset {
				return fake.NewSimpleClientset()
			},
			prepareMockExtenderSetFn: func(m *mock_extender.MockExtender) {
				m.EXPECT().Bind(extenderv1.ExtenderBindingArgs{}).Return(&extenderv1.ExtenderBindingResult{}, nil)
				m.EXPECT().Name().Return("ext1")
			},
			prepareMockStoreSetFn: func(m *mock_extender.MockStore) {
				m.EXPECT().AddBindResult(extenderv1.ExtenderBindingArgs{}, gomock.Any(), "ext1")
			},
			wantErr: false,
		},
		{
			name: "return an error if the extender return an error",
			prepareFakeClientSetFn: func() *fake.Clientset {
				return fake.NewSimpleClientset()
			},
			prepareMockExtenderSetFn: func(m *mock_extender.MockExtender) {
				m.EXPECT().Bind(extenderv1.ExtenderBindingArgs{}).Return(nil, xerrors.New("failed"))
			},
			prepareMockStoreSetFn: func(_ *mock_extender.MockStore) {
			},
			wantErr: true,
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			c := tt.prepareFakeClientSetFn()
			ctrl := gomock.NewController(t)
			mStore := mock_extender.NewMockStore(ctrl)
			mExtender := mock_extender.NewMockExtender(ctrl)
			tt.prepareMockStoreSetFn(mStore)
			tt.prepareMockExtenderSetFn(mExtender)

			s := &Service{
				client:    c,
				extenders: []Extender{mExtender},
				store:     mStore,
			}
			args := extenderv1.ExtenderBindingArgs{}
			_, err := s.Bind(0, args)

			if (err != nil) != tt.wantErr {
				t.Fatalf("New() error = %v, wantErr %v", err, tt.wantErr)
			}
		})
	}
}

func TestService_OverrideExtendersCfgToSimulator(t *testing.T) {
	t.Parallel()
	target := configv1.KubeSchedulerConfiguration{}
	es := make([]configv1.Extender, 2)
	port := 80
	for i := range es {
		es[i].EnableHTTPS = true
		es[i].TLSConfig = new(configv1.ExtenderTLSConfig)
		es[i].URLPrefix = "http://example.com/"
		es[i].FilterVerb = "f"
		es[i].PrioritizeVerb = "p"
		es[i].PreemptVerb = "pr"
		es[i].BindVerb = "b"
	}
	target.Extenders = es

	OverrideExtendersCfgToSimulator(&target, port)

	// OverrideExtendersCfgToSimulator changes all extender config included in KubeSchedulerConfiguration.
	for i, e := range target.Extenders {
		s := strconv.Itoa(i)
		// Replaced with false.
		assert.Equal(t, false, e.EnableHTTPS)
		// Replaced with nil.
		if e.TLSConfig != nil {
			t.Fatalf("TLSConfig = %v, expected = nil", e.TLSConfig)
		}
		// Rewrite settings for simulator.
		assert.Equal(t, "http://localhost:80/api/v1/extender/", e.URLPrefix)
		assert.Equal(t, "filter/"+s, e.FilterVerb)
		assert.Equal(t, "prioritize/"+s, e.PrioritizeVerb)
		assert.Equal(t, "preempt/"+s, e.PreemptVerb)
		assert.Equal(t, "bind/"+s, e.BindVerb)
	}
}
</file>

<file path="kube-scheduler-simulator/simulator/scheduler/extender/service.go">
package extender

//go:generate mockgen -package=mock_$GOPACKAGE -source=./resultstore/resultstore.go -destination=./mock_$GOPACKAGE/resultstore.go

import (
	"strconv"

	"golang.org/x/xerrors"
	clientset "k8s.io/client-go/kubernetes"
	configv1 "k8s.io/kube-scheduler/config/v1"
	extenderv1 "k8s.io/kube-scheduler/extender/v1"

	"sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler/extender/resultstore"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler/storereflector"
)

// Service manages Extenders and the result.
type Service struct {
	client    clientset.Interface
	extenders []Extender
	store     resultstore.Store
}

const ResultStoreKey = "ExtenderResultStoreKey"

// New initializes Service.
// `extenderCfgs` expect to receive an untouched config file(set by user).
func New(client clientset.Interface, extenderCfgs []configv1.Extender, storeReflector storereflector.Reflector) (*Service, error) {
	extenders, err := createExtenders(extenderCfgs)
	if err != nil {
		return nil, xerrors.Errorf("create HTTPExtenders: %w", err)
	}
	store := resultstore.New()
	// Register the result store of Extenders to the sharedStore.
	storeReflector.AddResultStore(store, ResultStoreKey)
	return &Service{
		client:    client,
		extenders: extenders,
		store:     store,
	}, nil
}

// Filter returns the result of the specified filter extender
// and store it.
func (s *Service) Filter(id int, args extenderv1.ExtenderArgs) (*extenderv1.ExtenderFilterResult, error) {
	result, err := s.extenders[id].Filter(args)
	if err != nil {
		return nil, xerrors.Errorf("call filter of specified HTTPExtender: %w", err)
	}
	s.store.AddFilterResult(args, *result, s.extenders[id].Name())
	return result, nil
}

// Prioritize returns the result of the specified prioritize extender
// and store it.
func (s *Service) Prioritize(id int, args extenderv1.ExtenderArgs) (*extenderv1.HostPriorityList, error) {
	result, err := s.extenders[id].Prioritize(args)
	if err != nil {
		return nil, xerrors.Errorf("call prioritize of specified HTTPExtender: %w", err)
	}
	s.store.AddPrioritizeResult(args, *result, s.extenders[id].Name())
	return result, nil
}

// Preempt returns the result of the specified preempt extender
// and store it.
func (s *Service) Preempt(id int, args extenderv1.ExtenderPreemptionArgs) (*extenderv1.ExtenderPreemptionResult, error) {
	result, err := s.extenders[id].Preempt(args)
	if err != nil {
		return nil, xerrors.Errorf("call preempt of specified HTTPExtender: %w", err)
	}
	s.store.AddPreemptResult(args, *result, s.extenders[id].Name())
	return result, nil
}

// Bind returns the result of the specified bind extender
// and store it.
func (s *Service) Bind(id int, args extenderv1.ExtenderBindingArgs) (*extenderv1.ExtenderBindingResult, error) {
	result, err := s.extenders[id].Bind(args)
	if err != nil {
		return nil, xerrors.Errorf("call bind of specified HTTPExtender: %w", err)
	}
	s.store.AddBindResult(args, *result, s.extenders[id].Name())
	return result, nil
}

// OverrideExtendersCfgToSimulator rewrites the scheduler config so that the extenders requests go through the simulator server.
func OverrideExtendersCfgToSimulator(cfg *configv1.KubeSchedulerConfiguration, simulatorPort int) {
	for i := range cfg.Extenders {
		// i will be the extender's index. That index is specified by request param as `id`.
		cfg.Extenders[i].EnableHTTPS = false
		cfg.Extenders[i].TLSConfig = nil
		// NOTE: We do not plan to launch the "HTTPS" simulator server with echo on our project.
		// If you customize the server to use HTTPS with echo, you need to fix this line.
		cfg.Extenders[i].URLPrefix = "http://localhost:" + strconv.Itoa(simulatorPort) + "/api/v1/extender/"
		if cfg.Extenders[i].FilterVerb != "" {
			cfg.Extenders[i].FilterVerb = "filter/" + strconv.Itoa(i)
		}
		if cfg.Extenders[i].PrioritizeVerb != "" {
			cfg.Extenders[i].PrioritizeVerb = "prioritize/" + strconv.Itoa(i)
		}
		if cfg.Extenders[i].PreemptVerb != "" {
			cfg.Extenders[i].PreemptVerb = "preempt/" + strconv.Itoa(i)
		}
		if cfg.Extenders[i].BindVerb != "" {
			cfg.Extenders[i].BindVerb = "bind/" + strconv.Itoa(i)
		}
	}
}
</file>

<file path="kube-scheduler-simulator/simulator/scheduler/plugin/annotation/annotation.go">
package annotation

const (
	// PreFilterStatusResultAnnotationKey has the prefilter result(framework.Status).
	PreFilterStatusResultAnnotationKey = "kube-scheduler-simulator.sigs.k8s.io/prefilter-result-status"
	// PreFilterResultAnnotationKey has the prefilter result(framework.PreFilterResult).
	PreFilterResultAnnotationKey = "kube-scheduler-simulator.sigs.k8s.io/prefilter-result"
	// FilterResultAnnotationKey has the filtering result.
	FilterResultAnnotationKey = "kube-scheduler-simulator.sigs.k8s.io/filter-result"
	// PostFilterResultAnnotationKey has the post filter result.
	PostFilterResultAnnotationKey = "kube-scheduler-simulator.sigs.k8s.io/postfilter-result"
	// PreScoreResultAnnotationKey has the prescore result.
	PreScoreResultAnnotationKey = "kube-scheduler-simulator.sigs.k8s.io/prescore-result"
	// ScoreResultAnnotationKey has the scoring result.
	ScoreResultAnnotationKey = "kube-scheduler-simulator.sigs.k8s.io/score-result"
	// FinalScoreResultAnnotationKey has the final score(= normalized and applied score plugin weight).
	FinalScoreResultAnnotationKey = "kube-scheduler-simulator.sigs.k8s.io/finalscore-result"
	// ReserveResultAnnotationKey has the reserve result.
	ReserveResultAnnotationKey = "kube-scheduler-simulator.sigs.k8s.io/reserve-result"
	// PermitStatusResultAnnotationKey has the permit result.
	PermitStatusResultAnnotationKey = "kube-scheduler-simulator.sigs.k8s.io/permit-result"
	// PermitTimeoutResultAnnotationKey has the permit result.
	PermitTimeoutResultAnnotationKey = "kube-scheduler-simulator.sigs.k8s.io/permit-result-timeout"
	// PreBindResultAnnotationKey has the prebind result.
	PreBindResultAnnotationKey = "kube-scheduler-simulator.sigs.k8s.io/prebind-result"
	// BindResultAnnotationKey has the prebind result.
	BindResultAnnotationKey = "kube-scheduler-simulator.sigs.k8s.io/bind-result"
	// SelectedNodeAnnotationKey has the selected node name. It's filled when a Pod go through the Reserve phase.
	SelectedNodeAnnotationKey = "kube-scheduler-simulator.sigs.k8s.io/selected-node"
)
</file>

<file path="kube-scheduler-simulator/simulator/scheduler/plugin/mock/framework.go">
// Code generated by MockGen. DO NOT EDIT.
// Source: k8s.io/kubernetes/pkg/scheduler/framework (interfaces: PreFilterPlugin,FilterPlugin,PostFilterPlugin,PreScorePlugin,ScorePlugin,ScoreExtensions,PermitPlugin,BindPlugin,PreBindPlugin,PostBindPlugin,ReservePlugin)
//
// Generated by this command:
//
//	mockgen -destination=./mock/framework.go -package=plugin k8s.io/kubernetes/pkg/scheduler/framework PreFilterPlugin,FilterPlugin,PostFilterPlugin,PreScorePlugin,ScorePlugin,ScoreExtensions,PermitPlugin,BindPlugin,PreBindPlugin,PostBindPlugin,ReservePlugin
//

// Package plugin is a generated GoMock package.
package plugin

import (
	context "context"
	reflect "reflect"
	time "time"

	gomock "go.uber.org/mock/gomock"
	v1 "k8s.io/api/core/v1"
	framework "k8s.io/kubernetes/pkg/scheduler/framework"
)

// MockPreFilterPlugin is a mock of PreFilterPlugin interface.
type MockPreFilterPlugin struct {
	ctrl     *gomock.Controller
	recorder *MockPreFilterPluginMockRecorder
	isgomock struct{}
}

// MockPreFilterPluginMockRecorder is the mock recorder for MockPreFilterPlugin.
type MockPreFilterPluginMockRecorder struct {
	mock *MockPreFilterPlugin
}

// NewMockPreFilterPlugin creates a new mock instance.
func NewMockPreFilterPlugin(ctrl *gomock.Controller) *MockPreFilterPlugin {
	mock := &MockPreFilterPlugin{ctrl: ctrl}
	mock.recorder = &MockPreFilterPluginMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockPreFilterPlugin) EXPECT() *MockPreFilterPluginMockRecorder {
	return m.recorder
}

// Name mocks base method.
func (m *MockPreFilterPlugin) Name() string {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Name")
	ret0, _ := ret[0].(string)
	return ret0
}

// Name indicates an expected call of Name.
func (mr *MockPreFilterPluginMockRecorder) Name() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Name", reflect.TypeOf((*MockPreFilterPlugin)(nil).Name))
}

// PreFilter mocks base method.
func (m *MockPreFilterPlugin) PreFilter(ctx context.Context, state *framework.CycleState, p *v1.Pod) (*framework.PreFilterResult, *framework.Status) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "PreFilter", ctx, state, p)
	ret0, _ := ret[0].(*framework.PreFilterResult)
	ret1, _ := ret[1].(*framework.Status)
	return ret0, ret1
}

// PreFilter indicates an expected call of PreFilter.
func (mr *MockPreFilterPluginMockRecorder) PreFilter(ctx, state, p any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "PreFilter", reflect.TypeOf((*MockPreFilterPlugin)(nil).PreFilter), ctx, state, p)
}

// PreFilterExtensions mocks base method.
func (m *MockPreFilterPlugin) PreFilterExtensions() framework.PreFilterExtensions {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "PreFilterExtensions")
	ret0, _ := ret[0].(framework.PreFilterExtensions)
	return ret0
}

// PreFilterExtensions indicates an expected call of PreFilterExtensions.
func (mr *MockPreFilterPluginMockRecorder) PreFilterExtensions() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "PreFilterExtensions", reflect.TypeOf((*MockPreFilterPlugin)(nil).PreFilterExtensions))
}

// MockFilterPlugin is a mock of FilterPlugin interface.
type MockFilterPlugin struct {
	ctrl     *gomock.Controller
	recorder *MockFilterPluginMockRecorder
	isgomock struct{}
}

// MockFilterPluginMockRecorder is the mock recorder for MockFilterPlugin.
type MockFilterPluginMockRecorder struct {
	mock *MockFilterPlugin
}

// NewMockFilterPlugin creates a new mock instance.
func NewMockFilterPlugin(ctrl *gomock.Controller) *MockFilterPlugin {
	mock := &MockFilterPlugin{ctrl: ctrl}
	mock.recorder = &MockFilterPluginMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockFilterPlugin) EXPECT() *MockFilterPluginMockRecorder {
	return m.recorder
}

// Filter mocks base method.
func (m *MockFilterPlugin) Filter(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo) *framework.Status {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Filter", ctx, state, pod, nodeInfo)
	ret0, _ := ret[0].(*framework.Status)
	return ret0
}

// Filter indicates an expected call of Filter.
func (mr *MockFilterPluginMockRecorder) Filter(ctx, state, pod, nodeInfo any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Filter", reflect.TypeOf((*MockFilterPlugin)(nil).Filter), ctx, state, pod, nodeInfo)
}

// Name mocks base method.
func (m *MockFilterPlugin) Name() string {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Name")
	ret0, _ := ret[0].(string)
	return ret0
}

// Name indicates an expected call of Name.
func (mr *MockFilterPluginMockRecorder) Name() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Name", reflect.TypeOf((*MockFilterPlugin)(nil).Name))
}

// MockPostFilterPlugin is a mock of PostFilterPlugin interface.
type MockPostFilterPlugin struct {
	ctrl     *gomock.Controller
	recorder *MockPostFilterPluginMockRecorder
	isgomock struct{}
}

// MockPostFilterPluginMockRecorder is the mock recorder for MockPostFilterPlugin.
type MockPostFilterPluginMockRecorder struct {
	mock *MockPostFilterPlugin
}

// NewMockPostFilterPlugin creates a new mock instance.
func NewMockPostFilterPlugin(ctrl *gomock.Controller) *MockPostFilterPlugin {
	mock := &MockPostFilterPlugin{ctrl: ctrl}
	mock.recorder = &MockPostFilterPluginMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockPostFilterPlugin) EXPECT() *MockPostFilterPluginMockRecorder {
	return m.recorder
}

// Name mocks base method.
func (m *MockPostFilterPlugin) Name() string {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Name")
	ret0, _ := ret[0].(string)
	return ret0
}

// Name indicates an expected call of Name.
func (mr *MockPostFilterPluginMockRecorder) Name() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Name", reflect.TypeOf((*MockPostFilterPlugin)(nil).Name))
}

// PostFilter mocks base method.
func (m *MockPostFilterPlugin) PostFilter(ctx context.Context, state *framework.CycleState, pod *v1.Pod, filteredNodeStatusMap framework.NodeToStatusMap) (*framework.PostFilterResult, *framework.Status) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "PostFilter", ctx, state, pod, filteredNodeStatusMap)
	ret0, _ := ret[0].(*framework.PostFilterResult)
	ret1, _ := ret[1].(*framework.Status)
	return ret0, ret1
}

// PostFilter indicates an expected call of PostFilter.
func (mr *MockPostFilterPluginMockRecorder) PostFilter(ctx, state, pod, filteredNodeStatusMap any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "PostFilter", reflect.TypeOf((*MockPostFilterPlugin)(nil).PostFilter), ctx, state, pod, filteredNodeStatusMap)
}

// MockPreScorePlugin is a mock of PreScorePlugin interface.
type MockPreScorePlugin struct {
	ctrl     *gomock.Controller
	recorder *MockPreScorePluginMockRecorder
	isgomock struct{}
}

// MockPreScorePluginMockRecorder is the mock recorder for MockPreScorePlugin.
type MockPreScorePluginMockRecorder struct {
	mock *MockPreScorePlugin
}

// NewMockPreScorePlugin creates a new mock instance.
func NewMockPreScorePlugin(ctrl *gomock.Controller) *MockPreScorePlugin {
	mock := &MockPreScorePlugin{ctrl: ctrl}
	mock.recorder = &MockPreScorePluginMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockPreScorePlugin) EXPECT() *MockPreScorePluginMockRecorder {
	return m.recorder
}

// Name mocks base method.
func (m *MockPreScorePlugin) Name() string {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Name")
	ret0, _ := ret[0].(string)
	return ret0
}

// Name indicates an expected call of Name.
func (mr *MockPreScorePluginMockRecorder) Name() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Name", reflect.TypeOf((*MockPreScorePlugin)(nil).Name))
}

// PreScore mocks base method.
func (m *MockPreScorePlugin) PreScore(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodes []*framework.NodeInfo) *framework.Status {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "PreScore", ctx, state, pod, nodes)
	ret0, _ := ret[0].(*framework.Status)
	return ret0
}

// PreScore indicates an expected call of PreScore.
func (mr *MockPreScorePluginMockRecorder) PreScore(ctx, state, pod, nodes any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "PreScore", reflect.TypeOf((*MockPreScorePlugin)(nil).PreScore), ctx, state, pod, nodes)
}

// MockScorePlugin is a mock of ScorePlugin interface.
type MockScorePlugin struct {
	ctrl     *gomock.Controller
	recorder *MockScorePluginMockRecorder
	isgomock struct{}
}

// MockScorePluginMockRecorder is the mock recorder for MockScorePlugin.
type MockScorePluginMockRecorder struct {
	mock *MockScorePlugin
}

// NewMockScorePlugin creates a new mock instance.
func NewMockScorePlugin(ctrl *gomock.Controller) *MockScorePlugin {
	mock := &MockScorePlugin{ctrl: ctrl}
	mock.recorder = &MockScorePluginMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockScorePlugin) EXPECT() *MockScorePluginMockRecorder {
	return m.recorder
}

// Name mocks base method.
func (m *MockScorePlugin) Name() string {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Name")
	ret0, _ := ret[0].(string)
	return ret0
}

// Name indicates an expected call of Name.
func (mr *MockScorePluginMockRecorder) Name() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Name", reflect.TypeOf((*MockScorePlugin)(nil).Name))
}

// Score mocks base method.
func (m *MockScorePlugin) Score(ctx context.Context, state *framework.CycleState, p *v1.Pod, nodeName string) (int64, *framework.Status) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Score", ctx, state, p, nodeName)
	ret0, _ := ret[0].(int64)
	ret1, _ := ret[1].(*framework.Status)
	return ret0, ret1
}

// Score indicates an expected call of Score.
func (mr *MockScorePluginMockRecorder) Score(ctx, state, p, nodeName any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Score", reflect.TypeOf((*MockScorePlugin)(nil).Score), ctx, state, p, nodeName)
}

// ScoreExtensions mocks base method.
func (m *MockScorePlugin) ScoreExtensions() framework.ScoreExtensions {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "ScoreExtensions")
	ret0, _ := ret[0].(framework.ScoreExtensions)
	return ret0
}

// ScoreExtensions indicates an expected call of ScoreExtensions.
func (mr *MockScorePluginMockRecorder) ScoreExtensions() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "ScoreExtensions", reflect.TypeOf((*MockScorePlugin)(nil).ScoreExtensions))
}

// MockScoreExtensions is a mock of ScoreExtensions interface.
type MockScoreExtensions struct {
	ctrl     *gomock.Controller
	recorder *MockScoreExtensionsMockRecorder
	isgomock struct{}
}

// MockScoreExtensionsMockRecorder is the mock recorder for MockScoreExtensions.
type MockScoreExtensionsMockRecorder struct {
	mock *MockScoreExtensions
}

// NewMockScoreExtensions creates a new mock instance.
func NewMockScoreExtensions(ctrl *gomock.Controller) *MockScoreExtensions {
	mock := &MockScoreExtensions{ctrl: ctrl}
	mock.recorder = &MockScoreExtensionsMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockScoreExtensions) EXPECT() *MockScoreExtensionsMockRecorder {
	return m.recorder
}

// NormalizeScore mocks base method.
func (m *MockScoreExtensions) NormalizeScore(ctx context.Context, state *framework.CycleState, p *v1.Pod, scores framework.NodeScoreList) *framework.Status {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "NormalizeScore", ctx, state, p, scores)
	ret0, _ := ret[0].(*framework.Status)
	return ret0
}

// NormalizeScore indicates an expected call of NormalizeScore.
func (mr *MockScoreExtensionsMockRecorder) NormalizeScore(ctx, state, p, scores any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "NormalizeScore", reflect.TypeOf((*MockScoreExtensions)(nil).NormalizeScore), ctx, state, p, scores)
}

// MockPermitPlugin is a mock of PermitPlugin interface.
type MockPermitPlugin struct {
	ctrl     *gomock.Controller
	recorder *MockPermitPluginMockRecorder
	isgomock struct{}
}

// MockPermitPluginMockRecorder is the mock recorder for MockPermitPlugin.
type MockPermitPluginMockRecorder struct {
	mock *MockPermitPlugin
}

// NewMockPermitPlugin creates a new mock instance.
func NewMockPermitPlugin(ctrl *gomock.Controller) *MockPermitPlugin {
	mock := &MockPermitPlugin{ctrl: ctrl}
	mock.recorder = &MockPermitPluginMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockPermitPlugin) EXPECT() *MockPermitPluginMockRecorder {
	return m.recorder
}

// Name mocks base method.
func (m *MockPermitPlugin) Name() string {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Name")
	ret0, _ := ret[0].(string)
	return ret0
}

// Name indicates an expected call of Name.
func (mr *MockPermitPluginMockRecorder) Name() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Name", reflect.TypeOf((*MockPermitPlugin)(nil).Name))
}

// Permit mocks base method.
func (m *MockPermitPlugin) Permit(ctx context.Context, state *framework.CycleState, p *v1.Pod, nodeName string) (*framework.Status, time.Duration) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Permit", ctx, state, p, nodeName)
	ret0, _ := ret[0].(*framework.Status)
	ret1, _ := ret[1].(time.Duration)
	return ret0, ret1
}

// Permit indicates an expected call of Permit.
func (mr *MockPermitPluginMockRecorder) Permit(ctx, state, p, nodeName any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Permit", reflect.TypeOf((*MockPermitPlugin)(nil).Permit), ctx, state, p, nodeName)
}

// MockBindPlugin is a mock of BindPlugin interface.
type MockBindPlugin struct {
	ctrl     *gomock.Controller
	recorder *MockBindPluginMockRecorder
	isgomock struct{}
}

// MockBindPluginMockRecorder is the mock recorder for MockBindPlugin.
type MockBindPluginMockRecorder struct {
	mock *MockBindPlugin
}

// NewMockBindPlugin creates a new mock instance.
func NewMockBindPlugin(ctrl *gomock.Controller) *MockBindPlugin {
	mock := &MockBindPlugin{ctrl: ctrl}
	mock.recorder = &MockBindPluginMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockBindPlugin) EXPECT() *MockBindPluginMockRecorder {
	return m.recorder
}

// Bind mocks base method.
func (m *MockBindPlugin) Bind(ctx context.Context, state *framework.CycleState, p *v1.Pod, nodeName string) *framework.Status {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Bind", ctx, state, p, nodeName)
	ret0, _ := ret[0].(*framework.Status)
	return ret0
}

// Bind indicates an expected call of Bind.
func (mr *MockBindPluginMockRecorder) Bind(ctx, state, p, nodeName any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Bind", reflect.TypeOf((*MockBindPlugin)(nil).Bind), ctx, state, p, nodeName)
}

// Name mocks base method.
func (m *MockBindPlugin) Name() string {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Name")
	ret0, _ := ret[0].(string)
	return ret0
}

// Name indicates an expected call of Name.
func (mr *MockBindPluginMockRecorder) Name() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Name", reflect.TypeOf((*MockBindPlugin)(nil).Name))
}

// MockPreBindPlugin is a mock of PreBindPlugin interface.
type MockPreBindPlugin struct {
	ctrl     *gomock.Controller
	recorder *MockPreBindPluginMockRecorder
	isgomock struct{}
}

// MockPreBindPluginMockRecorder is the mock recorder for MockPreBindPlugin.
type MockPreBindPluginMockRecorder struct {
	mock *MockPreBindPlugin
}

// NewMockPreBindPlugin creates a new mock instance.
func NewMockPreBindPlugin(ctrl *gomock.Controller) *MockPreBindPlugin {
	mock := &MockPreBindPlugin{ctrl: ctrl}
	mock.recorder = &MockPreBindPluginMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockPreBindPlugin) EXPECT() *MockPreBindPluginMockRecorder {
	return m.recorder
}

// Name mocks base method.
func (m *MockPreBindPlugin) Name() string {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Name")
	ret0, _ := ret[0].(string)
	return ret0
}

// Name indicates an expected call of Name.
func (mr *MockPreBindPluginMockRecorder) Name() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Name", reflect.TypeOf((*MockPreBindPlugin)(nil).Name))
}

// PreBind mocks base method.
func (m *MockPreBindPlugin) PreBind(ctx context.Context, state *framework.CycleState, p *v1.Pod, nodeName string) *framework.Status {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "PreBind", ctx, state, p, nodeName)
	ret0, _ := ret[0].(*framework.Status)
	return ret0
}

// PreBind indicates an expected call of PreBind.
func (mr *MockPreBindPluginMockRecorder) PreBind(ctx, state, p, nodeName any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "PreBind", reflect.TypeOf((*MockPreBindPlugin)(nil).PreBind), ctx, state, p, nodeName)
}

// MockPostBindPlugin is a mock of PostBindPlugin interface.
type MockPostBindPlugin struct {
	ctrl     *gomock.Controller
	recorder *MockPostBindPluginMockRecorder
	isgomock struct{}
}

// MockPostBindPluginMockRecorder is the mock recorder for MockPostBindPlugin.
type MockPostBindPluginMockRecorder struct {
	mock *MockPostBindPlugin
}

// NewMockPostBindPlugin creates a new mock instance.
func NewMockPostBindPlugin(ctrl *gomock.Controller) *MockPostBindPlugin {
	mock := &MockPostBindPlugin{ctrl: ctrl}
	mock.recorder = &MockPostBindPluginMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockPostBindPlugin) EXPECT() *MockPostBindPluginMockRecorder {
	return m.recorder
}

// Name mocks base method.
func (m *MockPostBindPlugin) Name() string {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Name")
	ret0, _ := ret[0].(string)
	return ret0
}

// Name indicates an expected call of Name.
func (mr *MockPostBindPluginMockRecorder) Name() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Name", reflect.TypeOf((*MockPostBindPlugin)(nil).Name))
}

// PostBind mocks base method.
func (m *MockPostBindPlugin) PostBind(ctx context.Context, state *framework.CycleState, p *v1.Pod, nodeName string) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "PostBind", ctx, state, p, nodeName)
}

// PostBind indicates an expected call of PostBind.
func (mr *MockPostBindPluginMockRecorder) PostBind(ctx, state, p, nodeName any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "PostBind", reflect.TypeOf((*MockPostBindPlugin)(nil).PostBind), ctx, state, p, nodeName)
}

// MockReservePlugin is a mock of ReservePlugin interface.
type MockReservePlugin struct {
	ctrl     *gomock.Controller
	recorder *MockReservePluginMockRecorder
	isgomock struct{}
}

// MockReservePluginMockRecorder is the mock recorder for MockReservePlugin.
type MockReservePluginMockRecorder struct {
	mock *MockReservePlugin
}

// NewMockReservePlugin creates a new mock instance.
func NewMockReservePlugin(ctrl *gomock.Controller) *MockReservePlugin {
	mock := &MockReservePlugin{ctrl: ctrl}
	mock.recorder = &MockReservePluginMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockReservePlugin) EXPECT() *MockReservePluginMockRecorder {
	return m.recorder
}

// Name mocks base method.
func (m *MockReservePlugin) Name() string {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Name")
	ret0, _ := ret[0].(string)
	return ret0
}

// Name indicates an expected call of Name.
func (mr *MockReservePluginMockRecorder) Name() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Name", reflect.TypeOf((*MockReservePlugin)(nil).Name))
}

// Reserve mocks base method.
func (m *MockReservePlugin) Reserve(ctx context.Context, state *framework.CycleState, p *v1.Pod, nodeName string) *framework.Status {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "Reserve", ctx, state, p, nodeName)
	ret0, _ := ret[0].(*framework.Status)
	return ret0
}

// Reserve indicates an expected call of Reserve.
func (mr *MockReservePluginMockRecorder) Reserve(ctx, state, p, nodeName any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Reserve", reflect.TypeOf((*MockReservePlugin)(nil).Reserve), ctx, state, p, nodeName)
}

// Unreserve mocks base method.
func (m *MockReservePlugin) Unreserve(ctx context.Context, state *framework.CycleState, p *v1.Pod, nodeName string) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "Unreserve", ctx, state, p, nodeName)
}

// Unreserve indicates an expected call of Unreserve.
func (mr *MockReservePluginMockRecorder) Unreserve(ctx, state, p, nodeName any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "Unreserve", reflect.TypeOf((*MockReservePlugin)(nil).Unreserve), ctx, state, p, nodeName)
}
</file>

<file path="kube-scheduler-simulator/simulator/scheduler/plugin/mock/wrappedplugin.go">
// Code generated by MockGen. DO NOT EDIT.
// Source: sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler/plugin (interfaces: Store,PreFilterPluginExtender,FilterPluginExtender,PostFilterPluginExtender,PreScorePluginExtender,ScorePluginExtender,NormalizeScorePluginExtender,ReservePluginExtender,PermitPluginExtender,PreBindPluginExtender,BindPluginExtender,PostBindPluginExtender)
//
// Generated by this command:
//
//	mockgen -destination=./mock/wrappedplugin.go -package=plugin . Store,PreFilterPluginExtender,FilterPluginExtender,PostFilterPluginExtender,PreScorePluginExtender,ScorePluginExtender,NormalizeScorePluginExtender,ReservePluginExtender,PermitPluginExtender,PreBindPluginExtender,BindPluginExtender,PostBindPluginExtender
//

// Package plugin is a generated GoMock package.
package plugin

import (
	context "context"
	reflect "reflect"
	time "time"

	gomock "go.uber.org/mock/gomock"
	v1 "k8s.io/api/core/v1"
	framework "k8s.io/kubernetes/pkg/scheduler/framework"
)

// MockStore is a mock of Store interface.
type MockStore struct {
	ctrl     *gomock.Controller
	recorder *MockStoreMockRecorder
	isgomock struct{}
}

// MockStoreMockRecorder is the mock recorder for MockStore.
type MockStoreMockRecorder struct {
	mock *MockStore
}

// NewMockStore creates a new mock instance.
func NewMockStore(ctrl *gomock.Controller) *MockStore {
	mock := &MockStore{ctrl: ctrl}
	mock.recorder = &MockStoreMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockStore) EXPECT() *MockStoreMockRecorder {
	return m.recorder
}

// AddBindResult mocks base method.
func (m *MockStore) AddBindResult(namespace, podName, pluginName, status string) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "AddBindResult", namespace, podName, pluginName, status)
}

// AddBindResult indicates an expected call of AddBindResult.
func (mr *MockStoreMockRecorder) AddBindResult(namespace, podName, pluginName, status any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "AddBindResult", reflect.TypeOf((*MockStore)(nil).AddBindResult), namespace, podName, pluginName, status)
}

// AddCustomResult mocks base method.
func (m *MockStore) AddCustomResult(namespace, podName, annotationKey, result string) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "AddCustomResult", namespace, podName, annotationKey, result)
}

// AddCustomResult indicates an expected call of AddCustomResult.
func (mr *MockStoreMockRecorder) AddCustomResult(namespace, podName, annotationKey, result any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "AddCustomResult", reflect.TypeOf((*MockStore)(nil).AddCustomResult), namespace, podName, annotationKey, result)
}

// AddFilterResult mocks base method.
func (m *MockStore) AddFilterResult(namespace, podName, nodeName, pluginName, reason string) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "AddFilterResult", namespace, podName, nodeName, pluginName, reason)
}

// AddFilterResult indicates an expected call of AddFilterResult.
func (mr *MockStoreMockRecorder) AddFilterResult(namespace, podName, nodeName, pluginName, reason any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "AddFilterResult", reflect.TypeOf((*MockStore)(nil).AddFilterResult), namespace, podName, nodeName, pluginName, reason)
}

// AddNormalizedScoreResult mocks base method.
func (m *MockStore) AddNormalizedScoreResult(namespace, podName, nodeName, pluginName string, normalizedscore int64) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "AddNormalizedScoreResult", namespace, podName, nodeName, pluginName, normalizedscore)
}

// AddNormalizedScoreResult indicates an expected call of AddNormalizedScoreResult.
func (mr *MockStoreMockRecorder) AddNormalizedScoreResult(namespace, podName, nodeName, pluginName, normalizedscore any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "AddNormalizedScoreResult", reflect.TypeOf((*MockStore)(nil).AddNormalizedScoreResult), namespace, podName, nodeName, pluginName, normalizedscore)
}

// AddPermitResult mocks base method.
func (m *MockStore) AddPermitResult(namespace, podName, pluginName, status string, timeout time.Duration) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "AddPermitResult", namespace, podName, pluginName, status, timeout)
}

// AddPermitResult indicates an expected call of AddPermitResult.
func (mr *MockStoreMockRecorder) AddPermitResult(namespace, podName, pluginName, status, timeout any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "AddPermitResult", reflect.TypeOf((*MockStore)(nil).AddPermitResult), namespace, podName, pluginName, status, timeout)
}

// AddPostFilterResult mocks base method.
func (m *MockStore) AddPostFilterResult(namespace, podName, nominatedNodeName, pluginName string, nodeNames []string) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "AddPostFilterResult", namespace, podName, nominatedNodeName, pluginName, nodeNames)
}

// AddPostFilterResult indicates an expected call of AddPostFilterResult.
func (mr *MockStoreMockRecorder) AddPostFilterResult(namespace, podName, nominatedNodeName, pluginName, nodeNames any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "AddPostFilterResult", reflect.TypeOf((*MockStore)(nil).AddPostFilterResult), namespace, podName, nominatedNodeName, pluginName, nodeNames)
}

// AddPreBindResult mocks base method.
func (m *MockStore) AddPreBindResult(namespace, podName, pluginName, status string) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "AddPreBindResult", namespace, podName, pluginName, status)
}

// AddPreBindResult indicates an expected call of AddPreBindResult.
func (mr *MockStoreMockRecorder) AddPreBindResult(namespace, podName, pluginName, status any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "AddPreBindResult", reflect.TypeOf((*MockStore)(nil).AddPreBindResult), namespace, podName, pluginName, status)
}

// AddPreFilterResult mocks base method.
func (m *MockStore) AddPreFilterResult(namespace, podName, pluginName, reason string, preFilterResult *framework.PreFilterResult) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "AddPreFilterResult", namespace, podName, pluginName, reason, preFilterResult)
}

// AddPreFilterResult indicates an expected call of AddPreFilterResult.
func (mr *MockStoreMockRecorder) AddPreFilterResult(namespace, podName, pluginName, reason, preFilterResult any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "AddPreFilterResult", reflect.TypeOf((*MockStore)(nil).AddPreFilterResult), namespace, podName, pluginName, reason, preFilterResult)
}

// AddPreScoreResult mocks base method.
func (m *MockStore) AddPreScoreResult(namespace, podName, pluginName, reason string) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "AddPreScoreResult", namespace, podName, pluginName, reason)
}

// AddPreScoreResult indicates an expected call of AddPreScoreResult.
func (mr *MockStoreMockRecorder) AddPreScoreResult(namespace, podName, pluginName, reason any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "AddPreScoreResult", reflect.TypeOf((*MockStore)(nil).AddPreScoreResult), namespace, podName, pluginName, reason)
}

// AddReserveResult mocks base method.
func (m *MockStore) AddReserveResult(namespace, podName, pluginName, status string) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "AddReserveResult", namespace, podName, pluginName, status)
}

// AddReserveResult indicates an expected call of AddReserveResult.
func (mr *MockStoreMockRecorder) AddReserveResult(namespace, podName, pluginName, status any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "AddReserveResult", reflect.TypeOf((*MockStore)(nil).AddReserveResult), namespace, podName, pluginName, status)
}

// AddScoreResult mocks base method.
func (m *MockStore) AddScoreResult(namespace, podName, nodeName, pluginName string, score int64) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "AddScoreResult", namespace, podName, nodeName, pluginName, score)
}

// AddScoreResult indicates an expected call of AddScoreResult.
func (mr *MockStoreMockRecorder) AddScoreResult(namespace, podName, nodeName, pluginName, score any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "AddScoreResult", reflect.TypeOf((*MockStore)(nil).AddScoreResult), namespace, podName, nodeName, pluginName, score)
}

// AddSelectedNode mocks base method.
func (m *MockStore) AddSelectedNode(namespace, podName, nodeName string) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "AddSelectedNode", namespace, podName, nodeName)
}

// AddSelectedNode indicates an expected call of AddSelectedNode.
func (mr *MockStoreMockRecorder) AddSelectedNode(namespace, podName, nodeName any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "AddSelectedNode", reflect.TypeOf((*MockStore)(nil).AddSelectedNode), namespace, podName, nodeName)
}

// MockPreFilterPluginExtender is a mock of PreFilterPluginExtender interface.
type MockPreFilterPluginExtender struct {
	ctrl     *gomock.Controller
	recorder *MockPreFilterPluginExtenderMockRecorder
	isgomock struct{}
}

// MockPreFilterPluginExtenderMockRecorder is the mock recorder for MockPreFilterPluginExtender.
type MockPreFilterPluginExtenderMockRecorder struct {
	mock *MockPreFilterPluginExtender
}

// NewMockPreFilterPluginExtender creates a new mock instance.
func NewMockPreFilterPluginExtender(ctrl *gomock.Controller) *MockPreFilterPluginExtender {
	mock := &MockPreFilterPluginExtender{ctrl: ctrl}
	mock.recorder = &MockPreFilterPluginExtenderMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockPreFilterPluginExtender) EXPECT() *MockPreFilterPluginExtenderMockRecorder {
	return m.recorder
}

// AfterPreFilter mocks base method.
func (m *MockPreFilterPluginExtender) AfterPreFilter(ctx context.Context, state *framework.CycleState, pod *v1.Pod, preFilterResult *framework.PreFilterResult, preFilterStatus *framework.Status) (*framework.PreFilterResult, *framework.Status) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "AfterPreFilter", ctx, state, pod, preFilterResult, preFilterStatus)
	ret0, _ := ret[0].(*framework.PreFilterResult)
	ret1, _ := ret[1].(*framework.Status)
	return ret0, ret1
}

// AfterPreFilter indicates an expected call of AfterPreFilter.
func (mr *MockPreFilterPluginExtenderMockRecorder) AfterPreFilter(ctx, state, pod, preFilterResult, preFilterStatus any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "AfterPreFilter", reflect.TypeOf((*MockPreFilterPluginExtender)(nil).AfterPreFilter), ctx, state, pod, preFilterResult, preFilterStatus)
}

// BeforePreFilter mocks base method.
func (m *MockPreFilterPluginExtender) BeforePreFilter(ctx context.Context, state *framework.CycleState, pod *v1.Pod) (*framework.PreFilterResult, *framework.Status) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "BeforePreFilter", ctx, state, pod)
	ret0, _ := ret[0].(*framework.PreFilterResult)
	ret1, _ := ret[1].(*framework.Status)
	return ret0, ret1
}

// BeforePreFilter indicates an expected call of BeforePreFilter.
func (mr *MockPreFilterPluginExtenderMockRecorder) BeforePreFilter(ctx, state, pod any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "BeforePreFilter", reflect.TypeOf((*MockPreFilterPluginExtender)(nil).BeforePreFilter), ctx, state, pod)
}

// MockFilterPluginExtender is a mock of FilterPluginExtender interface.
type MockFilterPluginExtender struct {
	ctrl     *gomock.Controller
	recorder *MockFilterPluginExtenderMockRecorder
	isgomock struct{}
}

// MockFilterPluginExtenderMockRecorder is the mock recorder for MockFilterPluginExtender.
type MockFilterPluginExtenderMockRecorder struct {
	mock *MockFilterPluginExtender
}

// NewMockFilterPluginExtender creates a new mock instance.
func NewMockFilterPluginExtender(ctrl *gomock.Controller) *MockFilterPluginExtender {
	mock := &MockFilterPluginExtender{ctrl: ctrl}
	mock.recorder = &MockFilterPluginExtenderMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockFilterPluginExtender) EXPECT() *MockFilterPluginExtenderMockRecorder {
	return m.recorder
}

// AfterFilter mocks base method.
func (m *MockFilterPluginExtender) AfterFilter(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo, filterResult *framework.Status) *framework.Status {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "AfterFilter", ctx, state, pod, nodeInfo, filterResult)
	ret0, _ := ret[0].(*framework.Status)
	return ret0
}

// AfterFilter indicates an expected call of AfterFilter.
func (mr *MockFilterPluginExtenderMockRecorder) AfterFilter(ctx, state, pod, nodeInfo, filterResult any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "AfterFilter", reflect.TypeOf((*MockFilterPluginExtender)(nil).AfterFilter), ctx, state, pod, nodeInfo, filterResult)
}

// BeforeFilter mocks base method.
func (m *MockFilterPluginExtender) BeforeFilter(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo) *framework.Status {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "BeforeFilter", ctx, state, pod, nodeInfo)
	ret0, _ := ret[0].(*framework.Status)
	return ret0
}

// BeforeFilter indicates an expected call of BeforeFilter.
func (mr *MockFilterPluginExtenderMockRecorder) BeforeFilter(ctx, state, pod, nodeInfo any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "BeforeFilter", reflect.TypeOf((*MockFilterPluginExtender)(nil).BeforeFilter), ctx, state, pod, nodeInfo)
}

// MockPostFilterPluginExtender is a mock of PostFilterPluginExtender interface.
type MockPostFilterPluginExtender struct {
	ctrl     *gomock.Controller
	recorder *MockPostFilterPluginExtenderMockRecorder
	isgomock struct{}
}

// MockPostFilterPluginExtenderMockRecorder is the mock recorder for MockPostFilterPluginExtender.
type MockPostFilterPluginExtenderMockRecorder struct {
	mock *MockPostFilterPluginExtender
}

// NewMockPostFilterPluginExtender creates a new mock instance.
func NewMockPostFilterPluginExtender(ctrl *gomock.Controller) *MockPostFilterPluginExtender {
	mock := &MockPostFilterPluginExtender{ctrl: ctrl}
	mock.recorder = &MockPostFilterPluginExtenderMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockPostFilterPluginExtender) EXPECT() *MockPostFilterPluginExtenderMockRecorder {
	return m.recorder
}

// AfterPostFilter mocks base method.
func (m *MockPostFilterPluginExtender) AfterPostFilter(ctx context.Context, state *framework.CycleState, pod *v1.Pod, filteredNodeStatusMap framework.NodeToStatusMap, postFilterResult *framework.PostFilterResult, status *framework.Status) (*framework.PostFilterResult, *framework.Status) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "AfterPostFilter", ctx, state, pod, filteredNodeStatusMap, postFilterResult, status)
	ret0, _ := ret[0].(*framework.PostFilterResult)
	ret1, _ := ret[1].(*framework.Status)
	return ret0, ret1
}

// AfterPostFilter indicates an expected call of AfterPostFilter.
func (mr *MockPostFilterPluginExtenderMockRecorder) AfterPostFilter(ctx, state, pod, filteredNodeStatusMap, postFilterResult, status any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "AfterPostFilter", reflect.TypeOf((*MockPostFilterPluginExtender)(nil).AfterPostFilter), ctx, state, pod, filteredNodeStatusMap, postFilterResult, status)
}

// BeforePostFilter mocks base method.
func (m *MockPostFilterPluginExtender) BeforePostFilter(ctx context.Context, state *framework.CycleState, pod *v1.Pod, filteredNodeStatusMap framework.NodeToStatusMap) (*framework.PostFilterResult, *framework.Status) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "BeforePostFilter", ctx, state, pod, filteredNodeStatusMap)
	ret0, _ := ret[0].(*framework.PostFilterResult)
	ret1, _ := ret[1].(*framework.Status)
	return ret0, ret1
}

// BeforePostFilter indicates an expected call of BeforePostFilter.
func (mr *MockPostFilterPluginExtenderMockRecorder) BeforePostFilter(ctx, state, pod, filteredNodeStatusMap any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "BeforePostFilter", reflect.TypeOf((*MockPostFilterPluginExtender)(nil).BeforePostFilter), ctx, state, pod, filteredNodeStatusMap)
}

// MockPreScorePluginExtender is a mock of PreScorePluginExtender interface.
type MockPreScorePluginExtender struct {
	ctrl     *gomock.Controller
	recorder *MockPreScorePluginExtenderMockRecorder
	isgomock struct{}
}

// MockPreScorePluginExtenderMockRecorder is the mock recorder for MockPreScorePluginExtender.
type MockPreScorePluginExtenderMockRecorder struct {
	mock *MockPreScorePluginExtender
}

// NewMockPreScorePluginExtender creates a new mock instance.
func NewMockPreScorePluginExtender(ctrl *gomock.Controller) *MockPreScorePluginExtender {
	mock := &MockPreScorePluginExtender{ctrl: ctrl}
	mock.recorder = &MockPreScorePluginExtenderMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockPreScorePluginExtender) EXPECT() *MockPreScorePluginExtenderMockRecorder {
	return m.recorder
}

// AfterPreScore mocks base method.
func (m *MockPreScorePluginExtender) AfterPreScore(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodes []*framework.NodeInfo, preScoreStatus *framework.Status) *framework.Status {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "AfterPreScore", ctx, state, pod, nodes, preScoreStatus)
	ret0, _ := ret[0].(*framework.Status)
	return ret0
}

// AfterPreScore indicates an expected call of AfterPreScore.
func (mr *MockPreScorePluginExtenderMockRecorder) AfterPreScore(ctx, state, pod, nodes, preScoreStatus any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "AfterPreScore", reflect.TypeOf((*MockPreScorePluginExtender)(nil).AfterPreScore), ctx, state, pod, nodes, preScoreStatus)
}

// BeforePreScore mocks base method.
func (m *MockPreScorePluginExtender) BeforePreScore(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodes []*framework.NodeInfo) *framework.Status {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "BeforePreScore", ctx, state, pod, nodes)
	ret0, _ := ret[0].(*framework.Status)
	return ret0
}

// BeforePreScore indicates an expected call of BeforePreScore.
func (mr *MockPreScorePluginExtenderMockRecorder) BeforePreScore(ctx, state, pod, nodes any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "BeforePreScore", reflect.TypeOf((*MockPreScorePluginExtender)(nil).BeforePreScore), ctx, state, pod, nodes)
}

// MockScorePluginExtender is a mock of ScorePluginExtender interface.
type MockScorePluginExtender struct {
	ctrl     *gomock.Controller
	recorder *MockScorePluginExtenderMockRecorder
	isgomock struct{}
}

// MockScorePluginExtenderMockRecorder is the mock recorder for MockScorePluginExtender.
type MockScorePluginExtenderMockRecorder struct {
	mock *MockScorePluginExtender
}

// NewMockScorePluginExtender creates a new mock instance.
func NewMockScorePluginExtender(ctrl *gomock.Controller) *MockScorePluginExtender {
	mock := &MockScorePluginExtender{ctrl: ctrl}
	mock.recorder = &MockScorePluginExtenderMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockScorePluginExtender) EXPECT() *MockScorePluginExtenderMockRecorder {
	return m.recorder
}

// AfterScore mocks base method.
func (m *MockScorePluginExtender) AfterScore(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeName string, score int64, scoreResult *framework.Status) (int64, *framework.Status) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "AfterScore", ctx, state, pod, nodeName, score, scoreResult)
	ret0, _ := ret[0].(int64)
	ret1, _ := ret[1].(*framework.Status)
	return ret0, ret1
}

// AfterScore indicates an expected call of AfterScore.
func (mr *MockScorePluginExtenderMockRecorder) AfterScore(ctx, state, pod, nodeName, score, scoreResult any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "AfterScore", reflect.TypeOf((*MockScorePluginExtender)(nil).AfterScore), ctx, state, pod, nodeName, score, scoreResult)
}

// BeforeScore mocks base method.
func (m *MockScorePluginExtender) BeforeScore(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeName string) (int64, *framework.Status) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "BeforeScore", ctx, state, pod, nodeName)
	ret0, _ := ret[0].(int64)
	ret1, _ := ret[1].(*framework.Status)
	return ret0, ret1
}

// BeforeScore indicates an expected call of BeforeScore.
func (mr *MockScorePluginExtenderMockRecorder) BeforeScore(ctx, state, pod, nodeName any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "BeforeScore", reflect.TypeOf((*MockScorePluginExtender)(nil).BeforeScore), ctx, state, pod, nodeName)
}

// MockNormalizeScorePluginExtender is a mock of NormalizeScorePluginExtender interface.
type MockNormalizeScorePluginExtender struct {
	ctrl     *gomock.Controller
	recorder *MockNormalizeScorePluginExtenderMockRecorder
	isgomock struct{}
}

// MockNormalizeScorePluginExtenderMockRecorder is the mock recorder for MockNormalizeScorePluginExtender.
type MockNormalizeScorePluginExtenderMockRecorder struct {
	mock *MockNormalizeScorePluginExtender
}

// NewMockNormalizeScorePluginExtender creates a new mock instance.
func NewMockNormalizeScorePluginExtender(ctrl *gomock.Controller) *MockNormalizeScorePluginExtender {
	mock := &MockNormalizeScorePluginExtender{ctrl: ctrl}
	mock.recorder = &MockNormalizeScorePluginExtenderMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockNormalizeScorePluginExtender) EXPECT() *MockNormalizeScorePluginExtenderMockRecorder {
	return m.recorder
}

// AfterNormalizeScore mocks base method.
func (m *MockNormalizeScorePluginExtender) AfterNormalizeScore(ctx context.Context, state *framework.CycleState, pod *v1.Pod, scores framework.NodeScoreList, normalizeScoreResult *framework.Status) *framework.Status {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "AfterNormalizeScore", ctx, state, pod, scores, normalizeScoreResult)
	ret0, _ := ret[0].(*framework.Status)
	return ret0
}

// AfterNormalizeScore indicates an expected call of AfterNormalizeScore.
func (mr *MockNormalizeScorePluginExtenderMockRecorder) AfterNormalizeScore(ctx, state, pod, scores, normalizeScoreResult any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "AfterNormalizeScore", reflect.TypeOf((*MockNormalizeScorePluginExtender)(nil).AfterNormalizeScore), ctx, state, pod, scores, normalizeScoreResult)
}

// BeforeNormalizeScore mocks base method.
func (m *MockNormalizeScorePluginExtender) BeforeNormalizeScore(ctx context.Context, state *framework.CycleState, pod *v1.Pod, scores framework.NodeScoreList) *framework.Status {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "BeforeNormalizeScore", ctx, state, pod, scores)
	ret0, _ := ret[0].(*framework.Status)
	return ret0
}

// BeforeNormalizeScore indicates an expected call of BeforeNormalizeScore.
func (mr *MockNormalizeScorePluginExtenderMockRecorder) BeforeNormalizeScore(ctx, state, pod, scores any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "BeforeNormalizeScore", reflect.TypeOf((*MockNormalizeScorePluginExtender)(nil).BeforeNormalizeScore), ctx, state, pod, scores)
}

// MockReservePluginExtender is a mock of ReservePluginExtender interface.
type MockReservePluginExtender struct {
	ctrl     *gomock.Controller
	recorder *MockReservePluginExtenderMockRecorder
	isgomock struct{}
}

// MockReservePluginExtenderMockRecorder is the mock recorder for MockReservePluginExtender.
type MockReservePluginExtenderMockRecorder struct {
	mock *MockReservePluginExtender
}

// NewMockReservePluginExtender creates a new mock instance.
func NewMockReservePluginExtender(ctrl *gomock.Controller) *MockReservePluginExtender {
	mock := &MockReservePluginExtender{ctrl: ctrl}
	mock.recorder = &MockReservePluginExtenderMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockReservePluginExtender) EXPECT() *MockReservePluginExtenderMockRecorder {
	return m.recorder
}

// AfterReserve mocks base method.
func (m *MockReservePluginExtender) AfterReserve(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodename string, reserveStatus *framework.Status) *framework.Status {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "AfterReserve", ctx, state, pod, nodename, reserveStatus)
	ret0, _ := ret[0].(*framework.Status)
	return ret0
}

// AfterReserve indicates an expected call of AfterReserve.
func (mr *MockReservePluginExtenderMockRecorder) AfterReserve(ctx, state, pod, nodename, reserveStatus any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "AfterReserve", reflect.TypeOf((*MockReservePluginExtender)(nil).AfterReserve), ctx, state, pod, nodename, reserveStatus)
}

// AfterUnreserve mocks base method.
func (m *MockReservePluginExtender) AfterUnreserve(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodename string) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "AfterUnreserve", ctx, state, pod, nodename)
}

// AfterUnreserve indicates an expected call of AfterUnreserve.
func (mr *MockReservePluginExtenderMockRecorder) AfterUnreserve(ctx, state, pod, nodename any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "AfterUnreserve", reflect.TypeOf((*MockReservePluginExtender)(nil).AfterUnreserve), ctx, state, pod, nodename)
}

// BeforeReserve mocks base method.
func (m *MockReservePluginExtender) BeforeReserve(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodename string) *framework.Status {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "BeforeReserve", ctx, state, pod, nodename)
	ret0, _ := ret[0].(*framework.Status)
	return ret0
}

// BeforeReserve indicates an expected call of BeforeReserve.
func (mr *MockReservePluginExtenderMockRecorder) BeforeReserve(ctx, state, pod, nodename any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "BeforeReserve", reflect.TypeOf((*MockReservePluginExtender)(nil).BeforeReserve), ctx, state, pod, nodename)
}

// BeforeUnreserve mocks base method.
func (m *MockReservePluginExtender) BeforeUnreserve(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodename string) *framework.Status {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "BeforeUnreserve", ctx, state, pod, nodename)
	ret0, _ := ret[0].(*framework.Status)
	return ret0
}

// BeforeUnreserve indicates an expected call of BeforeUnreserve.
func (mr *MockReservePluginExtenderMockRecorder) BeforeUnreserve(ctx, state, pod, nodename any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "BeforeUnreserve", reflect.TypeOf((*MockReservePluginExtender)(nil).BeforeUnreserve), ctx, state, pod, nodename)
}

// MockPermitPluginExtender is a mock of PermitPluginExtender interface.
type MockPermitPluginExtender struct {
	ctrl     *gomock.Controller
	recorder *MockPermitPluginExtenderMockRecorder
	isgomock struct{}
}

// MockPermitPluginExtenderMockRecorder is the mock recorder for MockPermitPluginExtender.
type MockPermitPluginExtenderMockRecorder struct {
	mock *MockPermitPluginExtender
}

// NewMockPermitPluginExtender creates a new mock instance.
func NewMockPermitPluginExtender(ctrl *gomock.Controller) *MockPermitPluginExtender {
	mock := &MockPermitPluginExtender{ctrl: ctrl}
	mock.recorder = &MockPermitPluginExtenderMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockPermitPluginExtender) EXPECT() *MockPermitPluginExtenderMockRecorder {
	return m.recorder
}

// AfterPermit mocks base method.
func (m *MockPermitPluginExtender) AfterPermit(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeName string, permitResult *framework.Status, timeout time.Duration) (*framework.Status, time.Duration) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "AfterPermit", ctx, state, pod, nodeName, permitResult, timeout)
	ret0, _ := ret[0].(*framework.Status)
	ret1, _ := ret[1].(time.Duration)
	return ret0, ret1
}

// AfterPermit indicates an expected call of AfterPermit.
func (mr *MockPermitPluginExtenderMockRecorder) AfterPermit(ctx, state, pod, nodeName, permitResult, timeout any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "AfterPermit", reflect.TypeOf((*MockPermitPluginExtender)(nil).AfterPermit), ctx, state, pod, nodeName, permitResult, timeout)
}

// BeforePermit mocks base method.
func (m *MockPermitPluginExtender) BeforePermit(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeName string) (*framework.Status, time.Duration) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "BeforePermit", ctx, state, pod, nodeName)
	ret0, _ := ret[0].(*framework.Status)
	ret1, _ := ret[1].(time.Duration)
	return ret0, ret1
}

// BeforePermit indicates an expected call of BeforePermit.
func (mr *MockPermitPluginExtenderMockRecorder) BeforePermit(ctx, state, pod, nodeName any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "BeforePermit", reflect.TypeOf((*MockPermitPluginExtender)(nil).BeforePermit), ctx, state, pod, nodeName)
}

// MockPreBindPluginExtender is a mock of PreBindPluginExtender interface.
type MockPreBindPluginExtender struct {
	ctrl     *gomock.Controller
	recorder *MockPreBindPluginExtenderMockRecorder
	isgomock struct{}
}

// MockPreBindPluginExtenderMockRecorder is the mock recorder for MockPreBindPluginExtender.
type MockPreBindPluginExtenderMockRecorder struct {
	mock *MockPreBindPluginExtender
}

// NewMockPreBindPluginExtender creates a new mock instance.
func NewMockPreBindPluginExtender(ctrl *gomock.Controller) *MockPreBindPluginExtender {
	mock := &MockPreBindPluginExtender{ctrl: ctrl}
	mock.recorder = &MockPreBindPluginExtenderMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockPreBindPluginExtender) EXPECT() *MockPreBindPluginExtenderMockRecorder {
	return m.recorder
}

// AfterPreBind mocks base method.
func (m *MockPreBindPluginExtender) AfterPreBind(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodename string, bindResult *framework.Status) *framework.Status {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "AfterPreBind", ctx, state, pod, nodename, bindResult)
	ret0, _ := ret[0].(*framework.Status)
	return ret0
}

// AfterPreBind indicates an expected call of AfterPreBind.
func (mr *MockPreBindPluginExtenderMockRecorder) AfterPreBind(ctx, state, pod, nodename, bindResult any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "AfterPreBind", reflect.TypeOf((*MockPreBindPluginExtender)(nil).AfterPreBind), ctx, state, pod, nodename, bindResult)
}

// BeforePreBind mocks base method.
func (m *MockPreBindPluginExtender) BeforePreBind(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodename string) *framework.Status {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "BeforePreBind", ctx, state, pod, nodename)
	ret0, _ := ret[0].(*framework.Status)
	return ret0
}

// BeforePreBind indicates an expected call of BeforePreBind.
func (mr *MockPreBindPluginExtenderMockRecorder) BeforePreBind(ctx, state, pod, nodename any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "BeforePreBind", reflect.TypeOf((*MockPreBindPluginExtender)(nil).BeforePreBind), ctx, state, pod, nodename)
}

// MockBindPluginExtender is a mock of BindPluginExtender interface.
type MockBindPluginExtender struct {
	ctrl     *gomock.Controller
	recorder *MockBindPluginExtenderMockRecorder
	isgomock struct{}
}

// MockBindPluginExtenderMockRecorder is the mock recorder for MockBindPluginExtender.
type MockBindPluginExtenderMockRecorder struct {
	mock *MockBindPluginExtender
}

// NewMockBindPluginExtender creates a new mock instance.
func NewMockBindPluginExtender(ctrl *gomock.Controller) *MockBindPluginExtender {
	mock := &MockBindPluginExtender{ctrl: ctrl}
	mock.recorder = &MockBindPluginExtenderMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockBindPluginExtender) EXPECT() *MockBindPluginExtenderMockRecorder {
	return m.recorder
}

// AfterBind mocks base method.
func (m *MockBindPluginExtender) AfterBind(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodename string, bindResult *framework.Status) *framework.Status {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "AfterBind", ctx, state, pod, nodename, bindResult)
	ret0, _ := ret[0].(*framework.Status)
	return ret0
}

// AfterBind indicates an expected call of AfterBind.
func (mr *MockBindPluginExtenderMockRecorder) AfterBind(ctx, state, pod, nodename, bindResult any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "AfterBind", reflect.TypeOf((*MockBindPluginExtender)(nil).AfterBind), ctx, state, pod, nodename, bindResult)
}

// BeforeBind mocks base method.
func (m *MockBindPluginExtender) BeforeBind(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodename string) *framework.Status {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "BeforeBind", ctx, state, pod, nodename)
	ret0, _ := ret[0].(*framework.Status)
	return ret0
}

// BeforeBind indicates an expected call of BeforeBind.
func (mr *MockBindPluginExtenderMockRecorder) BeforeBind(ctx, state, pod, nodename any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "BeforeBind", reflect.TypeOf((*MockBindPluginExtender)(nil).BeforeBind), ctx, state, pod, nodename)
}

// MockPostBindPluginExtender is a mock of PostBindPluginExtender interface.
type MockPostBindPluginExtender struct {
	ctrl     *gomock.Controller
	recorder *MockPostBindPluginExtenderMockRecorder
	isgomock struct{}
}

// MockPostBindPluginExtenderMockRecorder is the mock recorder for MockPostBindPluginExtender.
type MockPostBindPluginExtenderMockRecorder struct {
	mock *MockPostBindPluginExtender
}

// NewMockPostBindPluginExtender creates a new mock instance.
func NewMockPostBindPluginExtender(ctrl *gomock.Controller) *MockPostBindPluginExtender {
	mock := &MockPostBindPluginExtender{ctrl: ctrl}
	mock.recorder = &MockPostBindPluginExtenderMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockPostBindPluginExtender) EXPECT() *MockPostBindPluginExtenderMockRecorder {
	return m.recorder
}

// AfterPostBind mocks base method.
func (m *MockPostBindPluginExtender) AfterPostBind(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodename string) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "AfterPostBind", ctx, state, pod, nodename)
}

// AfterPostBind indicates an expected call of AfterPostBind.
func (mr *MockPostBindPluginExtenderMockRecorder) AfterPostBind(ctx, state, pod, nodename any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "AfterPostBind", reflect.TypeOf((*MockPostBindPluginExtender)(nil).AfterPostBind), ctx, state, pod, nodename)
}

// BeforePostBind mocks base method.
func (m *MockPostBindPluginExtender) BeforePostBind(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodename string) *framework.Status {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "BeforePostBind", ctx, state, pod, nodename)
	ret0, _ := ret[0].(*framework.Status)
	return ret0
}

// BeforePostBind indicates an expected call of BeforePostBind.
func (mr *MockPostBindPluginExtenderMockRecorder) BeforePostBind(ctx, state, pod, nodename any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "BeforePostBind", reflect.TypeOf((*MockPostBindPluginExtender)(nil).BeforePostBind), ctx, state, pod, nodename)
}
</file>

<file path="kube-scheduler-simulator/simulator/scheduler/plugin/resultstore/store_test.go">
package resultstore

import (
	"encoding/json"
	"sync"
	"testing"
	"time"

	"github.com/stretchr/testify/assert"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/util/sets"
	"k8s.io/kubernetes/pkg/scheduler/framework"

	"sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler/plugin/annotation"
)

func TestStore_AddFilterResult(t *testing.T) {
	t.Parallel()
	type args struct {
		namespace  string
		podName    string
		nodeName   string
		pluginName string
		reason     string
	}
	tests := []struct {
		name          string
		resultbefore  map[key]*result
		args          args
		wantResultMap map[key]*result
	}{
		{
			name:         "success with empty result",
			resultbefore: map[key]*result{},
			args: args{
				namespace:  "default",
				podName:    "pod1",
				nodeName:   "node1",
				pluginName: "plugin1",
				reason:     PassedFilterMessage,
			},
			wantResultMap: map[key]*result{
				"default/pod1": {
					selectedNode:    "",
					preScore:        map[string]string{},
					preFilterStatus: map[string]string{},
					preFilterResult: map[string][]string{},
					permit:          map[string]string{},
					permitTimeout:   map[string]string{},
					reserve:         map[string]string{},
					prebind:         map[string]string{},
					bind:            map[string]string{},
					score:           map[string]map[string]string{},
					finalScore:      map[string]map[string]string{},
					customResults:   map[string]string{},
					filter: map[string]map[string]string{
						"node1": {
							"plugin1": PassedFilterMessage,
						},
					},
					postFilter: map[string]map[string]string{},
				},
			},
		},
		{
			name: "success with non-empty filter map for the node",
			resultbefore: map[key]*result{
				"default/pod1": {
					score:      map[string]map[string]string{},
					finalScore: map[string]map[string]string{},
					filter: map[string]map[string]string{
						"node1": {
							"plugin1": PassedFilterMessage,
						},
					},
					postFilter: map[string]map[string]string{},
				},
			},
			args: args{
				namespace:  "default",
				podName:    "pod1",
				nodeName:   "node1",
				pluginName: "plugin2",
				reason:     PassedFilterMessage,
			},
			wantResultMap: map[key]*result{
				"default/pod1": {
					score:      map[string]map[string]string{},
					finalScore: map[string]map[string]string{},
					filter: map[string]map[string]string{
						"node1": {
							"plugin1": PassedFilterMessage,
							"plugin2": PassedFilterMessage,
						},
					},
					postFilter: map[string]map[string]string{},
				},
			},
		},
		{
			name: "success when no map for the node",
			resultbefore: map[key]*result{
				"default/pod1": {
					score:      map[string]map[string]string{},
					finalScore: map[string]map[string]string{},
					filter: map[string]map[string]string{
						"node0": {
							"plugin1": PassedFilterMessage,
						},
					},
					postFilter: map[string]map[string]string{},
				},
			},
			args: args{
				namespace:  "default",
				podName:    "pod1",
				nodeName:   "node1",
				pluginName: "plugin1",
				reason:     PassedFilterMessage,
			},
			wantResultMap: map[key]*result{
				"default/pod1": {
					score:      map[string]map[string]string{},
					finalScore: map[string]map[string]string{},
					filter: map[string]map[string]string{
						"node0": {
							"plugin1": PassedFilterMessage,
						},
						"node1": {
							"plugin1": PassedFilterMessage,
						},
					},
					postFilter: map[string]map[string]string{},
				},
			},
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			s := &Store{
				mu:      new(sync.Mutex),
				results: tt.resultbefore,
			}
			s.AddFilterResult(tt.args.namespace, tt.args.podName, tt.args.nodeName, tt.args.pluginName, tt.args.reason)
			assert.Equal(t, tt.wantResultMap, s.results)
		})
	}
}

func TestStore_AddPostFilterResult(t *testing.T) {
	t.Parallel()
	type args struct {
		namespace         string
		podName           string
		nominatedNodeName string
		pluginName        string
		nodeNames         []string
	}
	tests := []struct {
		name          string
		resultbefore  map[key]*result
		args          args
		wantResultMap map[key]*result
	}{
		{
			name:         "success with empty result",
			resultbefore: map[key]*result{},
			args: args{
				namespace:         "default",
				podName:           "pod1",
				nominatedNodeName: "node1",
				pluginName:        "plugin1",
				nodeNames:         []string{"node1", "node2"},
			},
			wantResultMap: map[key]*result{
				"default/pod1": {
					selectedNode:    "",
					preScore:        map[string]string{},
					preFilterStatus: map[string]string{},
					preFilterResult: map[string][]string{},
					permit:          map[string]string{},
					permitTimeout:   map[string]string{},
					reserve:         map[string]string{},
					prebind:         map[string]string{},
					bind:            map[string]string{},
					score:           map[string]map[string]string{},
					finalScore:      map[string]map[string]string{},
					filter:          map[string]map[string]string{},
					customResults:   map[string]string{},
					postFilter: map[string]map[string]string{
						"node1": {
							"plugin1": PostFilterNominatedMessage,
						},
						"node2": {},
					},
				},
			},
		},
		{
			name: "success with non-empty postFilter map for the node",
			resultbefore: map[key]*result{
				"default/pod1": {
					score:      map[string]map[string]string{},
					finalScore: map[string]map[string]string{},
					filter:     map[string]map[string]string{},
					postFilter: map[string]map[string]string{
						"node1": {},
					},
				},
			},
			args: args{
				namespace:         "default",
				podName:           "pod1",
				nominatedNodeName: "node1",
				pluginName:        "plugin2",
				nodeNames:         []string{"node1", "node2"},
			},
			wantResultMap: map[key]*result{
				"default/pod1": {
					score:      map[string]map[string]string{},
					finalScore: map[string]map[string]string{},
					filter:     map[string]map[string]string{},
					postFilter: map[string]map[string]string{
						"node1": {
							"plugin2": PostFilterNominatedMessage,
						},
						"node2": {},
					},
				},
			},
		},
		{
			name: "success when no map for the node",
			resultbefore: map[key]*result{
				"default/pod1": {
					score:      map[string]map[string]string{},
					finalScore: map[string]map[string]string{},
					filter:     map[string]map[string]string{},
					postFilter: map[string]map[string]string{
						"node0": {},
					},
				},
			},
			args: args{
				namespace:         "default",
				podName:           "pod1",
				nominatedNodeName: "node1",
				pluginName:        "plugin2",
				nodeNames:         []string{"node1", "node2"},
			},
			wantResultMap: map[key]*result{
				"default/pod1": {
					score:      map[string]map[string]string{},
					finalScore: map[string]map[string]string{},
					filter:     map[string]map[string]string{},
					postFilter: map[string]map[string]string{
						"node0": {},
						"node1": {
							"plugin2": PostFilterNominatedMessage,
						},
						"node2": {},
					},
				},
			},
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			s := &Store{
				mu:      new(sync.Mutex),
				results: tt.resultbefore,
			}
			s.AddPostFilterResult(tt.args.namespace, tt.args.podName, tt.args.nominatedNodeName, tt.args.pluginName, tt.args.nodeNames)
			assert.Equal(t, tt.wantResultMap, s.results)
		})
	}
}

func TestStore_AddScoreResult(t *testing.T) {
	t.Parallel()
	type args struct {
		namespace  string
		podName    string
		nodeName   string
		pluginName string
		score      int64
	}
	tests := []struct {
		name              string
		resultbefore      map[key]*result
		scorePluginWeight map[string]int32
		args              args
		wantResultMap     map[key]*result
	}{
		{
			name:              "success with empty result",
			resultbefore:      map[key]*result{},
			scorePluginWeight: map[string]int32{"plugin1": 2},
			args: args{
				namespace:  "default",
				podName:    "pod1",
				nodeName:   "node1",
				pluginName: "plugin1",
				score:      10,
			},
			wantResultMap: map[key]*result{
				"default/pod1": {
					selectedNode:    "",
					preScore:        map[string]string{},
					preFilterStatus: map[string]string{},
					preFilterResult: map[string][]string{},
					permit:          map[string]string{},
					permitTimeout:   map[string]string{},
					reserve:         map[string]string{},
					prebind:         map[string]string{},
					bind:            map[string]string{},
					filter:          map[string]map[string]string{},
					postFilter:      map[string]map[string]string{},
					customResults:   map[string]string{},
					finalScore: map[string]map[string]string{
						"node1": {
							"plugin1": "20",
						},
					},
					score: map[string]map[string]string{
						"node1": {
							"plugin1": "10",
						},
					},
				},
			},
		},
		{
			name: "success with non-empty filter map for the node",
			resultbefore: map[key]*result{
				"default/pod1": {
					filter:     map[string]map[string]string{},
					postFilter: map[string]map[string]string{},
					finalScore: map[string]map[string]string{
						"node1": {
							"plugin1": "30",
						},
					},
					score: map[string]map[string]string{
						"node1": {
							"plugin1": "10",
						},
					},
				},
			},
			scorePluginWeight: map[string]int32{"plugin2": 2},
			args: args{
				namespace:  "default",
				podName:    "pod1",
				nodeName:   "node1",
				pluginName: "plugin2",
				score:      10,
			},
			wantResultMap: map[key]*result{
				"default/pod1": {
					filter:     map[string]map[string]string{},
					postFilter: map[string]map[string]string{},
					finalScore: map[string]map[string]string{
						"node1": {
							"plugin1": "30",
							"plugin2": "20",
						},
					},
					score: map[string]map[string]string{
						"node1": {
							"plugin1": "10",
							"plugin2": "10",
						},
					},
				},
			},
		},
		{
			name: "success when no map for the node",
			resultbefore: map[key]*result{
				"default/pod1": {
					filter:     map[string]map[string]string{},
					postFilter: map[string]map[string]string{},
					finalScore: map[string]map[string]string{
						"node0": {
							"plugin1": "20",
						},
					},
					score: map[string]map[string]string{
						"node0": {
							"plugin1": "10",
						},
					},
				},
			},
			scorePluginWeight: map[string]int32{"plugin1": 2},
			args: args{
				namespace:  "default",
				podName:    "pod1",
				nodeName:   "node1",
				pluginName: "plugin1",
				score:      10,
			},
			wantResultMap: map[key]*result{
				"default/pod1": {
					filter:     map[string]map[string]string{},
					postFilter: map[string]map[string]string{},
					finalScore: map[string]map[string]string{
						"node0": {
							"plugin1": "20",
						},
						"node1": {
							"plugin1": "20",
						},
					},
					score: map[string]map[string]string{
						"node0": {
							"plugin1": "10",
						},
						"node1": {
							"plugin1": "10",
						},
					},
				},
			},
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			s := &Store{
				mu:                new(sync.Mutex),
				results:           tt.resultbefore,
				scorePluginWeight: tt.scorePluginWeight,
			}
			s.AddScoreResult(tt.args.namespace, tt.args.podName, tt.args.nodeName, tt.args.pluginName, tt.args.score)
			assert.Equal(t, tt.wantResultMap, s.results)
		})
	}
}

func TestStore_AddNormalizedScoreResult(t *testing.T) {
	t.Parallel()
	type args struct {
		namespace  string
		podName    string
		nodeName   string
		pluginName string
		score      int64
	}
	tests := []struct {
		name              string
		resultbefore      map[key]*result
		scorePluginWeight map[string]int32
		args              args
		wantResultMap     map[key]*result
	}{
		{
			name:              "success with empty result",
			resultbefore:      map[key]*result{},
			scorePluginWeight: map[string]int32{"plugin1": 2},
			args: args{
				namespace:  "default",
				podName:    "pod1",
				nodeName:   "node1",
				pluginName: "plugin1",
				score:      10,
			},
			wantResultMap: map[key]*result{
				"default/pod1": {
					selectedNode:    "",
					preScore:        map[string]string{},
					preFilterStatus: map[string]string{},
					preFilterResult: map[string][]string{},
					permit:          map[string]string{},
					permitTimeout:   map[string]string{},
					reserve:         map[string]string{},
					prebind:         map[string]string{},
					bind:            map[string]string{},
					filter:          map[string]map[string]string{},
					postFilter:      map[string]map[string]string{},
					score:           map[string]map[string]string{},
					customResults:   map[string]string{},
					finalScore: map[string]map[string]string{
						"node1": {
							"plugin1": "20",
						},
					},
				},
			},
		},
		{
			name: "success with non-empty filter map for the node",
			resultbefore: map[key]*result{
				"default/pod1": {
					filter:     map[string]map[string]string{},
					postFilter: map[string]map[string]string{},
					finalScore: map[string]map[string]string{
						"node1": {
							"plugin1": "30",
						},
					},
				},
			},
			scorePluginWeight: map[string]int32{"plugin2": 2},
			args: args{
				namespace:  "default",
				podName:    "pod1",
				nodeName:   "node1",
				pluginName: "plugin2",
				score:      10,
			},
			wantResultMap: map[key]*result{
				"default/pod1": {
					filter:     map[string]map[string]string{},
					postFilter: map[string]map[string]string{},
					finalScore: map[string]map[string]string{
						"node1": {
							"plugin1": "30",
							"plugin2": "20",
						},
					},
				},
			},
		},
		{
			name: "success when no map for the node",
			resultbefore: map[key]*result{
				"default/pod1": {
					filter:     map[string]map[string]string{},
					postFilter: map[string]map[string]string{},
					finalScore: map[string]map[string]string{
						"node0": {
							"plugin1": "20",
						},
					},
				},
			},
			scorePluginWeight: map[string]int32{"plugin1": 2},
			args: args{
				namespace:  "default",
				podName:    "pod1",
				nodeName:   "node1",
				pluginName: "plugin1",
				score:      10,
			},
			wantResultMap: map[key]*result{
				"default/pod1": {
					filter:     map[string]map[string]string{},
					postFilter: map[string]map[string]string{},
					finalScore: map[string]map[string]string{
						"node0": {
							"plugin1": "20",
						},
						"node1": {
							"plugin1": "20",
						},
					},
				},
			},
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			s := &Store{
				mu:                new(sync.Mutex),
				results:           tt.resultbefore,
				scorePluginWeight: tt.scorePluginWeight,
			}
			s.AddNormalizedScoreResult(tt.args.namespace, tt.args.podName, tt.args.nodeName, tt.args.pluginName, tt.args.score)
			assert.Equal(t, tt.wantResultMap, s.results)
		})
	}
}

func TestStore_GetStoredResult(t *testing.T) {
	t.Parallel()
	podName := "pod1"
	namespace := "default"
	tests := []struct {
		name           string
		result         map[key]*result
		newObj         *corev1.Pod
		wantAnnotation map[string]string
	}{
		{
			name: "success",
			result: map[key]*result{
				"default/pod1": {
					selectedNode: "node",
					preScore: map[string]string{
						"plugin1": "preScore",
					},
					preFilterStatus: map[string]string{
						"plugin1": "preFilterStatus",
					},
					preFilterResult: map[string][]string{
						"plugin1": {"node1", "node2"},
					},
					permit: map[string]string{
						"plugin1": "permit",
					},
					permitTimeout: map[string]string{
						"plugin1": "1s",
					},
					reserve: map[string]string{
						"plugin1": "reserve",
					},
					prebind: map[string]string{
						"plugin1": "prebind",
					},
					bind: map[string]string{
						"plugin1": "bind",
					},
					filter: map[string]map[string]string{
						"node0": {
							"plugin1": PassedFilterMessage,
						},
						"node1": {
							"plugin1": PassedFilterMessage,
						},
					},
					finalScore: map[string]map[string]string{
						"node0": {
							"plugin1": "20",
						},
						"node1": {
							"plugin1": "20",
						},
					},
					postFilter: map[string]map[string]string{
						"node0": {
							"plugin1": PostFilterNominatedMessage,
						},
						"node1": {},
					},
					score: map[string]map[string]string{
						"node0": {
							"plugin1": "10",
						},
						"node1": {
							"plugin1": "10",
						},
					},
				},
			},
			newObj: &corev1.Pod{
				ObjectMeta: metav1.ObjectMeta{
					Name:      podName,
					Namespace: namespace,
				},
			},
			wantAnnotation: map[string]string{
				annotation.SelectedNodeAnnotationKey: "node",
				annotation.PreScoreResultAnnotationKey: func() string {
					d, _ := json.Marshal(map[string]string{
						"plugin1": "preScore",
					})
					return string(d)
				}(),
				annotation.PreFilterResultAnnotationKey: func() string {
					d, _ := json.Marshal(map[string][]string{
						"plugin1": {"node1", "node2"},
					})
					return string(d)
				}(),
				annotation.PreFilterStatusResultAnnotationKey: func() string {
					d, _ := json.Marshal(map[string]string{
						"plugin1": "preFilterStatus",
					})
					return string(d)
				}(),
				annotation.PermitStatusResultAnnotationKey: func() string {
					d, _ := json.Marshal(map[string]string{
						"plugin1": "permit",
					})
					return string(d)
				}(),
				annotation.PermitTimeoutResultAnnotationKey: func() string {
					d, _ := json.Marshal(map[string]string{
						"plugin1": "1s",
					})
					return string(d)
				}(),
				annotation.ReserveResultAnnotationKey: func() string {
					d, _ := json.Marshal(map[string]string{
						"plugin1": "reserve",
					})
					return string(d)
				}(),
				annotation.PreBindResultAnnotationKey: func() string {
					d, _ := json.Marshal(map[string]string{
						"plugin1": "prebind",
					})
					return string(d)
				}(),
				annotation.BindResultAnnotationKey: func() string {
					d, _ := json.Marshal(map[string]string{
						"plugin1": "bind",
					})
					return string(d)
				}(),
				annotation.FilterResultAnnotationKey: func() string {
					r := map[string]map[string]string{
						"node0": {
							"plugin1": PassedFilterMessage,
						},
						"node1": {
							"plugin1": PassedFilterMessage,
						},
					}
					d, _ := json.Marshal(r)
					return string(d)
				}(),
				annotation.ScoreResultAnnotationKey: func() string {
					r := map[string]map[string]string{
						"node0": {
							"plugin1": "10",
						},
						"node1": {
							"plugin1": "10",
						},
					}
					d, _ := json.Marshal(r)
					return string(d)
				}(),
				annotation.FinalScoreResultAnnotationKey: func() string {
					r := map[string]map[string]string{
						"node0": {
							"plugin1": "20",
						},
						"node1": {
							"plugin1": "20",
						},
					}
					d, _ := json.Marshal(r)
					return string(d)
				}(),
				annotation.PostFilterResultAnnotationKey: func() string {
					r := map[string]map[string]string{
						"node0": {
							"plugin1": PostFilterNominatedMessage,
						},
						"node1": {},
					}
					d, _ := json.Marshal(r)
					return string(d)
				}(),
			},
		},
		{
			name:   "do nothing if store doesn't have data",
			result: map[key]*result{},
			newObj: &corev1.Pod{
				ObjectMeta: metav1.ObjectMeta{
					Name:      podName,
					Namespace: namespace,
				},
			},
		},
		{
			name: "success without some data on store",
			result: map[key]*result{
				"default/pod1": {
					score:      map[string]map[string]string{},
					finalScore: map[string]map[string]string{},
					filter: map[string]map[string]string{
						"node0": {
							"plugin1": PassedFilterMessage,
						},
						"node1": {
							"plugin1": PassedFilterMessage,
						},
					},
					postFilter: map[string]map[string]string{},
				},
			},
			newObj: &corev1.Pod{
				ObjectMeta: metav1.ObjectMeta{
					Name:      podName,
					Namespace: namespace,
				},
			},
			wantAnnotation: map[string]string{
				annotation.FilterResultAnnotationKey: func() string {
					r := map[string]map[string]string{
						"node0": {
							"plugin1": PassedFilterMessage,
						},
						"node1": {
							"plugin1": PassedFilterMessage,
						},
					}
					d, _ := json.Marshal(r)
					return string(d)
				}(),
				annotation.ScoreResultAnnotationKey:           "{}",
				annotation.FinalScoreResultAnnotationKey:      "{}",
				annotation.PostFilterResultAnnotationKey:      "{}",
				annotation.SelectedNodeAnnotationKey:          "",
				annotation.PreScoreResultAnnotationKey:        "{}",
				annotation.PreFilterResultAnnotationKey:       "{}",
				annotation.PreFilterStatusResultAnnotationKey: "{}",
				annotation.PermitStatusResultAnnotationKey:    "{}",
				annotation.PermitTimeoutResultAnnotationKey:   "{}",
				annotation.ReserveResultAnnotationKey:         "{}",
				annotation.PreBindResultAnnotationKey:         "{}",
				annotation.BindResultAnnotationKey:            "{}",
			},
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			s := &Store{
				mu:      new(sync.Mutex),
				results: tt.result,
			}
			p := tt.newObj
			result := s.GetStoredResult(p)
			assert.Equal(t, tt.wantAnnotation, result)
		})
	}
}

func TestStore_AddPreFilterResult(t *testing.T) {
	t.Parallel()
	type args struct {
		namespace       string
		podName         string
		pluginName      string
		reason          string
		preFilterResult *framework.PreFilterResult
	}
	tests := []struct {
		name          string
		args          args
		wantResultMap map[key]*result
	}{
		{
			name: "success",
			args: args{
				namespace:  "namespace",
				podName:    "pod",
				pluginName: "plugin",
				reason:     "reason",
				preFilterResult: &framework.PreFilterResult{
					NodeNames: sets.New("hoge"),
				},
			},
			wantResultMap: func() map[key]*result {
				d := newData()
				d.preFilterResult = map[string][]string{
					"plugin": {"hoge"},
				}
				d.preFilterStatus = map[string]string{
					"plugin": "reason",
				}
				return map[key]*result{
					"namespace/pod": d,
				}
			}(),
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			s := &Store{mu: &sync.Mutex{}, results: map[key]*result{}}
			s.AddPreFilterResult(tt.args.namespace, tt.args.podName, tt.args.pluginName, tt.args.reason, tt.args.preFilterResult)
			assert.Equal(t, tt.wantResultMap, s.results)
		})
	}
}

func TestStore_AddPreScoreResult(t *testing.T) {
	t.Parallel()
	type args struct {
		namespace  string
		podName    string
		pluginName string
		reason     string
	}
	tests := []struct {
		name          string
		args          args
		wantResultMap map[key]*result
	}{
		{
			name: "success",
			args: args{
				namespace:  "namespace",
				podName:    "pod",
				pluginName: "plugin",
				reason:     "reason",
			},
			wantResultMap: func() map[key]*result {
				d := newData()
				d.preScore = map[string]string{
					"plugin": "reason",
				}
				return map[key]*result{
					"namespace/pod": d,
				}
			}(),
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			s := &Store{mu: &sync.Mutex{}, results: map[key]*result{}}
			s.AddPreScoreResult(tt.args.namespace, tt.args.podName, tt.args.pluginName, tt.args.reason)
			assert.Equal(t, tt.wantResultMap, s.results)
		})
	}
}

func TestStore_AddPermitResult(t *testing.T) {
	t.Parallel()
	type args struct {
		namespace  string
		podName    string
		pluginName string
		status     string
		timeout    time.Duration
	}
	tests := []struct {
		name          string
		wantResultMap map[key]*result
		args          args
	}{
		{
			name: "success",
			args: args{
				namespace:  "namespace",
				podName:    "pod",
				pluginName: "plugin",
				status:     "success",
				timeout:    time.Duration(1), // meaning 1ns
			},
			wantResultMap: func() map[key]*result {
				d := newData()
				d.permit = map[string]string{
					"plugin": "success",
				}
				d.permitTimeout = map[string]string{
					"plugin": "1ns",
				}
				return map[key]*result{
					"namespace/pod": d,
				}
			}(),
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			s := &Store{mu: &sync.Mutex{}, results: map[key]*result{}}
			s.AddPermitResult(tt.args.namespace, tt.args.podName, tt.args.pluginName, tt.args.status, tt.args.timeout)
			assert.Equal(t, tt.wantResultMap, s.results)
		})
	}
}

func TestStore_AddSelectedNode(t *testing.T) {
	t.Parallel()
	type args struct {
		namespace string
		podName   string
		nodeName  string
	}
	tests := []struct {
		name          string
		args          args
		wantResultMap map[key]*result
	}{
		{
			name: "success",
			args: args{
				namespace: "namespace",
				podName:   "pod",
				nodeName:  "node",
			},
			wantResultMap: func() map[key]*result {
				d := newData()
				d.selectedNode = "node"
				return map[key]*result{
					"namespace/pod": d,
				}
			}(),
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			s := &Store{mu: &sync.Mutex{}, results: map[key]*result{}}
			s.AddSelectedNode(tt.args.namespace, tt.args.podName, tt.args.nodeName)
			assert.Equal(t, tt.wantResultMap, s.results)
		})
	}
}

func TestStore_AddReserveResult(t *testing.T) {
	t.Parallel()
	type args struct {
		namespace  string
		podName    string
		pluginName string
		status     string
	}
	tests := []struct {
		name          string
		args          args
		wantResultMap map[key]*result
	}{
		{
			name: "success",
			args: args{
				namespace:  "namespace",
				podName:    "pod",
				pluginName: "plugin",
				status:     "success",
			},
			wantResultMap: func() map[key]*result {
				d := newData()
				d.reserve = map[string]string{
					"plugin": "success",
				}
				return map[key]*result{
					"namespace/pod": d,
				}
			}(),
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			s := &Store{mu: &sync.Mutex{}, results: map[key]*result{}}
			s.AddReserveResult(tt.args.namespace, tt.args.podName, tt.args.pluginName, tt.args.status)
			assert.Equal(t, tt.wantResultMap, s.results)
		})
	}
}

func TestStore_AddBindResult(t *testing.T) {
	t.Parallel()
	type args struct {
		namespace  string
		podName    string
		pluginName string
		status     string
	}
	tests := []struct {
		name          string
		args          args
		wantResultMap map[key]*result
	}{
		{
			name: "success",
			args: args{
				namespace:  "namespace",
				podName:    "pod",
				pluginName: "plugin",
				status:     "success",
			},
			wantResultMap: func() map[key]*result {
				d := newData()
				d.bind = map[string]string{
					"plugin": "success",
				}
				return map[key]*result{
					"namespace/pod": d,
				}
			}(),
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			s := &Store{mu: &sync.Mutex{}, results: map[key]*result{}}
			s.AddBindResult(tt.args.namespace, tt.args.podName, tt.args.pluginName, tt.args.status)
			assert.Equal(t, tt.wantResultMap, s.results)
		})
	}
}

func TestStore_AddPreBindResult(t *testing.T) {
	t.Parallel()
	type args struct {
		namespace  string
		podName    string
		pluginName string
		status     string
	}
	tests := []struct {
		name          string
		args          args
		wantResultMap map[key]*result
	}{
		{
			name: "success",
			args: args{
				namespace:  "namespace",
				podName:    "pod",
				pluginName: "plugin",
				status:     "success",
			},
			wantResultMap: func() map[key]*result {
				d := newData()
				d.prebind = map[string]string{
					"plugin": "success",
				}
				return map[key]*result{
					"namespace/pod": d,
				}
			}(),
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			s := &Store{mu: &sync.Mutex{}, results: map[key]*result{}}
			s.AddPreBindResult(tt.args.namespace, tt.args.podName, tt.args.pluginName, tt.args.status)
			assert.Equal(t, tt.wantResultMap, s.results)
		})
	}
}

func TestStore_DeleteData(t *testing.T) {
	t.Parallel()
	podName := "pod1"
	namespace := "default"
	tests := []struct {
		name       string
		target     corev1.Pod
		result     map[key]*result
		wantResult map[key]*result
	}{
		{
			name: "success to delete the stored data which has the specified key.",
			target: corev1.Pod{
				ObjectMeta: metav1.ObjectMeta{
					Name:      podName,
					Namespace: namespace,
				},
			},
			result: map[key]*result{
				"default/pod1": {
					filter: map[string]map[string]string{
						"node0": {
							"plugin1": PassedFilterMessage,
						},
						"node1": {
							"plugin1": PassedFilterMessage,
						},
					},
					finalScore: map[string]map[string]string{
						"node0": {
							"plugin1": "20",
						},
						"node1": {
							"plugin1": "20",
						},
					},
					score: map[string]map[string]string{
						"node0": {
							"plugin1": "10",
						},
						"node1": {
							"plugin1": "10",
						},
					},
				},
				"default/pod2": {
					filter: map[string]map[string]string{
						"node0": {
							"plugin1": PassedFilterMessage,
						},
						"node1": {
							"plugin1": PassedFilterMessage,
						},
					},
					finalScore: map[string]map[string]string{
						"node0": {
							"plugin1": "20",
						},
						"node1": {
							"plugin1": "20",
						},
					},
					score: map[string]map[string]string{
						"node0": {
							"plugin1": "10",
						},
						"node1": {
							"plugin1": "10",
						},
					},
				},
			},
			wantResult: map[key]*result{
				"default/pod2": {
					filter: map[string]map[string]string{
						"node0": {
							"plugin1": PassedFilterMessage,
						},
						"node1": {
							"plugin1": PassedFilterMessage,
						},
					},
					finalScore: map[string]map[string]string{
						"node0": {
							"plugin1": "20",
						},
						"node1": {
							"plugin1": "20",
						},
					},
					score: map[string]map[string]string{
						"node0": {
							"plugin1": "10",
						},
						"node1": {
							"plugin1": "10",
						},
					},
				},
			},
		},
		{
			name: "do nothing if store doesn't have the data.",
			target: corev1.Pod{
				ObjectMeta: metav1.ObjectMeta{
					Name:      podName,
					Namespace: namespace,
				},
			},
			result: map[key]*result{
				"default/pod2": {
					filter: map[string]map[string]string{
						"node0": {
							"plugin1": PassedFilterMessage,
						},
						"node1": {
							"plugin1": PassedFilterMessage,
						},
					},
					finalScore: map[string]map[string]string{
						"node0": {
							"plugin1": "20",
						},
						"node1": {
							"plugin1": "20",
						},
					},
					score: map[string]map[string]string{
						"node0": {
							"plugin1": "10",
						},
						"node1": {
							"plugin1": "10",
						},
					},
				},
			},
			wantResult: map[key]*result{
				"default/pod2": {
					filter: map[string]map[string]string{
						"node0": {
							"plugin1": PassedFilterMessage,
						},
						"node1": {
							"plugin1": PassedFilterMessage,
						},
					},
					finalScore: map[string]map[string]string{
						"node0": {
							"plugin1": "20",
						},
						"node1": {
							"plugin1": "20",
						},
					},
					score: map[string]map[string]string{
						"node0": {
							"plugin1": "10",
						},
						"node1": {
							"plugin1": "10",
						},
					},
				},
			},
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			s := &Store{
				mu:      new(sync.Mutex),
				results: tt.result,
			}
			s.DeleteData(tt.target)

			assert.Equal(t, tt.wantResult, s.results)
		})
	}
}
</file>

<file path="kube-scheduler-simulator/simulator/scheduler/plugin/resultstore/store.go">
package resultstore

import (
	"encoding/json"
	"strconv"
	"sync"
	"time"

	"golang.org/x/xerrors"
	v1 "k8s.io/api/core/v1"
	"k8s.io/klog/v2"
	"k8s.io/kubernetes/pkg/scheduler/framework"

	"sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler/plugin/annotation"
)

// Store has results of scheduling.
// It manages all scheduling results and reflects all results on the pod annotation when the scheduling is finished.
type Store struct {
	mu *sync.Mutex

	results           map[key]*result
	scorePluginWeight map[string]int32
}

const (
	// PassedFilterMessage is used when a node pass the filter plugin.
	PassedFilterMessage = "passed"
	// SuccessMessage is used when no error is retured from the plugin.
	SuccessMessage = "success"
	// WaitMessage is used when wait status is retured from the plugin.
	WaitMessage = "wait"
	// PostFilterNominatedMessage is used when a postFilter plugin returns success.
	PostFilterNominatedMessage = "preemption victim"
)

// result has a scheduling result of pod.
type result struct {
	// selectedNode is the scheduling result. It'll be filled when the Pod go through Reserve phase.
	selectedNode string

	// plugin name → pre score(string)
	// If success, SuccessMessage is shown.
	// If non success, status.Message() is shown.
	preScore map[string]string

	// node name → plugin name → score(string)
	score map[string]map[string]string

	// node name → plugin name → finalScore(string)
	// This score is normalized and applied weight for each plugins.
	finalScore map[string]map[string]string

	// plugin name → pre filter status.
	// If success, SuccessMessage is shown.
	// If non success, status.Message() is shown.
	preFilterStatus map[string]string

	// plugin name → pre filter result(framework.PreFilterResult)
	// NodeNames in framework.PreFilterResult is shown.
	preFilterResult map[string][]string

	// node name → plugin name → filtering result
	// When node pass the filter, filtering result will be PassedFilterMessage.
	// When node blocked by the filter, filtering result is blocked reason.
	filter map[string]map[string]string

	// node name → plugin name → post filtering result
	postFilter map[string]map[string]string

	// plugin name → permit result (framework.Status)
	permit map[string]string

	// plugin name → permit timeout(string)
	permitTimeout map[string]string

	// plugin name → reserve result(string)
	reserve map[string]string

	// plugin name → prebind result(string)
	prebind map[string]string

	// plugin name → bind result(string)
	bind map[string]string

	// customResults has the user defined custom results.
	// annotation key -> result(string)
	customResults map[string]string
}

func New(scorePluginWeight map[string]int32) *Store {
	s := &Store{
		mu:                new(sync.Mutex),
		results:           map[key]*result{},
		scorePluginWeight: scorePluginWeight,
	}

	return s
}

// key is the key of result map on Store.
// key is created from namespace and podName.
type key string

// newKey creates key with namespace and podName.
func newKey(namespace, podName string) key {
	k := namespace + "/" + podName
	return key(k)
}

func newData() *result {
	d := &result{
		score:           map[string]map[string]string{},
		finalScore:      map[string]map[string]string{},
		preFilterResult: map[string][]string{},
		preFilterStatus: map[string]string{},
		preScore:        map[string]string{},
		filter:          map[string]map[string]string{},
		postFilter:      map[string]map[string]string{},
		permit:          map[string]string{},
		permitTimeout:   map[string]string{},
		reserve:         map[string]string{},
		bind:            map[string]string{},
		prebind:         map[string]string{},
		customResults:   map[string]string{},
	}
	return d
}

// GetStoredResult get all stored result of a given Pod.
//
//nolint:cyclop,funlen
func (s *Store) GetStoredResult(pod *v1.Pod) map[string]string {
	s.mu.Lock()
	defer s.mu.Unlock()

	k := newKey(pod.Namespace, pod.Name)
	if _, ok := s.results[k]; !ok {
		// Store doesn't have scheduling result of pod.
		return nil
	}

	annotation := map[string]string{}
	if err := s.addPreFilterResultToMap(annotation, k); err != nil {
		klog.Errorf("failed to add prefilter result to pod: %+v", err)
		return nil
	}

	if err := s.addFilterResultToMap(annotation, k); err != nil {
		klog.Errorf("failed to add filtering result to pod: %+v", err)
		return nil
	}

	if err := s.addPostFilterResultToMap(annotation, k); err != nil {
		klog.Errorf("failed to add post filtering result to pod: %+v", err)
		return nil
	}

	if err := s.addPreScoreResultToMap(annotation, k); err != nil {
		klog.Errorf("failed to add prescore result to pod: %+v", err)
		return nil
	}

	if err := s.addScoreResultToMap(annotation, k); err != nil {
		klog.Errorf("failed to add scoring result to pod: %+v", err)
		return nil
	}

	if err := s.addFinalScoreResultToMap(annotation, k); err != nil {
		klog.Errorf("failed to add final score result to pod: %+v", err)
		return nil
	}

	if err := s.addReserveResultToMap(annotation, k); err != nil {
		klog.Errorf("failed to add reserve result to pod: %+v", err)
		return nil
	}

	if err := s.addPermitResultToMap(annotation, k); err != nil {
		klog.Errorf("failed to add permit result to pod: %+v", err)
		return nil
	}

	if err := s.addPreBindResultToMap(annotation, k); err != nil {
		klog.Errorf("failed to add prebind result to pod: %+v", err)
		return nil
	}

	if err := s.addBindResultToMap(annotation, k); err != nil {
		klog.Errorf("failed to add bind result to pod: %+v", err)
		return nil
	}

	s.addCustomResultsToMap(annotation, k)
	s.addSelectedNodeToPod(annotation, k)

	return annotation
}

func (s *Store) addPreFilterResultToMap(anno map[string]string, k key) error {
	_, ok := anno[annotation.PreFilterResultAnnotationKey]
	if !ok {
		if s.results[k].preFilterResult == nil {
			s.results[k].preFilterResult = map[string][]string{}
		}
		r, err := json.Marshal(s.results[k].preFilterResult)
		if err != nil {
			return xerrors.Errorf("encode json to record preFilter result: %w", err)
		}

		anno[annotation.PreFilterResultAnnotationKey] = string(r)
	}

	_, ok2 := anno[annotation.PreFilterStatusResultAnnotationKey]
	if ok2 {
		return nil
	}

	if s.results[k].preFilterStatus == nil {
		s.results[k].preFilterStatus = map[string]string{}
	}
	sta, err := json.Marshal(s.results[k].preFilterStatus)
	if err != nil {
		return xerrors.Errorf("encode json to record preFilter status: %w", err)
	}

	anno[annotation.PreFilterStatusResultAnnotationKey] = string(sta)

	return nil
}

func (s *Store) addPreScoreResultToMap(anno map[string]string, k key) error {
	_, ok := anno[annotation.PreScoreResultAnnotationKey]
	if ok {
		return nil
	}

	if s.results[k].preScore == nil {
		s.results[k].preScore = map[string]string{}
	}
	status, err := json.Marshal(s.results[k].preScore)
	if err != nil {
		return xerrors.Errorf("encode json to record preScore status: %w", err)
	}

	anno[annotation.PreScoreResultAnnotationKey] = string(status)
	return nil
}

func (s *Store) addBindResultToMap(anno map[string]string, k key) error {
	_, ok := anno[annotation.BindResultAnnotationKey]
	if ok {
		return nil
	}

	if s.results[k].bind == nil {
		s.results[k].bind = map[string]string{}
	}
	status, err := json.Marshal(s.results[k].bind)
	if err != nil {
		return xerrors.Errorf("encode json to record bind status: %w", err)
	}

	anno[annotation.BindResultAnnotationKey] = string(status)
	return nil
}

func (s *Store) addPreBindResultToMap(anno map[string]string, k key) error {
	_, ok := anno[annotation.PreBindResultAnnotationKey]
	if ok {
		return nil
	}

	if s.results[k].prebind == nil {
		s.results[k].prebind = map[string]string{}
	}
	status, err := json.Marshal(s.results[k].prebind)
	if err != nil {
		return xerrors.Errorf("encode json to record preBind status: %w", err)
	}

	anno[annotation.PreBindResultAnnotationKey] = string(status)
	return nil
}

func (s *Store) addSelectedNodeToPod(anno map[string]string, k key) {
	_, ok := anno[annotation.SelectedNodeAnnotationKey]
	if ok {
		return
	}

	anno[annotation.SelectedNodeAnnotationKey] = s.results[k].selectedNode
}

func (s *Store) addReserveResultToMap(anno map[string]string, k key) error {
	_, ok := anno[annotation.ReserveResultAnnotationKey]
	if ok {
		return nil
	}

	if s.results[k].reserve == nil {
		s.results[k].reserve = map[string]string{}
	}
	status, err := json.Marshal(s.results[k].reserve)
	if err != nil {
		return xerrors.Errorf("encode json to record reserve status: %w", err)
	}
	anno[annotation.ReserveResultAnnotationKey] = string(status)
	return nil
}

func (s *Store) addPermitResultToMap(anno map[string]string, k key) error {
	_, ok := anno[annotation.PermitTimeoutResultAnnotationKey]
	if !ok {
		if s.results[k].permitTimeout == nil {
			s.results[k].permitTimeout = map[string]string{}
		}
		timeout, err := json.Marshal(s.results[k].permitTimeout)
		if err != nil {
			return xerrors.Errorf("encode json to record permit timeout: %w", err)
		}
		anno[annotation.PermitTimeoutResultAnnotationKey] = string(timeout)
	}

	_, ok = anno[annotation.PermitStatusResultAnnotationKey]
	if ok {
		return nil
	}

	if s.results[k].permit == nil {
		s.results[k].permit = map[string]string{}
	}
	status, err := json.Marshal(s.results[k].permit)
	if err != nil {
		return xerrors.Errorf("encode json to record permit status: %w", err)
	}
	anno[annotation.PermitStatusResultAnnotationKey] = string(status)
	return nil
}

func (s *Store) addFilterResultToMap(anno map[string]string, k key) error {
	_, ok := anno[annotation.FilterResultAnnotationKey]
	if ok {
		return nil
	}

	if s.results[k].filter == nil {
		s.results[k].filter = map[string]map[string]string{}
	}
	status, err := json.Marshal(s.results[k].filter)
	if err != nil {
		return xerrors.Errorf("encode json to record filter status: %w", err)
	}

	anno[annotation.FilterResultAnnotationKey] = string(status)
	return nil
}

func (s *Store) addPostFilterResultToMap(anno map[string]string, k key) error {
	_, ok := anno[annotation.PostFilterResultAnnotationKey]
	if ok {
		return nil
	}

	if s.results[k].postFilter == nil {
		s.results[k].postFilter = map[string]map[string]string{}
	}
	result, err := json.Marshal(s.results[k].postFilter)
	if err != nil {
		return xerrors.Errorf("encode json to record post filter results: %w", err)
	}
	anno[annotation.PostFilterResultAnnotationKey] = string(result)
	return nil
}

func (s *Store) addScoreResultToMap(anno map[string]string, k key) error {
	_, ok := anno[annotation.ScoreResultAnnotationKey]
	if ok {
		return nil
	}

	if s.results[k].score == nil {
		s.results[k].score = map[string]map[string]string{}
	}
	scores, err := json.Marshal(s.results[k].score)
	if err != nil {
		return xerrors.Errorf("encode json to record scores: %w", err)
	}

	anno[annotation.ScoreResultAnnotationKey] = string(scores)
	return nil
}

func (s *Store) addFinalScoreResultToMap(anno map[string]string, k key) error {
	_, ok := anno[annotation.FinalScoreResultAnnotationKey]
	if ok {
		return nil
	}

	if s.results[k].finalScore == nil {
		s.results[k].finalScore = map[string]map[string]string{}
	}
	scores, err := json.Marshal(s.results[k].finalScore)
	if err != nil {
		return xerrors.Errorf("encode json to record scores: %w", err)
	}

	anno[annotation.FinalScoreResultAnnotationKey] = string(scores)
	return nil
}

func (s *Store) addCustomResultsToMap(anno map[string]string, k key) {
	for annokey, r := range s.results[k].customResults {
		_, ok := anno[annokey]
		if ok {
			continue
		}
		anno[annokey] = r
	}
}

// AddFilterResult adds filtering result to pod annotation.
func (s *Store) AddFilterResult(namespace, podName, nodeName, pluginName, reason string) {
	s.mu.Lock()
	defer s.mu.Unlock()

	k := newKey(namespace, podName)
	if _, ok := s.results[k]; !ok {
		s.results[k] = newData()
	}

	if _, ok := s.results[k].filter[nodeName]; !ok {
		s.results[k].filter[nodeName] = map[string]string{}
	}

	s.results[k].filter[nodeName][pluginName] = reason
}

// AddPostFilterResult adds post filter result to the pod annotaiton.
//   - nominatedNodeName represents the node name which nominated by the postFilter plugin.
//     Otherwise, the string "" would be stored in this arg.
func (s *Store) AddPostFilterResult(namespace, podName, nominatedNodeName, pluginName string, nodeNames []string) {
	s.mu.Lock()
	defer s.mu.Unlock()

	k := newKey(namespace, podName)
	if _, ok := s.results[k]; !ok {
		s.results[k] = newData()
	}
	for _, nodeName := range nodeNames {
		if _, ok := s.results[k].postFilter[nodeName]; !ok {
			s.results[k].postFilter[nodeName] = map[string]string{}
		}
		if nodeName == nominatedNodeName {
			s.results[k].postFilter[nodeName][pluginName] = PostFilterNominatedMessage
		}
	}
}

// AddScoreResult adds scoring result to pod annotation.
func (s *Store) AddScoreResult(namespace, podName, nodeName, pluginName string, score int64) {
	s.mu.Lock()
	defer s.mu.Unlock()

	k := newKey(namespace, podName)
	if _, ok := s.results[k]; !ok {
		s.results[k] = newData()
	}

	if _, ok := s.results[k].score[nodeName]; !ok {
		s.results[k].score[nodeName] = map[string]string{}
	}

	s.results[k].score[nodeName][pluginName] = strconv.FormatInt(score, 10)

	// we already locked on first of this func
	s.addNormalizedScoreResultWithoutLock(namespace, podName, nodeName, pluginName, score)
}

// AddNormalizedScoreResult adds final score result to pod annotation.
func (s *Store) AddNormalizedScoreResult(namespace, podName, nodeName, pluginName string, normalizedscore int64) {
	s.mu.Lock()
	defer s.mu.Unlock()

	s.addNormalizedScoreResultWithoutLock(namespace, podName, nodeName, pluginName, normalizedscore)
}

func (s *Store) addNormalizedScoreResultWithoutLock(namespace, podName, nodeName, pluginName string, normalizedscore int64) {
	k := newKey(namespace, podName)
	if _, ok := s.results[k]; !ok {
		s.results[k] = newData()
	}

	if _, ok := s.results[k].finalScore[nodeName]; !ok {
		s.results[k].finalScore[nodeName] = map[string]string{}
	}

	finalscore := s.applyWeightOnScore(pluginName, normalizedscore)

	// apply weight to calculate final score.
	s.results[k].finalScore[nodeName][pluginName] = strconv.FormatInt(finalscore, 10)
}

func (s *Store) applyWeightOnScore(pluginName string, score int64) int64 {
	weight := s.scorePluginWeight[pluginName]
	return score * int64(weight)
}

// DeleteData deletes the data corresponding to the specified Pod.
func (s *Store) DeleteData(pod v1.Pod) {
	s.mu.Lock()
	defer s.mu.Unlock()
	s.deleteData(newKey(pod.Namespace, pod.Name))
}

// deleteData deletes the result stored with the given key.
// Note: we assume the store lock is already acquired.
func (s *Store) deleteData(k key) {
	delete(s.results, k)
}

func (s *Store) AddPreFilterResult(namespace, podName, pluginName, reason string, preFilterResult *framework.PreFilterResult) {
	s.mu.Lock()
	defer s.mu.Unlock()

	k := newKey(namespace, podName)
	if _, ok := s.results[k]; !ok {
		s.results[k] = newData()
	}

	s.results[k].preFilterStatus[pluginName] = reason
	if preFilterResult != nil {
		s.results[k].preFilterResult[pluginName] = preFilterResult.NodeNames.UnsortedList()
	}
}

func (s *Store) AddPreScoreResult(namespace, podName, pluginName, reason string) {
	s.mu.Lock()
	defer s.mu.Unlock()

	k := newKey(namespace, podName)
	if _, ok := s.results[k]; !ok {
		s.results[k] = newData()
	}

	s.results[k].preScore[pluginName] = reason
}

func (s *Store) AddPermitResult(namespace, podName, pluginName, status string, timeout time.Duration) {
	s.mu.Lock()
	defer s.mu.Unlock()

	k := newKey(namespace, podName)
	if _, ok := s.results[k]; !ok {
		s.results[k] = newData()
	}

	s.results[k].permit[pluginName] = status
	s.results[k].permitTimeout[pluginName] = timeout.String()
}

func (s *Store) AddSelectedNode(namespace, podName, nodeName string) {
	s.mu.Lock()
	defer s.mu.Unlock()

	k := newKey(namespace, podName)
	if _, ok := s.results[k]; !ok {
		s.results[k] = newData()
	}

	s.results[k].selectedNode = nodeName
}

func (s *Store) AddReserveResult(namespace, podName, pluginName, status string) {
	s.mu.Lock()
	defer s.mu.Unlock()

	k := newKey(namespace, podName)
	if _, ok := s.results[k]; !ok {
		s.results[k] = newData()
	}

	s.results[k].reserve[pluginName] = status
}

func (s *Store) AddBindResult(namespace, podName, pluginName, status string) {
	s.mu.Lock()
	defer s.mu.Unlock()

	k := newKey(namespace, podName)
	if _, ok := s.results[k]; !ok {
		s.results[k] = newData()
	}

	s.results[k].bind[pluginName] = status
}

func (s *Store) AddPreBindResult(namespace, podName, pluginName, status string) {
	s.mu.Lock()
	defer s.mu.Unlock()

	k := newKey(namespace, podName)
	if _, ok := s.results[k]; !ok {
		s.results[k] = newData()
	}

	s.results[k].prebind[pluginName] = status
}

// AddCustomResult adds user defined data.
// The results added through this func is reflected on the Pod's annotation eventually like other scheduling results.
// This function is intended to be called from the plugin.PluginExtender; allow users to export some internal state on Pods for debugging purpose.
// For example,
// Calling AddCustomResult in NodeAffinity's PreFilterPluginExtender:
// AddCustomResult("namespace", "incomingPod", "node-affinity-filter-internal-state-anno-key", "internal-state")
// Then, "incomingPod" Pod will get {"node-affinity-filter-internal-state-anno-key": "internal-state"} annotation after scheduling.
func (s *Store) AddCustomResult(namespace, podName, annotationKey, result string) {
	s.mu.Lock()
	defer s.mu.Unlock()

	k := newKey(namespace, podName)
	if _, ok := s.results[k]; !ok {
		s.results[k] = newData()
	}
	s.results[k].customResults[annotationKey] = result
}
</file>

<file path="kube-scheduler-simulator/simulator/scheduler/plugin/plugins_test.go">
package plugin

import (
	"encoding/json"
	"sort"
	"testing"

	"github.com/google/go-cmp/cmp"
	"github.com/stretchr/testify/assert"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	configv1 "k8s.io/kube-scheduler/config/v1"
	schedulerConfig "k8s.io/kubernetes/pkg/scheduler/apis/config"
)

func TestConvertForSimulator(t *testing.T) {
	t.Parallel()
	var weight1 int32 = 1
	var weight2 int32 = 2
	var weight3 int32 = 3

	tests := []struct {
		name    string
		arg     *configv1.Plugins
		want    *configv1.Plugins
		wantErr bool
	}{
		{
			name: "success",
			arg: &configv1.Plugins{
				PreFilter: configv1.PluginSet{
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				PreScore: configv1.PluginSet{
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				Reserve: configv1.PluginSet{
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				Permit: configv1.PluginSet{
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				PreBind: configv1.PluginSet{
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				Bind: configv1.PluginSet{
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				PostBind: configv1.PluginSet{
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				Filter: configv1.PluginSet{
					Enabled: []configv1.Plugin{
						{Name: "NodeUnschedulable"},
						{Name: "NodeName"},
					},
				},
				PostFilter: configv1.PluginSet{
					Enabled: []configv1.Plugin{
						{Name: "DefaultPreemption"},
					},
				},
				Score: configv1.PluginSet{
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				MultiPoint: configv1.PluginSet{
					Disabled: []configv1.Plugin{
						{
							Name: "EBSLimits",
						},
					},
				},
			},
			want: &configv1.Plugins{
				PreFilter: configv1.PluginSet{
					Enabled: []configv1.Plugin{},
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				PreScore: configv1.PluginSet{
					Enabled: []configv1.Plugin{},
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				Reserve: configv1.PluginSet{
					Enabled: []configv1.Plugin{},
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				Permit: configv1.PluginSet{
					Enabled: []configv1.Plugin{},
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				PreBind: configv1.PluginSet{
					Enabled: []configv1.Plugin{},
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				Bind: configv1.PluginSet{
					Enabled: []configv1.Plugin{},
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				PostBind: configv1.PluginSet{
					Enabled: []configv1.Plugin{},
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				Filter: configv1.PluginSet{
					Enabled: []configv1.Plugin{
						{Name: "NodeUnschedulableWrapped"},
						{Name: "NodeNameWrapped"},
					},
					Disabled: []configv1.Plugin{},
				},
				PostFilter: configv1.PluginSet{
					Enabled: []configv1.Plugin{
						{Name: "DefaultPreemptionWrapped"},
					},
					Disabled: []configv1.Plugin{},
				},
				Score: configv1.PluginSet{
					Enabled: []configv1.Plugin{},
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				MultiPoint: configv1.PluginSet{
					Enabled: []configv1.Plugin{
						{Name: "SchedulingGatesWrapped"},
						{Name: "PrioritySortWrapped"},
						{Name: "NodeUnschedulableWrapped"},
						{Name: "NodeNameWrapped"},
						{Name: "TaintTolerationWrapped", Weight: &weight3},
						{Name: "NodeAffinityWrapped", Weight: &weight2},
						{Name: "NodePortsWrapped"},
						{Name: "NodeResourcesFitWrapped", Weight: &weight1},
						{Name: "VolumeRestrictionsWrapped"},
						{Name: "GCEPDLimitsWrapped"},
						{Name: "NodeVolumeLimitsWrapped"},
						{Name: "AzureDiskLimitsWrapped"},
						{Name: "VolumeBindingWrapped"},
						{Name: "VolumeZoneWrapped"},
						{Name: "PodTopologySpreadWrapped", Weight: &weight2},
						{Name: "InterPodAffinityWrapped", Weight: &weight2},
						{Name: "DefaultPreemptionWrapped"},
						{Name: "NodeResourcesBalancedAllocationWrapped", Weight: &weight1},
						{Name: "ImageLocalityWrapped", Weight: &weight1},
						{Name: "DefaultBinderWrapped"},
					},
					Disabled: []configv1.Plugin{
						{Name: "*"},
					},
				},
			},
			wantErr: false,
		},
		{
			name: "success when user disable all plugins with '*'",
			arg: &configv1.Plugins{
				PreFilter: configv1.PluginSet{
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				PreScore: configv1.PluginSet{
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				Reserve: configv1.PluginSet{
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				Permit: configv1.PluginSet{
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				PreBind: configv1.PluginSet{
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				Bind: configv1.PluginSet{
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				PostBind: configv1.PluginSet{
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				Filter: configv1.PluginSet{
					Disabled: []configv1.Plugin{
						{Name: "*"},
					},
				},
				PostFilter: configv1.PluginSet{
					Disabled: []configv1.Plugin{
						{Name: "*"},
					},
				},
				Score: configv1.PluginSet{
					Disabled: []configv1.Plugin{
						{Name: "*"},
					},
				},
				MultiPoint: configv1.PluginSet{
					Disabled: []configv1.Plugin{
						{Name: "*"},
					},
				},
			},
			want: &configv1.Plugins{
				PreFilter: configv1.PluginSet{
					Enabled: []configv1.Plugin{},
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				PreScore: configv1.PluginSet{
					Enabled: []configv1.Plugin{},
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				Reserve: configv1.PluginSet{
					Enabled: []configv1.Plugin{},
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				Permit: configv1.PluginSet{
					Enabled: []configv1.Plugin{},
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				PreBind: configv1.PluginSet{
					Enabled: []configv1.Plugin{},
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				Bind: configv1.PluginSet{
					Enabled: []configv1.Plugin{},
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				PostBind: configv1.PluginSet{
					Enabled: []configv1.Plugin{},
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				Filter: configv1.PluginSet{
					Enabled: []configv1.Plugin{},
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				PostFilter: configv1.PluginSet{
					Enabled: []configv1.Plugin{},
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				Score: configv1.PluginSet{
					Enabled: []configv1.Plugin{},
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				MultiPoint: configv1.PluginSet{
					Enabled: []configv1.Plugin{},
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
			},
			wantErr: false,
		},
		{
			name: "success with non in-tree plugins",
			arg: &configv1.Plugins{
				PreFilter: configv1.PluginSet{
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				PreScore: configv1.PluginSet{
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				Reserve: configv1.PluginSet{
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				Permit: configv1.PluginSet{
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				PreBind: configv1.PluginSet{
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				Bind: configv1.PluginSet{
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				PostBind: configv1.PluginSet{
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				Filter: configv1.PluginSet{
					Enabled: []configv1.Plugin{
						{Name: "CustomPlugin1"},
					},
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				PostFilter: configv1.PluginSet{
					Enabled: []configv1.Plugin{
						{Name: "CustomPlugin1"},
					},
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				Score: configv1.PluginSet{
					Enabled: []configv1.Plugin{
						{Name: "CustomPlugin1"},
					},
					Disabled: []configv1.Plugin{
						{Name: "*"},
					},
				},
				MultiPoint: configv1.PluginSet{
					Enabled: []configv1.Plugin{},
					Disabled: []configv1.Plugin{
						{Name: "*"},
					},
				},
			},
			want: &configv1.Plugins{
				PreFilter: configv1.PluginSet{
					Enabled: []configv1.Plugin{},
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				PreScore: configv1.PluginSet{
					Enabled: []configv1.Plugin{},
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				Reserve: configv1.PluginSet{
					Enabled: []configv1.Plugin{},
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				Permit: configv1.PluginSet{
					Enabled: []configv1.Plugin{},
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				PreBind: configv1.PluginSet{
					Enabled: []configv1.Plugin{},
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				Bind: configv1.PluginSet{
					Enabled: []configv1.Plugin{},
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				PostBind: configv1.PluginSet{
					Enabled: []configv1.Plugin{},
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				Filter: configv1.PluginSet{
					Enabled: []configv1.Plugin{
						{Name: "CustomPlugin1Wrapped"},
					},
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				PostFilter: configv1.PluginSet{
					Enabled: []configv1.Plugin{
						{Name: "CustomPlugin1Wrapped"},
					},
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				Score: configv1.PluginSet{
					Enabled: []configv1.Plugin{
						{Name: "CustomPlugin1Wrapped"},
					},
					Disabled: []configv1.Plugin{
						{
							Name: "*",
						},
					},
				},
				MultiPoint: configv1.PluginSet{
					Enabled: []configv1.Plugin{},
					Disabled: []configv1.Plugin{
						{Name: "*"},
					},
				},
			},
			wantErr: false,
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			got, err := ConvertForSimulator(tt.arg)
			if (err != nil) != tt.wantErr {
				t.Errorf("ConvertWrapped() error = %v, wantErr %v", err, tt.wantErr)
				return
			}
			assert.Equal(t, tt.want, got)
		})
	}
}

//nolint:gocognit // it is because of huge test cases.
func Test_NewPluginConfig(t *testing.T) {
	t.Parallel()

	var minCandidateNodesPercentage int32 = 20
	var minCandidateNodesAbsolute int32 = 100
	var hardPodAffinityWeight int32 = 2

	tests := []struct {
		name    string
		pc      []configv1.PluginConfig
		want    []configv1.PluginConfig
		wantErr bool
	}{
		{
			name:    "success with empty arg",
			pc:      nil,
			want:    defaultPluginConfig(),
			wantErr: false,
		},
		{
			name: "success with plugin config of postFilter",
			pc: []configv1.PluginConfig{
				{
					Name: "DefaultPreemption",
					Args: runtime.RawExtension{
						Object: &configv1.DefaultPreemptionArgs{
							TypeMeta: metav1.TypeMeta{
								Kind:       "DefaultPreemptionArgs",
								APIVersion: "kubescheduler.config.k8s.io/v1",
							},
							MinCandidateNodesPercentage: &minCandidateNodesPercentage,
							MinCandidateNodesAbsolute:   &minCandidateNodesAbsolute,
						},
					},
				},
			},
			want: func() []configv1.PluginConfig {
				pc := defaultPluginConfig()
				for i := range pc {
					if pc[i].Name == "DefaultPreemption" {
						pc[i] = configv1.PluginConfig{
							Name: "DefaultPreemption",
							Args: runtime.RawExtension{
								Object: &configv1.DefaultPreemptionArgs{
									TypeMeta: metav1.TypeMeta{
										Kind:       "DefaultPreemptionArgs",
										APIVersion: "kubescheduler.config.k8s.io/v1",
									},
									MinCandidateNodesPercentage: &minCandidateNodesPercentage,
									MinCandidateNodesAbsolute:   &minCandidateNodesAbsolute,
								},
							},
						}
					}
					if pc[i].Name == "DefaultPreemptionWrapped" {
						pc[i] = configv1.PluginConfig{
							Name: "DefaultPreemptionWrapped",
							Args: runtime.RawExtension{
								Object: &configv1.DefaultPreemptionArgs{
									TypeMeta: metav1.TypeMeta{
										Kind:       "DefaultPreemptionArgs",
										APIVersion: "kubescheduler.config.k8s.io/v1",
									},
									MinCandidateNodesPercentage: &minCandidateNodesPercentage,
									MinCandidateNodesAbsolute:   &minCandidateNodesAbsolute,
								},
							},
						}
					}
				}

				return pc
			}(),
			wantErr: false,
		},
		{
			name: "success with plugin config on Args.Object",
			pc: []configv1.PluginConfig{
				{
					Name: "InterPodAffinity",
					Args: runtime.RawExtension{
						Object: &configv1.InterPodAffinityArgs{
							TypeMeta: metav1.TypeMeta{
								Kind:       "InterPodAffinityArgs",
								APIVersion: "kubescheduler.config.k8s.io/v1",
							},
							HardPodAffinityWeight: &hardPodAffinityWeight,
						},
					},
				},
			},
			want: func() []configv1.PluginConfig {
				pc := defaultPluginConfig()
				var defaultMinCandidateNodesPercentage int32 = 10
				for i := range pc {
					if pc[i].Name == "InterPodAffinity" {
						pc[i] = configv1.PluginConfig{
							Name: "InterPodAffinity",
							Args: runtime.RawExtension{
								Object: &configv1.InterPodAffinityArgs{
									TypeMeta: metav1.TypeMeta{
										Kind:       "InterPodAffinityArgs",
										APIVersion: "kubescheduler.config.k8s.io/v1",
									},
									HardPodAffinityWeight: &hardPodAffinityWeight,
								},
							},
						}
					}
					if pc[i].Name == "InterPodAffinityWrapped" {
						pc[i] = configv1.PluginConfig{
							Name: "InterPodAffinityWrapped",
							Args: runtime.RawExtension{
								Object: &configv1.InterPodAffinityArgs{
									TypeMeta: metav1.TypeMeta{
										Kind:       "InterPodAffinityArgs",
										APIVersion: "kubescheduler.config.k8s.io/v1",
									},
									HardPodAffinityWeight: &hardPodAffinityWeight,
								},
							},
						}
					}
					if pc[i].Name == "DefaultPreemption" {
						pc[i] = configv1.PluginConfig{
							Name: "DefaultPreemption",
							Args: runtime.RawExtension{
								Object: &configv1.DefaultPreemptionArgs{
									TypeMeta: metav1.TypeMeta{
										Kind:       "DefaultPreemptionArgs",
										APIVersion: "kubescheduler.config.k8s.io/v1",
									},
									MinCandidateNodesPercentage: &defaultMinCandidateNodesPercentage,
									MinCandidateNodesAbsolute:   &minCandidateNodesAbsolute,
								},
							},
						}
					}
					if pc[i].Name == "DefaultPreemptionWrapped" {
						pc[i] = configv1.PluginConfig{
							Name: "DefaultPreemptionWrapped",
							Args: runtime.RawExtension{
								Object: &configv1.DefaultPreemptionArgs{
									TypeMeta: metav1.TypeMeta{
										Kind:       "DefaultPreemptionArgs",
										APIVersion: "kubescheduler.config.k8s.io/v1",
									},
									MinCandidateNodesPercentage: &defaultMinCandidateNodesPercentage,
									MinCandidateNodesAbsolute:   &minCandidateNodesAbsolute,
								},
							},
						}
					}
				}

				return pc
			}(),
			wantErr: false,
		},
		{
			name: "Success: if data exists in both PluginConfig.Args.Raw and PluginConfig.Args.Object," +
				"PluginConfig.Args.Raw would be ignored",
			pc: []configv1.PluginConfig{
				{
					Name: "InterPodAffinity",
					Args: runtime.RawExtension{
						Object: &configv1.InterPodAffinityArgs{
							TypeMeta: metav1.TypeMeta{
								Kind:       "InterPodAffinityArgs",
								APIVersion: "kubescheduler.config.k8s.io/v1",
							},
							HardPodAffinityWeight: &hardPodAffinityWeight,
						},
						Raw: func() []byte {
							anotherHardPodAffinityWeight := hardPodAffinityWeight + 1
							cfg := configv1.InterPodAffinityArgs{
								TypeMeta: metav1.TypeMeta{
									Kind:       "InterPodAffinityArgs",
									APIVersion: "kubescheduler.config.k8s.io/v1",
								},
								HardPodAffinityWeight: &anotherHardPodAffinityWeight,
							}
							d, _ := json.Marshal(cfg)
							return d
						}(),
					},
				},
			},
			want: func() []configv1.PluginConfig {
				pc := defaultPluginConfig()
				for i := range pc {
					if pc[i].Name == "InterPodAffinity" {
						pc[i] = configv1.PluginConfig{
							Name: "InterPodAffinity",
							Args: runtime.RawExtension{
								Object: &configv1.InterPodAffinityArgs{
									TypeMeta: metav1.TypeMeta{
										Kind:       "InterPodAffinityArgs",
										APIVersion: "kubescheduler.config.k8s.io/v1",
									},
									HardPodAffinityWeight: &hardPodAffinityWeight,
								},
							},
						}
					}
					if pc[i].Name == "InterPodAffinityWrapped" {
						pc[i] = configv1.PluginConfig{
							Name: "InterPodAffinityWrapped",
							Args: runtime.RawExtension{
								Object: &configv1.InterPodAffinityArgs{
									TypeMeta: metav1.TypeMeta{
										Kind:       "InterPodAffinityArgs",
										APIVersion: "kubescheduler.config.k8s.io/v1",
									},
									HardPodAffinityWeight: &hardPodAffinityWeight,
								},
							},
						}
					}
				}

				return pc
			}(),
			wantErr: false,
		},
		{
			name: "success with plugin config on Args.Raw ",
			pc: []configv1.PluginConfig{
				{
					Name: "InterPodAffinity",
					Args: runtime.RawExtension{
						Raw: func() []byte {
							cfg := configv1.InterPodAffinityArgs{
								TypeMeta: metav1.TypeMeta{
									Kind:       "InterPodAffinityArgs",
									APIVersion: "kubescheduler.config.k8s.io/v1",
								},
								HardPodAffinityWeight: &hardPodAffinityWeight,
							}
							d, _ := json.Marshal(cfg)
							return d
						}(),
					},
				},
			},
			want: func() []configv1.PluginConfig {
				pc := defaultPluginConfig()
				for i := range pc {
					if pc[i].Name == "InterPodAffinity" {
						pc[i] = configv1.PluginConfig{
							Name: "InterPodAffinity",
							Args: runtime.RawExtension{
								Object: &configv1.InterPodAffinityArgs{
									TypeMeta: metav1.TypeMeta{
										Kind:       "InterPodAffinityArgs",
										APIVersion: "kubescheduler.config.k8s.io/v1",
									},
									HardPodAffinityWeight: &hardPodAffinityWeight,
								},
							},
						}
					}
					if pc[i].Name == "InterPodAffinityWrapped" {
						pc[i] = configv1.PluginConfig{
							Name: "InterPodAffinityWrapped",
							Args: runtime.RawExtension{
								Object: &configv1.InterPodAffinityArgs{
									TypeMeta: metav1.TypeMeta{
										Kind:       "InterPodAffinityArgs",
										APIVersion: "kubescheduler.config.k8s.io/v1",
									},
									HardPodAffinityWeight: &hardPodAffinityWeight,
								},
							},
						}
					}
				}

				return pc
			}(),
			wantErr: false,
		},
	}

	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			got, err := NewPluginConfig(tt.pc)
			if (err != nil) != tt.wantErr {
				t.Errorf("NewPluginConfig() error = %v, wantErr %v", err, tt.wantErr)
				return
			}
			sort.SliceStable(got, func(i, j int) bool { return got[i].Name < got[j].Name })
			sort.SliceStable(tt.want, func(i, j int) bool { return tt.want[i].Name < tt.want[j].Name })

			assert.Equal(t, tt.want, got)
		})
	}
}

func defaultPluginConfig() []configv1.PluginConfig {
	var minCandidateNodesPercentage int32 = 10
	var minCandidateNodesAbsolute int32 = 100
	var hardPodAffinityWeight int32 = 1
	var bindTimeoutSeconds int64 = 600

	return []configv1.PluginConfig{
		{
			Name: "DefaultPreemption",
			Args: runtime.RawExtension{
				Object: &configv1.DefaultPreemptionArgs{
					TypeMeta: metav1.TypeMeta{
						Kind:       "DefaultPreemptionArgs",
						APIVersion: "kubescheduler.config.k8s.io/v1",
					},
					MinCandidateNodesPercentage: &minCandidateNodesPercentage,
					MinCandidateNodesAbsolute:   &minCandidateNodesAbsolute,
				},
			},
		},
		{
			Name: "InterPodAffinity",
			Args: runtime.RawExtension{
				Object: &configv1.InterPodAffinityArgs{
					TypeMeta: metav1.TypeMeta{
						Kind:       "InterPodAffinityArgs",
						APIVersion: "kubescheduler.config.k8s.io/v1",
					},
					HardPodAffinityWeight: &hardPodAffinityWeight,
				},
			},
		},
		{
			Name: "NodeAffinity",
			Args: runtime.RawExtension{
				Object: &configv1.NodeAffinityArgs{
					TypeMeta: metav1.TypeMeta{
						Kind:       "NodeAffinityArgs",
						APIVersion: "kubescheduler.config.k8s.io/v1",
					},
				},
			},
		},
		{
			Name: "NodeResourcesBalancedAllocation",
			Args: runtime.RawExtension{
				Object: &configv1.NodeResourcesBalancedAllocationArgs{
					TypeMeta: metav1.TypeMeta{
						Kind:       "NodeResourcesBalancedAllocationArgs",
						APIVersion: "kubescheduler.config.k8s.io/v1",
					},
					Resources: []configv1.ResourceSpec{
						{
							Name:   "cpu",
							Weight: 1,
						},
						{
							Name:   "memory",
							Weight: 1,
						},
					},
				},
			},
		},
		{
			Name: "NodeResourcesFit",
			Args: runtime.RawExtension{
				Object: &configv1.NodeResourcesFitArgs{
					TypeMeta: metav1.TypeMeta{
						Kind:       "NodeResourcesFitArgs",
						APIVersion: "kubescheduler.config.k8s.io/v1",
					},
					ScoringStrategy: &configv1.ScoringStrategy{
						Type: "LeastAllocated",
						Resources: []configv1.ResourceSpec{
							{
								Name:   "cpu",
								Weight: 1,
							},
							{
								Name:   "memory",
								Weight: 1,
							},
						},
					},
				},
			},
		},
		{
			Name: "PodTopologySpread",
			Args: runtime.RawExtension{
				Object: &configv1.PodTopologySpreadArgs{
					TypeMeta: metav1.TypeMeta{
						Kind:       "PodTopologySpreadArgs",
						APIVersion: "kubescheduler.config.k8s.io/v1",
					},
					DefaultingType: "System",
				},
			},
		},
		{
			Name: "VolumeBinding",
			Args: runtime.RawExtension{
				Object: &configv1.VolumeBindingArgs{
					TypeMeta: metav1.TypeMeta{
						Kind:       "VolumeBindingArgs",
						APIVersion: "kubescheduler.config.k8s.io/v1",
					},
					BindTimeoutSeconds: &bindTimeoutSeconds,
				},
			},
		},
		{
			Name: "DefaultPreemptionWrapped",
			Args: runtime.RawExtension{
				Object: &configv1.DefaultPreemptionArgs{
					TypeMeta: metav1.TypeMeta{
						Kind:       "DefaultPreemptionArgs",
						APIVersion: "kubescheduler.config.k8s.io/v1",
					},
					MinCandidateNodesPercentage: &minCandidateNodesPercentage,
					MinCandidateNodesAbsolute:   &minCandidateNodesAbsolute,
				},
			},
		},
		{
			Name: "NodeResourcesBalancedAllocationWrapped",
			Args: runtime.RawExtension{
				Object: &configv1.NodeResourcesBalancedAllocationArgs{
					TypeMeta: metav1.TypeMeta{
						Kind:       "NodeResourcesBalancedAllocationArgs",
						APIVersion: "kubescheduler.config.k8s.io/v1",
					},
					Resources: []configv1.ResourceSpec{
						{
							Name:   "cpu",
							Weight: 1,
						},
						{
							Name:   "memory",
							Weight: 1,
						},
					},
				},
			},
		},
		{
			Name: "InterPodAffinityWrapped",
			Args: runtime.RawExtension{
				Object: &configv1.InterPodAffinityArgs{
					TypeMeta: metav1.TypeMeta{
						Kind:       "InterPodAffinityArgs",
						APIVersion: "kubescheduler.config.k8s.io/v1",
					},
					HardPodAffinityWeight: &hardPodAffinityWeight,
				},
			},
		},
		{
			Name: "NodeResourcesFitWrapped",
			Args: runtime.RawExtension{
				Object: &configv1.NodeResourcesFitArgs{
					TypeMeta: metav1.TypeMeta{
						Kind:       "NodeResourcesFitArgs",
						APIVersion: "kubescheduler.config.k8s.io/v1",
					},
					ScoringStrategy: &configv1.ScoringStrategy{
						Type: "LeastAllocated",
						Resources: []configv1.ResourceSpec{
							{
								Name:   "cpu",
								Weight: 1,
							},
							{
								Name:   "memory",
								Weight: 1,
							},
						},
					},
				},
			},
		},
		{
			Name: "NodeAffinityWrapped",
			Args: runtime.RawExtension{
				Object: &configv1.NodeAffinityArgs{
					TypeMeta: metav1.TypeMeta{
						Kind:       "NodeAffinityArgs",
						APIVersion: "kubescheduler.config.k8s.io/v1",
					},
				},
			},
		},
		{
			Name: "PodTopologySpreadWrapped",
			Args: runtime.RawExtension{
				Object: &configv1.PodTopologySpreadArgs{
					TypeMeta: metav1.TypeMeta{
						Kind:       "PodTopologySpreadArgs",
						APIVersion: "kubescheduler.config.k8s.io/v1",
					},
					DefaultingType: "System",
				},
			},
		},
		{
			Name: "VolumeBindingWrapped",
			Args: runtime.RawExtension{
				Object: &configv1.VolumeBindingArgs{
					TypeMeta: metav1.TypeMeta{
						Kind:       "VolumeBindingArgs",
						APIVersion: "kubescheduler.config.k8s.io/v1",
					},
					BindTimeoutSeconds: &bindTimeoutSeconds,
				},
			},
		},
	}
}

func TestGetScorePluginWeight(t *testing.T) {
	t.Parallel()
	tests := []struct {
		name string
		cfg  *schedulerConfig.KubeSchedulerConfiguration
		want map[string]int32
	}{
		{
			name: "score and multipoint plugins",
			cfg: &schedulerConfig.KubeSchedulerConfiguration{
				Profiles: []schedulerConfig.KubeSchedulerProfile{
					{
						Plugins: &schedulerConfig.Plugins{
							Score: schedulerConfig.PluginSet{
								Enabled: []schedulerConfig.Plugin{
									{
										Name:   "score1",
										Weight: 1,
									},
									{
										Name:   "score2",
										Weight: 2,
									},
								},
							},
							MultiPoint: schedulerConfig.PluginSet{
								Enabled: []schedulerConfig.Plugin{
									{
										Name:   "multipoint1",
										Weight: 1,
									},
									{
										Name:   "multipoint2",
										Weight: 2,
									},
								},
							},
						},
					},
				},
			},
			want: map[string]int32{
				"multipoint1": 1,
				"multipoint2": 2,
				"score1":      1,
				"score2":      2,
			},
		},
		{
			name: "only score plugins",
			cfg: &schedulerConfig.KubeSchedulerConfiguration{
				Profiles: []schedulerConfig.KubeSchedulerProfile{
					{
						Plugins: &schedulerConfig.Plugins{
							Score: schedulerConfig.PluginSet{
								Enabled: []schedulerConfig.Plugin{
									{
										Name:   "score1",
										Weight: 1,
									},
									{
										Name:   "score2",
										Weight: 2,
									},
								},
							},
						},
					},
				},
			},
			want: map[string]int32{
				"score1": 1,
				"score2": 2,
			},
		},
		{
			name: "only multipoint plugins",
			cfg: &schedulerConfig.KubeSchedulerConfiguration{
				Profiles: []schedulerConfig.KubeSchedulerProfile{
					{
						Plugins: &schedulerConfig.Plugins{
							MultiPoint: schedulerConfig.PluginSet{
								Enabled: []schedulerConfig.Plugin{
									{
										Name:   "multipoint1",
										Weight: 1,
									},
									{
										Name:   "multipoint2",
										Weight: 2,
									},
								},
							},
						},
					},
				},
			},
			want: map[string]int32{
				"multipoint1": 1,
				"multipoint2": 2,
			},
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			got := getScorePluginWeight(tt.cfg)
			if diff := cmp.Diff(tt.want, got); diff != "" {
				t.Errorf("unexpected plugins map: (-want, +got):\n%s", diff)
			}
		})
	}
}
</file>

<file path="kube-scheduler-simulator/simulator/scheduler/plugin/plugins.go">
package plugin

import (
	"context"
	"encoding/json"
	"strings"

	"golang.org/x/xerrors"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/util/sets"
	"k8s.io/klog/v2"
	configv1 "k8s.io/kube-scheduler/config/v1"
	schedulerConfig "k8s.io/kubernetes/pkg/scheduler/apis/config"
	"k8s.io/kubernetes/pkg/scheduler/framework"
	schedulerRuntime "k8s.io/kubernetes/pkg/scheduler/framework/runtime"

	"sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler/config"
	schedulingresultstore "sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler/plugin/resultstore"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler/storereflector"
)

// ResultStoreKey represents key name of plugins results on sharedstore.
const ResultStoreKey = "PluginResultStoreKey"

func NewRegistry(sharedStore storereflector.Reflector, cfg *schedulerConfig.KubeSchedulerConfiguration, pluginExtenders map[string]PluginExtenderInitializer) (map[string]schedulerRuntime.PluginFactory, error) {
	scorePluginWeight := getScorePluginWeight(cfg)
	store := schedulingresultstore.New(scorePluginWeight)
	// Add the resultStore to the sharedStore to store the results and share it.
	sharedStore.AddResultStore(store, ResultStoreKey)

	ret, err := newPluginFactories(store, pluginExtenders)
	if err != nil {
		return nil, xerrors.Errorf("New pluginFactories: %w", err)
	}

	return ret, nil
}

func newPluginFactories(store *schedulingresultstore.Store, pluginExtenders map[string]PluginExtenderInitializer) (map[string]schedulerRuntime.PluginFactory, error) {
	intreeRegistries := config.InTreeRegistries()
	outoftreeRegistries := config.OutOfTreeRegistries()
	pls, err := config.RegisteredMultiPointPluginNames()
	if err != nil {
		return nil, xerrors.Errorf("failed to get registered plugin names: %w", err)
	}
	ret := map[string]schedulerRuntime.PluginFactory{}
	for _, pluginname := range pls {
		pluginname := pluginname

		r, ok := intreeRegistries[pluginname]
		if !ok {
			// not found in intreeRegistries. search registry in outoftreeRegistries.
			r, ok = outoftreeRegistries[pluginname]
			if !ok {
				return nil, xerrors.Errorf("registry for %s is not found", pluginname)
			}
			// For out-of-tree plugins, we need to add original registry to registries.
			// (For in-tree plugins, schedulers add original registry to registries internally.)
			ret[pluginname] = r
		}

		if _, ok := ret[pluginName(pluginname)]; ok {
			// already created
			continue
		}

		factory := func(ctx context.Context, configuration runtime.Object, f framework.Handle) (framework.Plugin, error) {
			p, err := r(ctx, configuration, f)
			if err != nil {
				return nil, xerrors.Errorf("create original plugin: %w", err)
			}

			opts := []Option{}
			extender, ok := pluginExtenders[pluginname]
			if ok {
				opts = append(opts, WithExtendersOption(extender))
			}

			return NewWrappedPlugin(store, p, opts...), nil
		}
		ret[pluginName(pluginname)] = factory
	}

	return ret, nil
}

// NewPluginConfig converts []configv1.PluginConfig for simulator.
// Passed []v1beta.PluginConfig overrides default config values.
//
// NewPluginConfig expects that either PluginConfig.Args.Raw or PluginConfig.Args.Object has data
// in the passed configv1.PluginConfig parameter.
// If data exists in both PluginConfig.Args.Raw and PluginConfig.Args.Object, PluginConfig.Args.Raw would be ignored
// since PluginConfig.Args.Object has higher priority.
//
//nolint:funlen,cyclop
func NewPluginConfig(pc []configv1.PluginConfig) ([]configv1.PluginConfig, error) {
	defaultcfg, err := config.DefaultSchedulerConfig()
	if err != nil || len(defaultcfg.Profiles) != 1 {
		return nil, xerrors.Errorf("get default scheduler configuration: %w", err)
	}

	pluginConfig := make(map[string]*runtime.RawExtension, len(defaultcfg.Profiles[0].PluginConfig))
	for i := range defaultcfg.Profiles[0].PluginConfig {
		name := defaultcfg.Profiles[0].PluginConfig[i].Name
		pluginConfig[name] = &defaultcfg.Profiles[0].PluginConfig[i].Args
	}

	for i := range pc {
		name := pc[i].Name
		if _, ok := pluginConfig[name]; !ok {
			// it's non-in-tree's plugin's config.
			pluginConfig[name] = &pc[i].Args
			continue
		}

		ret := pluginConfig[name].DeepCopy()
		// If ret is nil, to reference ret.Object is occurred invalid memory address or nil pointer dereference.
		// To avoid this error, if ret is nil, we continue to next loop.
		if ret == nil {
			continue
		}

		// configv1.PluginConfig may have data in pc[i].Args.Raw as []byte.
		// We have to encoding it in this case.
		if len(pc[i].Args.Raw) != 0 {
			// override default configuration
			if err := json.Unmarshal(pc[i].Args.Raw, &ret.Object); err != nil {
				return nil, err
			}
		}

		if pc[i].Args.Object != nil {
			// If data exists in both PluginConfig.Args.Raw and PluginConfig.Args.Object,
			// PluginConfig.Args.Raw would be ignored
			ret.Object = pc[i].Args.Object
		}

		pluginConfig[name] = ret
	}

	ret := make([]configv1.PluginConfig, 0, len(pluginConfig))
	for name, arg := range pluginConfig {
		// add plugin configs for default plugins.
		ret = append(ret, configv1.PluginConfig{
			Name: name,
			Args: *arg,
		})
	}

	defaultpls, err := config.RegisteredMultiPointPluginNames()
	if err != nil {
		return nil, xerrors.Errorf("get default score/filter plugins: %w", err)
	}

	for _, name := range defaultpls {
		pc, ok := pluginConfig[name]
		if !ok {
			continue
		}

		ret = append(ret, configv1.PluginConfig{
			Name: pluginName(name),
			Args: *pc,
		})

		// avoid adding same plugin config.
		delete(pluginConfig, name)
	}

	return ret, nil
}

// ConvertForSimulator convert configv1.Plugins for simulator.
func ConvertForSimulator(pls *configv1.Plugins) (*configv1.Plugins, error) {
	newpls := pls.DeepCopy()

	applyPluginSet(&newpls.PreFilter, pls.PreFilter, configv1.PluginSet{})
	applyPluginSet(&newpls.Filter, pls.Filter, configv1.PluginSet{})
	applyPluginSet(&newpls.PostFilter, pls.PostFilter, configv1.PluginSet{})
	applyPluginSet(&newpls.PreScore, pls.PreScore, configv1.PluginSet{})
	applyPluginSet(&newpls.Score, pls.Score, configv1.PluginSet{})
	applyPluginSet(&newpls.Reserve, pls.Reserve, configv1.PluginSet{})
	applyPluginSet(&newpls.Permit, pls.Permit, configv1.PluginSet{})
	applyPluginSet(&newpls.PreBind, pls.PreBind, configv1.PluginSet{})
	applyPluginSet(&newpls.Bind, pls.Bind, configv1.PluginSet{})
	applyPluginSet(&newpls.PostBind, pls.PostBind, configv1.PluginSet{})
	inTreeMultiPointPls, err := config.InTreeMultiPointPluginSet()
	if err != nil {
		return nil, xerrors.Errorf("get in tree multi point plugins: %w", err)
	}
	applyPluginSet(&newpls.MultiPoint, pls.MultiPoint, inTreeMultiPointPls)
	// The default MultiPoint PluginSets should be disable to "*" here
	// so that the scheduler won't enable all default plugins.
	disableAllPluginSet(&newpls.MultiPoint)

	return newpls, nil
}

// disableAllPluginSet set target PluginSet to "*".
func disableAllPluginSet(targetPlsSet *configv1.PluginSet) {
	targetPlsSet.Disabled = []configv1.Plugin{
		{
			Name: "*",
		},
	}
}

// applyPluginSet merges inTree and outOfTree PluginSet.
func applyPluginSet(targetPlsSet *configv1.PluginSet, plsSet configv1.PluginSet, inTreePls configv1.PluginSet) {
	merged := mergePluginSet(inTreePls, plsSet)
	enabledPls := make([]configv1.Plugin, 0, len(merged.Enabled))
	for _, p := range merged.Enabled {
		enabledPls = append(enabledPls, configv1.Plugin{Name: pluginName(p.Name), Weight: p.Weight})
	}
	targetPlsSet.Enabled = enabledPls

	disabledPls := make([]configv1.Plugin, 0, len(merged.Disabled))
	for _, p := range merged.Disabled {
		wName := pluginName(p.Name)
		if p.Name == "*" {
			wName = p.Name
		}
		disabledPls = append(disabledPls, configv1.Plugin{Name: wName, Weight: p.Weight})
	}
	targetPlsSet.Disabled = disabledPls
}

// mergePluginsSet merges two plugin sets.
// This function is copied from https://github.com/kubernetes/kubernetes/blob/release-1.27/pkg/scheduler/apis/config/v1/default_plugins.go.
func mergePluginSet(defaultPluginSet, customPluginSet configv1.PluginSet) configv1.PluginSet {
	type pluginIndex struct {
		index  int
		plugin configv1.Plugin
	}

	disabledPlugins := sets.New[string]()
	enabledCustomPlugins := make(map[string]pluginIndex)
	// replacedPluginIndex is a set of index of plugins, which have replaced the default plugins.
	replacedPluginIndex := sets.NewInt()
	disabled := make([]configv1.Plugin, 0, len(customPluginSet.Disabled))
	for _, disabledPlugin := range customPluginSet.Disabled {
		// if the user is manually disabling any (or all, with "*") default plugins for an extension point,
		// we need to track that so that the MultiPoint extension logic in the framework can know to skip
		// inserting unspecified default plugins to this point.
		disabled = append(disabled, configv1.Plugin{Name: disabledPlugin.Name})
		disabledPlugins.Insert(disabledPlugin.Name)
	}

	// With MultiPoint, we may now have some disabledPlugins in the default registry
	// For example, we enable PluginX with Filter+Score through MultiPoint but disable its Score plugin by default.
	for _, disabledPlugin := range defaultPluginSet.Disabled {
		disabled = append(disabled, configv1.Plugin{Name: disabledPlugin.Name})
		disabledPlugins.Insert(disabledPlugin.Name)
	}

	for index, enabledPlugin := range customPluginSet.Enabled {
		enabledCustomPlugins[enabledPlugin.Name] = pluginIndex{index, enabledPlugin}
	}
	var enabledPlugins []configv1.Plugin
	if !disabledPlugins.Has("*") {
		for _, defaultEnabledPlugin := range defaultPluginSet.Enabled {
			if disabledPlugins.Has(defaultEnabledPlugin.Name) {
				continue
			}
			// The default plugin is explicitly re-configured, update the default plugin accordingly.
			if customPlugin, ok := enabledCustomPlugins[defaultEnabledPlugin.Name]; ok {
				klog.Info("Default plugin is explicitly re-configured; overriding", "plugin", defaultEnabledPlugin.Name)
				// Update the default plugin in place to preserve order.
				defaultEnabledPlugin = customPlugin.plugin
				replacedPluginIndex.Insert(customPlugin.index)
			}
			enabledPlugins = append(enabledPlugins, defaultEnabledPlugin)
		}
	}

	// Append all the custom plugins which haven't replaced any default plugins.
	// Note: duplicated custom plugins will still be appended here.
	// If so, the instantiation of scheduler framework will detect it and abort.
	for index, plugin := range customPluginSet.Enabled {
		if !replacedPluginIndex.Has(index) {
			enabledPlugins = append(enabledPlugins, plugin)
		}
	}
	return configv1.PluginSet{Enabled: enabledPlugins, Disabled: disabled}
}

// getScorePluginWeight get weights of enabled score plugins in the scheduler configuration.
// It only supports the scheduler configuration with one profile -- scheduler with multiple profiles isn't supported.
func getScorePluginWeight(cfg *schedulerConfig.KubeSchedulerConfiguration) map[string]int32 {
	scorePluginWeight := make(map[string]int32)
	enabledScorePlugins := cfg.Profiles[0].Plugins.Score.Enabled
	enabledScorePlugins = append(enabledScorePlugins, cfg.Profiles[0].Plugins.MultiPoint.Enabled...)
	for _, p := range enabledScorePlugins {
		if p.Weight != 0 {
			scorePluginWeight[strings.TrimSuffix(p.Name, pluginSuffix)] = p.Weight
		} else {
			// a weight of zero is not permitted, plugins can be disabled explicitly
			// when configured.
			scorePluginWeight[strings.TrimSuffix(p.Name, pluginSuffix)] = 1
		}
	}

	return scorePluginWeight
}
</file>

<file path="kube-scheduler-simulator/simulator/scheduler/plugin/wrappedplugin_test.go">
package plugin

import (
	"context"
	"errors"
	"sort"
	"testing"
	"time"

	"github.com/stretchr/testify/assert"
	"go.uber.org/mock/gomock"
	v1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/util/sets"
	"k8s.io/kubernetes/pkg/scheduler/framework"

	mock_plugin "sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler/plugin/mock"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler/plugin/resultstore"
)

func Test_NewWrappedPlugin(t *testing.T) {
	t.Parallel()
	store := resultstore.New(nil)

	type args struct {
		s *resultstore.Store
		p framework.Plugin
	}
	tests := []struct {
		name string
		args args
		want framework.Plugin
	}{
		{
			name: "success with filter plugin",
			args: args{
				s: store,
				p: fakeFilterPlugin{},
			},
			want: &wrappedPlugin{
				name:                 "fakeFilterPluginWrapped",
				originalFilterPlugin: fakeFilterPlugin{},
				originalScorePlugin:  nil,
				store:                store,
			},
		},
		{
			name: "success with postFilter plugin",
			args: args{
				s: store,
				p: fakePostFilterPlugin{},
			},
			want: &wrappedPlugin{
				name:                     "fakePostFilterPluginWrapped",
				originalFilterPlugin:     nil,
				originalPostFilterPlugin: fakePostFilterPlugin{},
				originalScorePlugin:      nil,
				store:                    store,
			},
		},
		{
			name: "success with score plugin",
			args: args{
				s: store,
				p: fakeScorePlugin{},
			},
			want: &wrappedPlugin{
				name:                 "fakeScorePluginWrapped",
				originalFilterPlugin: nil,
				originalScorePlugin:  fakeScorePlugin{},
				store:                store,
			},
		},
		{
			name: "success with score/filter/postFilter plugin",
			args: args{
				s: store,
				p: fakeWrappedPlugin{},
			},
			want: &wrappedPlugin{
				name:                     "fakeWrappedPluginWrapped",
				originalFilterPlugin:     fakeWrappedPlugin{},
				originalScorePlugin:      fakeWrappedPlugin{},
				originalPostFilterPlugin: fakeWrappedPlugin{},
				store:                    store,
			},
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			got := NewWrappedPlugin(tt.args.s, tt.args.p)
			assert.Equal(t, tt.want, got)
		})
	}
}

func Test_NewWrappedPlugin_WithPluginNameOption(t *testing.T) {
	t.Parallel()
	store := resultstore.New(nil)

	type args struct {
		s    *resultstore.Store
		p    framework.Plugin
		name string
	}
	tests := []struct {
		name string
		args args
		want framework.Plugin
	}{
		{
			name: "plugin name is named by user",
			args: args{
				s:    store,
				p:    fakeFilterPlugin{},
				name: "userNamedPlugin",
			},
			want: &wrappedPlugin{
				name:                 "userNamedPlugin",
				originalFilterPlugin: fakeFilterPlugin{},
				originalScorePlugin:  nil,
				store:                store,
			},
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			got := NewWrappedPlugin(tt.args.s, tt.args.p, WithPluginNameOption(&tt.args.name))
			assert.Equal(t, tt.want, got)
		})
	}
}

func Test_pluginName(t *testing.T) {
	t.Parallel()
	tests := []struct {
		name       string
		pluginName string
		want       string
	}{
		{
			name:       "success",
			pluginName: "pluginname",
			want:       "pluginnameWrapped",
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			if got := pluginName(tt.pluginName); got != tt.want {
				t.Errorf("pluginName() = %v, want %v", got, tt.want)
			}
		})
	}
}

func Test_wrappedPlugin_Filter(t *testing.T) {
	t.Parallel()

	type args struct {
		pod      *v1.Pod
		nodeInfo *framework.NodeInfo
	}
	tests := []struct {
		name                 string
		prepareStoreFn       func(m *mock_plugin.MockStore)
		originalFilterPlugin framework.FilterPlugin
		args                 args
		want                 *framework.Status
	}{
		{
			name: "success",
			prepareStoreFn: func(m *mock_plugin.MockStore) {
				m.EXPECT().AddFilterResult("default", "pod1", "node1", "fakeFilterPlugin", resultstore.PassedFilterMessage)
			},
			originalFilterPlugin: fakeFilterPlugin{},
			args: args{
				pod: &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "pod1", Namespace: "default"}},
				nodeInfo: func() *framework.NodeInfo {
					n := &framework.NodeInfo{}
					n.SetNode(&v1.Node{ObjectMeta: metav1.ObjectMeta{Name: "node1"}})
					return n
				}(),
			},
			want: nil,
		},
		{
			name:                 "success when it is not filter plugin",
			prepareStoreFn:       func(_ *mock_plugin.MockStore) {},
			originalFilterPlugin: nil, // don't have filter plugin
			args: args{
				pod: &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "pod1"}},
				nodeInfo: func() *framework.NodeInfo {
					n := &framework.NodeInfo{}
					n.SetNode(&v1.Node{ObjectMeta: metav1.ObjectMeta{Name: "node1"}})
					return n
				}(),
			},
			want: nil,
		},
		{
			name: "fail when original plugin return non-success",
			prepareStoreFn: func(m *mock_plugin.MockStore) {
				m.EXPECT().AddFilterResult("default", "pod1", "node1", "fakeMustFailWrappedPlugin", "filter failed")
			},
			originalFilterPlugin: fakeMustFailWrappedPlugin{},
			args: args{
				pod: &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "pod1", Namespace: "default"}},
				nodeInfo: func() *framework.NodeInfo {
					n := &framework.NodeInfo{}
					n.SetNode(&v1.Node{ObjectMeta: metav1.ObjectMeta{Name: "node1"}})
					return n
				}(),
			},
			want: framework.AsStatus(errFilter),
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			ctrl := gomock.NewController(t)

			s := mock_plugin.NewMockStore(ctrl)
			tt.prepareStoreFn(s)
			pl := &wrappedPlugin{
				originalFilterPlugin: tt.originalFilterPlugin,
				store:                s,
			}
			got := pl.Filter(context.Background(), nil, tt.args.pod, tt.args.nodeInfo)
			assert.Equal(t, tt.want, got)
		})
	}
}

func Test_wrappedPlugin_Filter_WithPluginExtender(t *testing.T) {
	t.Parallel()

	type args struct {
		pod      *v1.Pod
		nodeInfo *framework.NodeInfo
	}
	tests := []struct {
		name              string
		prepareEachMockFn func(ctx context.Context, s *mock_plugin.MockStore, p *mock_plugin.MockFilterPlugin, fe *mock_plugin.MockFilterPluginExtender, as args)
		args              args
		wantstatus        *framework.Status
	}{
		{
			name: "return AfterFilter's results when Filter is successful",
			prepareEachMockFn: func(ctx context.Context, s *mock_plugin.MockStore, p *mock_plugin.MockFilterPlugin, fe *mock_plugin.MockFilterPluginExtender, as args) {
				success1 := framework.NewStatus(framework.Success, "BeforeFilter returned")
				success2 := framework.NewStatus(framework.Success, "Filter returned")
				success3 := framework.NewStatus(framework.Success, "AfterFilter returned")
				fe.EXPECT().BeforeFilter(ctx, nil, as.pod, as.nodeInfo).Return(success1)
				p.EXPECT().Filter(ctx, nil, as.pod, as.nodeInfo).Return(success2)
				fe.EXPECT().AfterFilter(ctx, nil, as.pod, as.nodeInfo, success2).Return(success3)
				p.EXPECT().Name().Return("fakeFilterPlugin").AnyTimes()
				// Filter sotres resultstore.PassedFilterMessage if it is successful.
				s.EXPECT().AddFilterResult("default", "pod1", "node1", "fakeFilterPlugin", resultstore.PassedFilterMessage)
			},
			args: args{
				pod: &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "pod1", Namespace: "default"}},
				nodeInfo: func() *framework.NodeInfo {
					n := &framework.NodeInfo{}
					n.SetNode(&v1.Node{ObjectMeta: metav1.ObjectMeta{Name: "node1"}})
					return n
				}(),
			},
			wantstatus: framework.NewStatus(framework.Success, "AfterFilter returned"),
		},
		{
			name: "return AfterFilter's results if Filter is fails",
			prepareEachMockFn: func(ctx context.Context, s *mock_plugin.MockStore, p *mock_plugin.MockFilterPlugin, fe *mock_plugin.MockFilterPluginExtender, as args) {
				success1 := framework.NewStatus(framework.Success, "BeforeFilter returned")
				failure := framework.NewStatus(framework.Error, "Filter returned")
				success3 := framework.NewStatus(framework.Success, "AfterFilter returned")
				fe.EXPECT().BeforeFilter(ctx, nil, as.pod, as.nodeInfo).Return(success1)
				p.EXPECT().Filter(ctx, nil, as.pod, as.nodeInfo).Return(failure)
				fe.EXPECT().AfterFilter(ctx, nil, as.pod, as.nodeInfo, failure).Return(success3)
				p.EXPECT().Name().Return("fakeFilterPlugin").AnyTimes()
				// Filter stores own message if it is successful.
				s.EXPECT().AddFilterResult("default", "pod1", "node1", "fakeFilterPlugin", failure.Message())
			},
			args: args{
				pod: &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "pod1", Namespace: "default"}},
				nodeInfo: func() *framework.NodeInfo {
					n := &framework.NodeInfo{}
					n.SetNode(&v1.Node{ObjectMeta: metav1.ObjectMeta{Name: "node1"}})
					return n
				}(),
			},
			wantstatus: framework.NewStatus(framework.Success, "AfterFilter returned"),
		},
		{
			name: "return BeforeFilter's results when BeforeFilter is fails",
			prepareEachMockFn: func(ctx context.Context, _ *mock_plugin.MockStore, p *mock_plugin.MockFilterPlugin, fe *mock_plugin.MockFilterPluginExtender, as args) {
				failure := framework.NewStatus(framework.Error, "BeforeFilter returned")
				fe.EXPECT().BeforeFilter(ctx, nil, as.pod, as.nodeInfo).Return(failure)
				p.EXPECT().Name().Return("fakeFilterPlugin").AnyTimes()
			},
			args: args{
				pod: &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "pod1", Namespace: "default"}},
				nodeInfo: func() *framework.NodeInfo {
					n := &framework.NodeInfo{}
					n.SetNode(&v1.Node{ObjectMeta: metav1.ObjectMeta{Name: "node1"}})
					return n
				}(),
			},
			wantstatus: framework.NewStatus(framework.Error, "BeforeFilter returned"),
		},
		{
			name: "return AfterFilter's results when AfterFilter is fails",
			prepareEachMockFn: func(ctx context.Context, s *mock_plugin.MockStore, p *mock_plugin.MockFilterPlugin, fe *mock_plugin.MockFilterPluginExtender, as args) {
				success1 := framework.NewStatus(framework.Success, "BeforeFilter returned")
				success2 := framework.NewStatus(framework.Success, "Filter returned")
				failure := framework.NewStatus(framework.Error, "AfterFilter returned")
				fe.EXPECT().BeforeFilter(ctx, nil, as.pod, as.nodeInfo).Return(success1)
				p.EXPECT().Filter(ctx, nil, as.pod, as.nodeInfo).Return(success2)
				fe.EXPECT().AfterFilter(ctx, nil, as.pod, as.nodeInfo, success2).Return(failure)
				p.EXPECT().Name().Return("fakeFilterPlugin").AnyTimes()
				s.EXPECT().AddFilterResult("default", "pod1", "node1", "fakeFilterPlugin", resultstore.PassedFilterMessage)
			},
			args: args{
				pod: &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "pod1", Namespace: "default"}},
				nodeInfo: func() *framework.NodeInfo {
					n := &framework.NodeInfo{}
					n.SetNode(&v1.Node{ObjectMeta: metav1.ObjectMeta{Name: "node1"}})
					return n
				}(),
			},
			wantstatus: framework.NewStatus(framework.Error, "AfterFilter returned"),
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			ctrl := gomock.NewController(t)
			s := mock_plugin.NewMockStore(ctrl)
			p := mock_plugin.NewMockFilterPlugin(ctrl)
			fe := mock_plugin.NewMockFilterPluginExtender(ctrl)
			ctx := context.Background()
			tt.prepareEachMockFn(ctx, s, p, fe, tt.args)
			pl, ok := NewWrappedPlugin(s, p, WithExtendersOption(func(_ SimulatorHandle) PluginExtenders {
				return PluginExtenders{
					FilterPluginExtender: fe,
				}
			})).(*wrappedPlugin)
			if !ok { // should never happen
				t.Fatalf("Assert to wrapped plugin: %v", ok)
			}
			gotstatus := pl.Filter(ctx, nil, tt.args.pod, tt.args.nodeInfo)
			assert.Equal(t, tt.wantstatus, gotstatus)
		})
	}
}

func Test_wrappedPlugin_PostFilter(t *testing.T) {
	t.Parallel()

	type args struct {
		pod                   *v1.Pod
		filteredNodeStatusMap framework.NodeToStatusMap
	}
	tests := []struct {
		name                     string
		prepareStoreFn           func(m *mock_plugin.MockStore)
		originalPostFilterPlugin framework.PostFilterPlugin
		args                     args
		wantResult               *framework.PostFilterResult
		wantStatus               *framework.Status
	}{
		{
			name: "success",
			prepareStoreFn: func(m *mock_plugin.MockStore) {
				m.EXPECT().AddPostFilterResult("default", "pod1", "node1", "fakePostFilterPlugin", gomock.Any()).Do(func(_, _, _, _ string, nodeNames []string) {
					sort.SliceStable(nodeNames, func(i, j int) bool {
						return nodeNames[i] < nodeNames[j]
					})
					assert.Equal(t, []string{"node1", "node2"}, nodeNames)
				})
			},
			originalPostFilterPlugin: fakePostFilterPlugin{},
			args: args{
				pod: &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "pod1", Namespace: "default"}},
				filteredNodeStatusMap: framework.NodeToStatusMap{
					"node1": framework.NewStatus(framework.UnschedulableAndUnresolvable, ""),
					"node2": framework.NewStatus(framework.UnschedulableAndUnresolvable, ""),
				},
			},
			wantResult: &framework.PostFilterResult{
				NominatingInfo: &framework.NominatingInfo{
					NominatedNodeName: "node1",
				},
			},
			wantStatus: framework.NewStatus(framework.Success, "postfilter success"),
		},
		{
			name:                     "success when it is not post filter plugin",
			prepareStoreFn:           func(_ *mock_plugin.MockStore) {},
			originalPostFilterPlugin: nil, // don't have post filter plugin
			args: args{
				pod: &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "pod1"}},
				filteredNodeStatusMap: framework.NodeToStatusMap{
					"node1": framework.NewStatus(framework.UnschedulableAndUnresolvable, ""),
					"node2": framework.NewStatus(framework.UnschedulableAndUnresolvable, ""),
				},
			},
			wantResult: nil,
			wantStatus: framework.NewStatus(framework.Unschedulable),
		},
		{
			name: "fail when original plugin return non-success",
			prepareStoreFn: func(m *mock_plugin.MockStore) {
				m.EXPECT().AddPostFilterResult("default", "pod1", "", "fakeMustFailWrappedPlugin", gomock.Any()).Do(func(_, _, _, _ string, nodeNames []string) {
					sort.SliceStable(nodeNames, func(i, j int) bool {
						return nodeNames[i] < nodeNames[j]
					})
					assert.Equal(t, []string{"node1", "node2"}, nodeNames)
				})
			},
			originalPostFilterPlugin: fakeMustFailWrappedPlugin{},
			args: args{
				pod: &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "pod1", Namespace: "default"}},
				filteredNodeStatusMap: framework.NodeToStatusMap{
					"node1": framework.NewStatus(framework.UnschedulableAndUnresolvable, ""),
					"node2": framework.NewStatus(framework.UnschedulableAndUnresolvable, ""),
				},
			},
			wantResult: nil,
			wantStatus: framework.AsStatus(errPost),
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			ctrl := gomock.NewController(t)

			s := mock_plugin.NewMockStore(ctrl)
			tt.prepareStoreFn(s)
			pl := &wrappedPlugin{
				originalPostFilterPlugin: tt.originalPostFilterPlugin,
				store:                    s,
			}
			gotResult, gotStatus := pl.PostFilter(context.Background(), nil, tt.args.pod, tt.args.filteredNodeStatusMap)
			assert.Equal(t, tt.wantResult, gotResult)
			assert.Equal(t, tt.wantStatus, gotStatus)
		})
	}
}

func Test_wrappedPlugin_PostFilter_WithPluginExtender(t *testing.T) {
	t.Parallel()

	type args struct {
		pod                   *v1.Pod
		filteredNodeStatusMap framework.NodeToStatusMap
	}
	tests := []struct {
		name              string
		prepareEachMockFn func(ctx context.Context, s *mock_plugin.MockStore, p *mock_plugin.MockPostFilterPlugin, fe *mock_plugin.MockPostFilterPluginExtender, as args)
		args              args
		wantResult        *framework.PostFilterResult
		wantStatus        *framework.Status
	}{
		{
			name: "return AfterPostFilter's results when PostFilter is successful",
			prepareEachMockFn: func(ctx context.Context, s *mock_plugin.MockStore, p *mock_plugin.MockPostFilterPlugin, fe *mock_plugin.MockPostFilterPluginExtender, as args) {
				success1 := framework.NewStatus(framework.Success, "BeforePostFilter returned")
				success2 := framework.NewStatus(framework.Success, "PostFilter returned")
				success3 := framework.NewStatus(framework.Success, "AfterPostFilter returned")
				result1 := &framework.PostFilterResult{
					NominatingInfo: &framework.NominatingInfo{
						NominatedNodeName: "node1",
					},
				}
				result2 := &framework.PostFilterResult{
					NominatingInfo: &framework.NominatingInfo{
						NominatedNodeName: "node2",
					},
				}
				fe.EXPECT().BeforePostFilter(ctx, nil, as.pod, as.filteredNodeStatusMap).Return(nil, success1)
				p.EXPECT().PostFilter(ctx, nil, as.pod, as.filteredNodeStatusMap).Return(result1, success2)
				fe.EXPECT().AfterPostFilter(ctx, nil, as.pod, as.filteredNodeStatusMap, result1, success2).Return(result2, success3)
				p.EXPECT().Name().Return("fakePostFilterPlugin").AnyTimes()
				// PostFilter sotres resultstore.PassedFilterMessage if it is successful.
				s.EXPECT().AddPostFilterResult("default", "pod1", "node1", "fakePostFilterPlugin", gomock.Any()).Do(func(_, _, _, _ string, nodeNames []string) {
					sort.SliceStable(nodeNames, func(i, j int) bool {
						return nodeNames[i] < nodeNames[j]
					})
					assert.Equal(t, []string{"node1", "node2"}, nodeNames)
				})
			},
			args: args{
				pod: &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "pod1", Namespace: "default"}},
				filteredNodeStatusMap: framework.NodeToStatusMap{
					"node1": framework.NewStatus(framework.UnschedulableAndUnresolvable, ""),
					"node2": framework.NewStatus(framework.UnschedulableAndUnresolvable, ""),
				},
			},
			wantResult: &framework.PostFilterResult{
				NominatingInfo: &framework.NominatingInfo{
					NominatedNodeName: "node2",
				},
			},
			wantStatus: framework.NewStatus(framework.Success, "AfterPostFilter returned"),
		},
		{
			name: "return AfterPostFilter's results if Filter is fails",
			prepareEachMockFn: func(ctx context.Context, s *mock_plugin.MockStore, p *mock_plugin.MockPostFilterPlugin, fe *mock_plugin.MockPostFilterPluginExtender, as args) {
				success1 := framework.NewStatus(framework.Success, "BeforePostFilter returned")
				failure := framework.NewStatus(framework.Error, "PostFilter returned")
				success3 := framework.NewStatus(framework.Success, "AfterPostFilter returned")
				result2 := &framework.PostFilterResult{
					NominatingInfo: &framework.NominatingInfo{
						NominatedNodeName: "node2",
					},
				}
				fe.EXPECT().BeforePostFilter(ctx, nil, as.pod, as.filteredNodeStatusMap).Return(nil, success1)
				p.EXPECT().PostFilter(ctx, nil, as.pod, as.filteredNodeStatusMap).Return(nil, failure)
				fe.EXPECT().AfterPostFilter(ctx, nil, as.pod, as.filteredNodeStatusMap, nil, failure).Return(result2, success3)
				p.EXPECT().Name().Return("fakePostFilterPlugin").AnyTimes()
				// Filter stores own message if it is successful.
				s.EXPECT().AddPostFilterResult("default", "pod1", "", "fakePostFilterPlugin", gomock.Any()).Do(func(_, _, _, _ string, nodeNames []string) {
					sort.SliceStable(nodeNames, func(i, j int) bool {
						return nodeNames[i] < nodeNames[j]
					})
					assert.Equal(t, []string{"node1", "node2"}, nodeNames)
				})
			},
			args: args{
				pod: &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "pod1", Namespace: "default"}},
				filteredNodeStatusMap: framework.NodeToStatusMap{
					"node1": framework.NewStatus(framework.UnschedulableAndUnresolvable, ""),
					"node2": framework.NewStatus(framework.UnschedulableAndUnresolvable, ""),
				},
			},
			wantResult: &framework.PostFilterResult{
				NominatingInfo: &framework.NominatingInfo{
					NominatedNodeName: "node2",
				},
			},
			wantStatus: framework.NewStatus(framework.Success, "AfterPostFilter returned"),
		},
		{
			name: "return BeforeFilter's results when BeforeFilter is fails",
			prepareEachMockFn: func(ctx context.Context, _ *mock_plugin.MockStore, p *mock_plugin.MockPostFilterPlugin, fe *mock_plugin.MockPostFilterPluginExtender, as args) {
				failure := framework.NewStatus(framework.Error, "BeforePostFilter returned")
				fe.EXPECT().BeforePostFilter(ctx, nil, as.pod, as.filteredNodeStatusMap).Return(nil, failure)
				p.EXPECT().Name().Return("fakePostFilterPlugin").AnyTimes()
			},
			args: args{
				pod: &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "pod1", Namespace: "default"}},
				filteredNodeStatusMap: framework.NodeToStatusMap{
					"node1": framework.NewStatus(framework.UnschedulableAndUnresolvable, ""),
					"node2": framework.NewStatus(framework.UnschedulableAndUnresolvable, ""),
				},
			},
			wantResult: nil,
			wantStatus: framework.NewStatus(framework.Error, "BeforePostFilter returned"),
		},
		{
			name: "return AfterFilter's results when AfterFilter is fails",
			prepareEachMockFn: func(ctx context.Context, s *mock_plugin.MockStore, p *mock_plugin.MockPostFilterPlugin, fe *mock_plugin.MockPostFilterPluginExtender, as args) {
				success1 := framework.NewStatus(framework.Success, "BeforePostFilter returned")
				success2 := framework.NewStatus(framework.Success, "PostFilter returned")
				result1 := &framework.PostFilterResult{
					NominatingInfo: &framework.NominatingInfo{
						NominatedNodeName: "node1",
					},
				}
				failure := framework.NewStatus(framework.Error, "AfterPostFilter returned")
				fe.EXPECT().BeforePostFilter(ctx, nil, as.pod, as.filteredNodeStatusMap).Return(nil, success1)
				p.EXPECT().PostFilter(ctx, nil, as.pod, as.filteredNodeStatusMap).Return(result1, success2)
				fe.EXPECT().AfterPostFilter(ctx, nil, as.pod, as.filteredNodeStatusMap, result1, success2).Return(nil, failure)
				p.EXPECT().Name().Return("fakePostFilterPlugin").AnyTimes()
				// PostFilter sotres resultstore.PassedFilterMessage if it is successful.
				s.EXPECT().AddPostFilterResult("default", "pod1", "node1", "fakePostFilterPlugin", gomock.Any()).Do(func(_, _, _, _ string, nodeNames []string) {
					sort.SliceStable(nodeNames, func(i, j int) bool {
						return nodeNames[i] < nodeNames[j]
					})
					assert.Equal(t, []string{"node1", "node2"}, nodeNames)
				})
			},
			args: args{
				pod: &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "pod1", Namespace: "default"}},
				filteredNodeStatusMap: framework.NodeToStatusMap{
					"node1": framework.NewStatus(framework.UnschedulableAndUnresolvable, ""),
					"node2": framework.NewStatus(framework.UnschedulableAndUnresolvable, ""),
				},
			},
			wantResult: nil,
			wantStatus: framework.NewStatus(framework.Error, "AfterPostFilter returned"),
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			ctrl := gomock.NewController(t)
			s := mock_plugin.NewMockStore(ctrl)
			p := mock_plugin.NewMockPostFilterPlugin(ctrl)
			fe := mock_plugin.NewMockPostFilterPluginExtender(ctrl)
			ctx := context.Background()
			tt.prepareEachMockFn(ctx, s, p, fe, tt.args)
			pl, ok := NewWrappedPlugin(s, p, WithExtendersOption(func(_ SimulatorHandle) PluginExtenders {
				return PluginExtenders{
					PostFilterPluginExtender: fe,
				}
			})).(*wrappedPlugin)
			if !ok { // should never happen
				t.Fatalf("Assert to wrapped plugin: %v", ok)
			}
			gotResult, gotStatus := pl.PostFilter(context.Background(), nil, tt.args.pod, tt.args.filteredNodeStatusMap)
			assert.Equal(t, tt.wantResult, gotResult)
			assert.Equal(t, tt.wantStatus, gotStatus)
		})
	}
}

func Test_wrappedPlugin_Name(t *testing.T) {
	t.Parallel()
	type fields struct {
		name string
	}
	tests := []struct {
		name   string
		fields fields
		want   string
	}{
		{
			name:   "success",
			fields: fields{name: "pluginWrapped"},
			want:   "pluginWrapped",
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			pl := &wrappedPlugin{
				name: tt.fields.name,
			}
			if got := pl.Name(); got != tt.want {
				t.Errorf("Name() = %v, want %v", got, tt.want)
			}
		})
	}
}

func Test_wrappedPlugin_NormalizeScore(t *testing.T) {
	t.Parallel()

	type args struct {
		pod    *v1.Pod
		scores framework.NodeScoreList
	}
	tests := []struct {
		name                string
		prepareStoreFn      func(m *mock_plugin.MockStore)
		originalScorePlugin framework.ScorePlugin
		args                args
		want                *framework.Status
	}{
		{
			name: "success",
			prepareStoreFn: func(m *mock_plugin.MockStore) {
				m.EXPECT().AddNormalizedScoreResult("default", "pod1", "node1", "fakeScorePlugin", int64(10))
				m.EXPECT().AddNormalizedScoreResult("default", "pod1", "node1", "fakeScorePlugin", int64(200))
			},
			originalScorePlugin: fakeScorePlugin{},
			args: args{
				pod: &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "pod1", Namespace: "default"}},
				scores: []framework.NodeScore{
					{
						Name:  "node1",
						Score: 10,
					},
					{
						Name:  "node1",
						Score: 200,
					},
				},
			},
			want: nil,
		},
		{
			name:                "return score 0 when it is not filter plugin",
			prepareStoreFn:      func(_ *mock_plugin.MockStore) {},
			originalScorePlugin: nil, // don't have filter plugin
			args: args{
				pod: &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "pod1"}},
				scores: []framework.NodeScore{
					{
						Name:  "node1",
						Score: 10,
					},
				},
			},
			want: nil,
		},
		{
			name:                "fail when original plugin return non-success",
			prepareStoreFn:      func(_ *mock_plugin.MockStore) {},
			originalScorePlugin: fakeMustFailWrappedPlugin{},
			args: args{
				pod: &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "pod1", Namespace: "default"}},
				scores: []framework.NodeScore{
					{
						Name:  "node1",
						Score: 10,
					},
				},
			},
			want: framework.AsStatus(errNormalize),
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			ctrl := gomock.NewController(t)

			s := mock_plugin.NewMockStore(ctrl)
			tt.prepareStoreFn(s)
			pl := &wrappedPlugin{
				originalScorePlugin: tt.originalScorePlugin,
				store:               s,
			}
			got := pl.NormalizeScore(context.Background(), nil, tt.args.pod, tt.args.scores)
			assert.Equal(t, tt.want, got)
		})
	}
}

func Test_wrappedPlugin_NormalizeScore_WithPluginExtender(t *testing.T) {
	t.Parallel()

	type args struct {
		pod    *v1.Pod
		scores framework.NodeScoreList
	}
	tests := []struct {
		name                      string
		prepareEachMockFn         func(ctx context.Context, s *mock_plugin.MockStore, se *mock_plugin.MockScoreExtensions, sp *mock_plugin.MockScorePlugin, spe *mock_plugin.MockNormalizeScorePluginExtender, as args)
		calOnBeforeNormalizeScore func(ctx context.Context, state *framework.CycleState, pod *v1.Pod, scores framework.NodeScoreList)
		args                      args
		wantScores                framework.NodeScoreList
		wantstatus                *framework.Status
	}{
		{
			name: "return AfterNormalizeScore's results when NormalizeScore is successful",
			prepareEachMockFn: func(ctx context.Context, s *mock_plugin.MockStore, se *mock_plugin.MockScoreExtensions, sp *mock_plugin.MockScorePlugin, spe *mock_plugin.MockNormalizeScorePluginExtender, as args) {
				calOnNormalizeScore := func(_ context.Context, _ *framework.CycleState, _ *v1.Pod, scores framework.NodeScoreList) {
					for i := range scores {
						scores[i].Score += 1000
					}
				}
				calOnAfterNormalizeScore := func(_ context.Context, _ *framework.CycleState, _ *v1.Pod, scores framework.NodeScoreList, _ *framework.Status) {
					for i := range scores {
						scores[i].Score += 1000
					}
				}
				success1 := framework.NewStatus(framework.Success, "BeforeNormalizeScore returned")
				success2 := framework.NewStatus(framework.Success, "NormalizeScore returned")
				success3 := framework.NewStatus(framework.Success, "AfterNormalizeScore returned")
				spe.EXPECT().BeforeNormalizeScore(ctx, nil, as.pod, as.scores).Return(success1).Do(calOnNormalizeScore)
				sp.EXPECT().ScoreExtensions().Return(se).Times(2)
				se.EXPECT().NormalizeScore(ctx, nil, as.pod, as.scores).Return(success2).Do(calOnNormalizeScore)
				spe.EXPECT().AfterNormalizeScore(ctx, nil, as.pod, as.scores, success2).Return(success3).Do(calOnAfterNormalizeScore)
				sp.EXPECT().Name().Return("fakeNormalizeScorePlugin").AnyTimes()
				s.EXPECT().AddNormalizedScoreResult("default", "pod1", "node1", "fakeNormalizeScorePlugin", int64(2000))
				s.EXPECT().AddNormalizedScoreResult("default", "pod1", "node2", "fakeNormalizeScorePlugin", int64(2010))
			},
			args: args{
				pod: &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "pod1", Namespace: "default"}},
				scores: []framework.NodeScore{
					{
						Name:  "node1",
						Score: 0,
					},
					{
						Name:  "node2",
						Score: 10,
					},
				},
			},
			wantScores: []framework.NodeScore{
				{
					Name:  "node1",
					Score: 3000,
				},
				{
					Name:  "node2",
					Score: 3010,
				},
			},
			wantstatus: framework.NewStatus(framework.Success, "AfterNormalizeScore returned"),
		},
		{
			name: "return AfterNormalizeScore's results when NormalizeScore is fails",
			prepareEachMockFn: func(ctx context.Context, _ *mock_plugin.MockStore, se *mock_plugin.MockScoreExtensions, sp *mock_plugin.MockScorePlugin, spe *mock_plugin.MockNormalizeScorePluginExtender, as args) {
				calOnNormalizeScore := func(_ context.Context, _ *framework.CycleState, _ *v1.Pod, scores framework.NodeScoreList) {
					for i := range scores {
						scores[i].Score += 1000
					}
				}
				calOnAfterNormalizeScore := func(_ context.Context, _ *framework.CycleState, _ *v1.Pod, scores framework.NodeScoreList, _ *framework.Status) {
					for i := range scores {
						scores[i].Score += 1000
					}
				}
				success1 := framework.NewStatus(framework.Success, "BeforeNormalizeScore returned")
				failure := framework.NewStatus(framework.Error, "NormalizeScore returned")
				success3 := framework.NewStatus(framework.Success, "AfterNormalizeScore returned")
				spe.EXPECT().BeforeNormalizeScore(ctx, nil, as.pod, as.scores).Return(success1).Do(calOnNormalizeScore)
				sp.EXPECT().ScoreExtensions().Return(se).Times(2)
				se.EXPECT().NormalizeScore(ctx, nil, as.pod, as.scores).Return(failure).Do(calOnNormalizeScore)
				spe.EXPECT().AfterNormalizeScore(ctx, nil, as.pod, as.scores, failure).Return(success3).Do(calOnAfterNormalizeScore)
				sp.EXPECT().Name().Return("fakeNormalizeScorePlugin").AnyTimes()
				// NormalizeScore isnt't stores own results if return error.
			},
			args: args{
				pod: &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "pod1", Namespace: "default"}},
				scores: []framework.NodeScore{
					{
						Name:  "node1",
						Score: 0,
					},
					{
						Name:  "node2",
						Score: 10,
					},
				},
			},
			wantScores: []framework.NodeScore{
				{
					Name:  "node1",
					Score: 3000,
				},
				{
					Name:  "node2",
					Score: 3010,
				},
			},
			wantstatus: framework.NewStatus(framework.Success, "AfterNormalizeScore returned"),
		},
		{
			name: "return AfterNormalizeScore's results, when NormalizeScore is successful and AfterNormalizeScore is fails",
			prepareEachMockFn: func(ctx context.Context, s *mock_plugin.MockStore, se *mock_plugin.MockScoreExtensions, sp *mock_plugin.MockScorePlugin, spe *mock_plugin.MockNormalizeScorePluginExtender, as args) {
				calOnNormalizeScore := func(_ context.Context, _ *framework.CycleState, _ *v1.Pod, scores framework.NodeScoreList) {
					for i := range scores {
						scores[i].Score += 1000
					}
				}
				calOnAfterNormalizeScore := func(_ context.Context, _ *framework.CycleState, _ *v1.Pod, scores framework.NodeScoreList, _ *framework.Status) {
					for i := range scores {
						scores[i].Score += 1000
					}
				}
				success1 := framework.NewStatus(framework.Success, "BeforeNormalizeScore returned")
				success2 := framework.NewStatus(framework.Success, "NormalizeScore returned")
				failure := framework.NewStatus(framework.Error, "AfterNormalizeScore returned")
				spe.EXPECT().BeforeNormalizeScore(ctx, nil, as.pod, as.scores).Return(success1).Do(calOnNormalizeScore)
				sp.EXPECT().ScoreExtensions().Return(se).Times(2)
				se.EXPECT().NormalizeScore(ctx, nil, as.pod, as.scores).Return(success2).Do(calOnNormalizeScore)
				spe.EXPECT().AfterNormalizeScore(ctx, nil, as.pod, as.scores, success2).Return(failure).Do(calOnAfterNormalizeScore)
				sp.EXPECT().Name().Return("fakeNormalizeScorePlugin").AnyTimes()
				s.EXPECT().AddNormalizedScoreResult("default", "pod1", "node1", "fakeNormalizeScorePlugin", int64(2000))
				s.EXPECT().AddNormalizedScoreResult("default", "pod1", "node2", "fakeNormalizeScorePlugin", int64(2010))
			},
			args: args{
				pod: &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "pod1", Namespace: "default"}},
				scores: []framework.NodeScore{
					{
						Name:  "node1",
						Score: 0,
					},
					{
						Name:  "node2",
						Score: 10,
					},
				},
			},
			wantScores: []framework.NodeScore{
				{
					Name:  "node1",
					Score: 3000,
				},
				{
					Name:  "node2",
					Score: 3010,
				},
			},
			wantstatus: framework.NewStatus(framework.Error, "AfterNormalizeScore returned"),
		},
		{
			name: "return BeforeNormalizeScore when BeforeNormalizeScore is fails",
			prepareEachMockFn: func(ctx context.Context, _ *mock_plugin.MockStore, se *mock_plugin.MockScoreExtensions, sp *mock_plugin.MockScorePlugin, spe *mock_plugin.MockNormalizeScorePluginExtender, as args) {
				calOnNormalizeScore := func(_ context.Context, _ *framework.CycleState, _ *v1.Pod, scores framework.NodeScoreList) {
					for i := range scores {
						scores[i].Score += 1000
					}
				}
				success1 := framework.NewStatus(framework.Error, "BeforeNormalizeScore returned")
				spe.EXPECT().BeforeNormalizeScore(ctx, nil, as.pod, as.scores).Return(success1).Do(calOnNormalizeScore)
				sp.EXPECT().ScoreExtensions().Return(se).Times(1)
				sp.EXPECT().Name().Return("fakeNormalizeScorePlugin").AnyTimes()
			},
			args: args{
				pod: &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "pod1", Namespace: "default"}},
				scores: []framework.NodeScore{
					{
						Name:  "node1",
						Score: 0,
					},
					{
						Name:  "node2",
						Score: 10,
					},
				},
			},
			wantScores: []framework.NodeScore{
				{
					Name:  "node1",
					Score: 1000,
				},
				{
					Name:  "node2",
					Score: 1010,
				},
			},
			wantstatus: framework.NewStatus(framework.Error, "BeforeNormalizeScore returned"),
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			ctrl := gomock.NewController(t)
			s := mock_plugin.NewMockStore(ctrl)
			se := mock_plugin.NewMockScoreExtensions(ctrl)
			sp := mock_plugin.NewMockScorePlugin(ctrl)

			spe := mock_plugin.NewMockNormalizeScorePluginExtender(ctrl)
			ctx := context.Background()
			tt.prepareEachMockFn(ctx, s, se, sp, spe, tt.args)
			pl, ok := NewWrappedPlugin(s, sp, WithExtendersOption(func(_ SimulatorHandle) PluginExtenders {
				return PluginExtenders{
					NormalizeScorePluginExtender: spe,
				}
			})).(*wrappedPlugin)
			if !ok { // should never happen
				t.Fatalf("Assert to wrapped plugin: %v", ok)
			}
			gotstatus := pl.NormalizeScore(ctx, nil, tt.args.pod, tt.args.scores)
			assert.Equal(t, tt.wantScores, tt.args.scores)
			assert.Equal(t, tt.wantstatus, gotstatus)
		})
	}
}

func Test_wrappedPlugin_Score(t *testing.T) {
	t.Parallel()

	type args struct {
		pod      *v1.Pod
		nodename string
	}
	tests := []struct {
		name                string
		prepareStoreFn      func(m *mock_plugin.MockStore)
		originalScorePlugin framework.ScorePlugin
		args                args
		want                int64
		wantstatus          *framework.Status
	}{
		{
			name: "success",
			prepareStoreFn: func(m *mock_plugin.MockStore) {
				m.EXPECT().AddScoreResult("default", "pod1", "node1", "fakeScorePlugin", int64(1))
			},
			originalScorePlugin: fakeScorePlugin{},
			args: args{
				pod:      &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "pod1", Namespace: "default"}},
				nodename: "node1",
			},
			want:       1,
			wantstatus: nil,
		},
		{
			name:                "return score 0 when it is not filter plugin",
			prepareStoreFn:      func(_ *mock_plugin.MockStore) {},
			originalScorePlugin: nil, // don't have filter plugin
			args: args{
				pod:      &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "pod1"}},
				nodename: "node1",
			},
			want:       0,
			wantstatus: nil,
		},
		{
			name:                "fail when original plugin return non-success",
			prepareStoreFn:      func(_ *mock_plugin.MockStore) {},
			originalScorePlugin: fakeMustFailWrappedPlugin{},
			args: args{
				pod:      &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "pod1", Namespace: "default"}},
				nodename: "node1",
			},
			want:       0,
			wantstatus: framework.AsStatus(errScore),
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			ctrl := gomock.NewController(t)

			s := mock_plugin.NewMockStore(ctrl)
			tt.prepareStoreFn(s)
			pl := &wrappedPlugin{
				originalScorePlugin: tt.originalScorePlugin,
				store:               s,
			}
			got, gotstatus := pl.Score(context.Background(), nil, tt.args.pod, tt.args.nodename)
			assert.Equal(t, tt.want, got)
			assert.Equal(t, tt.wantstatus, gotstatus)
		})
	}
}

func Test_wrappedPlugin_Score_WithPluginExtender(t *testing.T) {
	t.Parallel()

	type args struct {
		pod      *v1.Pod
		nodename string
	}
	tests := []struct {
		name              string
		prepareEachMockFn func(ctx context.Context, s *mock_plugin.MockStore, p *mock_plugin.MockScorePlugin, se *mock_plugin.MockScorePluginExtender, as args)
		args              args
		want              int64
		wantstatus        *framework.Status
	}{
		{
			name: "return AfterScore's results when Score is successful",
			prepareEachMockFn: func(ctx context.Context, s *mock_plugin.MockStore, p *mock_plugin.MockScorePlugin, se *mock_plugin.MockScorePluginExtender, as args) {
				success1 := framework.NewStatus(framework.Success, "BeforeScore returned")
				success2 := framework.NewStatus(framework.Success, "Score returned")
				success3 := framework.NewStatus(framework.Success, "AfterScore returned")
				se.EXPECT().BeforeScore(ctx, nil, as.pod, "node1").Return(int64(1111), success1)
				p.EXPECT().Score(ctx, nil, as.pod, "node1").Return(int64(2222), success2)
				se.EXPECT().AfterScore(ctx, nil, as.pod, "node1", int64(2222), success2).Return(int64(3333), success3)
				p.EXPECT().Name().Return("fakeScorePlugin").AnyTimes()
				s.EXPECT().AddScoreResult("default", "pod1", "node1", "fakeScorePlugin", int64(2222))
			},
			args: args{
				pod:      &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "pod1", Namespace: "default"}},
				nodename: "node1",
			},
			want:       3333,
			wantstatus: framework.NewStatus(framework.Success, "AfterScore returned"),
		},
		{
			name: "return AfterScore's results & does not call AddScoreResult after Score, if Score fails",
			prepareEachMockFn: func(ctx context.Context, s *mock_plugin.MockStore, p *mock_plugin.MockScorePlugin, se *mock_plugin.MockScorePluginExtender, as args) {
				success1 := framework.NewStatus(framework.Success, "BeforeScore returned")
				failure := framework.NewStatus(framework.Error, "Score returned")
				success3 := framework.NewStatus(framework.Success, "AfterScore returned")
				se.EXPECT().BeforeScore(ctx, nil, as.pod, "node1").Return(int64(1111), success1)
				p.EXPECT().Score(ctx, nil, as.pod, "node1").Return(int64(2222), failure)
				se.EXPECT().AfterScore(ctx, nil, as.pod, "node1", int64(2222), failure).Return(int64(3333), success3)
				p.EXPECT().Name().Return("fakeScorePlugin").AnyTimes()
				s.EXPECT().AddScoreResult(gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any(), gomock.Any()).Times(0)
			},
			args: args{
				pod:      &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "pod1", Namespace: "default"}},
				nodename: "node1",
			},
			want:       3333,
			wantstatus: framework.NewStatus(framework.Success, "AfterScore returned"),
		},
		{
			name: "return Before's results & does not call Score, if BeforeScore fails",
			prepareEachMockFn: func(ctx context.Context, _ *mock_plugin.MockStore, p *mock_plugin.MockScorePlugin, se *mock_plugin.MockScorePluginExtender, as args) {
				failure := framework.NewStatus(framework.Error, "BeforeScore returned")
				se.EXPECT().BeforeScore(ctx, nil, as.pod, "node1").Return(int64(1111), failure)
				p.EXPECT().Name().Return("fakeScorePlugin").AnyTimes()
			},
			args: args{
				pod:      &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "pod1", Namespace: "default"}},
				nodename: "node1",
			},
			want:       1111,
			wantstatus: framework.NewStatus(framework.Error, "BeforeScore returned"),
		},
		{
			name: "return AfterScore's results when AfterScore is fails",
			prepareEachMockFn: func(ctx context.Context, s *mock_plugin.MockStore, p *mock_plugin.MockScorePlugin, se *mock_plugin.MockScorePluginExtender, as args) {
				success1 := framework.NewStatus(framework.Success, "BeforeScore returned")
				success2 := framework.NewStatus(framework.Success, "Score returned")
				failure := framework.NewStatus(framework.Error, "AfterScore returned")
				se.EXPECT().BeforeScore(ctx, nil, as.pod, "node1").Return(int64(1111), success1)
				p.EXPECT().Score(ctx, nil, as.pod, "node1").Return(int64(2222), success2)
				se.EXPECT().AfterScore(ctx, nil, as.pod, "node1", int64(2222), success2).Return(int64(3333), failure)
				p.EXPECT().Name().Return("fakeScorePlugin").AnyTimes()
				s.EXPECT().AddScoreResult("default", "pod1", "node1", "fakeScorePlugin", int64(2222))
			},
			args: args{
				pod:      &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "pod1", Namespace: "default"}},
				nodename: "node1",
			},
			want:       3333,
			wantstatus: framework.NewStatus(framework.Error, "AfterScore returned"),
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			ctrl := gomock.NewController(t)
			s := mock_plugin.NewMockStore(ctrl)
			p := mock_plugin.NewMockScorePlugin(ctrl)
			se := mock_plugin.NewMockScorePluginExtender(ctrl)
			ctx := context.Background()
			tt.prepareEachMockFn(ctx, s, p, se, tt.args)
			pl, ok := NewWrappedPlugin(s, p, WithExtendersOption(func(_ SimulatorHandle) PluginExtenders {
				return PluginExtenders{
					ScorePluginExtender: se,
				}
			})).(*wrappedPlugin)
			if !ok { // should never happen
				t.Fatalf("Assert to wrapped plugin: %v", ok)
			}
			gotscore, gotstatus := pl.Score(ctx, nil, tt.args.pod, tt.args.nodename)
			assert.Equal(t, tt.want, gotscore)
			assert.Equal(t, tt.wantstatus, gotstatus)
		})
	}
}

func Test_wrappedPlugin_ScoreExtensions(t *testing.T) {
	t.Parallel()
	tests := []struct {
		name                string
		originalScorePlugin framework.ScorePlugin
		want                framework.ScoreExtensions
	}{
		{
			name:                "success",
			originalScorePlugin: fakeScorePlugin{},
			want: &wrappedPlugin{
				originalScorePlugin: fakeScorePlugin{},
			},
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			pl := &wrappedPlugin{
				originalScorePlugin: tt.originalScorePlugin,
			}
			got := pl.ScoreExtensions()
			assert.Equal(t, tt.want, got)
		})
	}
}

func Test_wrappedPlugin_PreScore(t *testing.T) {
	t.Parallel()

	testPod := &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "pod", Namespace: "namespace"}}
	testNodes := []*v1.Node{{ObjectMeta: metav1.ObjectMeta{Name: "node"}}}
	testNodeInfos := make([]*framework.NodeInfo, len(testNodes))
	for i, node := range testNodes {
		nodeInfo := &framework.NodeInfo{}
		nodeInfo.SetNode(node)
		testNodeInfos[i] = nodeInfo
	}

	tests := []struct {
		name           string
		prepareMocksFn func(s *mock_plugin.MockStore, se *mock_plugin.MockPreScorePlugin, extender *mock_plugin.MockPreScorePluginExtender)
		noExtender     bool
		want           *framework.Status
	}{
		{
			name: "happy with extender",
			prepareMocksFn: func(s *mock_plugin.MockStore, se *mock_plugin.MockPreScorePlugin, extender *mock_plugin.MockPreScorePluginExtender) {
				extender.EXPECT().BeforePreScore(gomock.Any(), gomock.Any(), testPod, testNodeInfos).Return(framework.NewStatus(framework.Success))
				se.EXPECT().PreScore(gomock.Any(), gomock.Any(), testPod, testNodeInfos).Return(framework.NewStatus(framework.Success))
				se.EXPECT().Name().Return("name")
				s.EXPECT().AddPreScoreResult("namespace", "pod", "name", resultstore.SuccessMessage)
				extender.EXPECT().AfterPreScore(gomock.Any(), gomock.Any(), testPod, testNodeInfos, framework.NewStatus(framework.Success)).Return(framework.NewStatus(framework.Success))
			},
			want: framework.NewStatus(framework.Success),
		},
		{
			name: "unhappy: BeforePreScore returns non-success",
			prepareMocksFn: func(_ *mock_plugin.MockStore, _ *mock_plugin.MockPreScorePlugin, extender *mock_plugin.MockPreScorePluginExtender) {
				extender.EXPECT().BeforePreScore(gomock.Any(), gomock.Any(), testPod, testNodeInfos).Return(framework.NewStatus(framework.Unschedulable))
			},
			want: framework.NewStatus(framework.Unschedulable),
		},
		{
			name: "unhappy: AfterPreScore returns non-success",
			prepareMocksFn: func(s *mock_plugin.MockStore, se *mock_plugin.MockPreScorePlugin, extender *mock_plugin.MockPreScorePluginExtender) {
				extender.EXPECT().BeforePreScore(gomock.Any(), gomock.Any(), testPod, testNodeInfos).Return(framework.NewStatus(framework.Success))
				se.EXPECT().PreScore(gomock.Any(), gomock.Any(), testPod, testNodeInfos).Return(framework.NewStatus(framework.Success))
				se.EXPECT().Name().Return("name")
				s.EXPECT().AddPreScoreResult("namespace", "pod", "name", resultstore.SuccessMessage)
				extender.EXPECT().AfterPreScore(gomock.Any(), gomock.Any(), testPod, testNodeInfos, framework.NewStatus(framework.Success)).Return(framework.NewStatus(framework.Unschedulable))
			},
			want: framework.NewStatus(framework.Unschedulable),
		},
		{
			name: "unhappy: PreScore and AfterPreScore return non-success",
			prepareMocksFn: func(s *mock_plugin.MockStore, se *mock_plugin.MockPreScorePlugin, extender *mock_plugin.MockPreScorePluginExtender) {
				extender.EXPECT().BeforePreScore(gomock.Any(), gomock.Any(), testPod, testNodeInfos).Return(framework.NewStatus(framework.Success))
				se.EXPECT().PreScore(gomock.Any(), gomock.Any(), testPod, testNodeInfos).Return(framework.NewStatus(framework.Unschedulable, "error"))
				se.EXPECT().Name().Return("name")
				s.EXPECT().AddPreScoreResult("namespace", "pod", "name", "error")
				extender.EXPECT().AfterPreScore(gomock.Any(), gomock.Any(), testPod, testNodeInfos, framework.NewStatus(framework.Unschedulable, "error")).Return(framework.NewStatus(framework.Unschedulable))
			},
			want: framework.NewStatus(framework.Unschedulable),
		},
		{
			name: "happy: PreScore returns non-success, but AfterPreScore return success",
			prepareMocksFn: func(s *mock_plugin.MockStore, se *mock_plugin.MockPreScorePlugin, extender *mock_plugin.MockPreScorePluginExtender) {
				extender.EXPECT().BeforePreScore(gomock.Any(), gomock.Any(), testPod, testNodeInfos).Return(framework.NewStatus(framework.Success))
				se.EXPECT().PreScore(gomock.Any(), gomock.Any(), testPod, testNodeInfos).Return(framework.NewStatus(framework.Unschedulable, "error"))
				se.EXPECT().Name().Return("name")
				s.EXPECT().AddPreScoreResult("namespace", "pod", "name", "error")
				extender.EXPECT().AfterPreScore(gomock.Any(), gomock.Any(), testPod, testNodeInfos, framework.NewStatus(framework.Unschedulable, "error")).Return(framework.NewStatus(framework.Success))
			},
			want: framework.NewStatus(framework.Success),
		},
		{
			name: "happy without extender",
			prepareMocksFn: func(s *mock_plugin.MockStore, se *mock_plugin.MockPreScorePlugin, _ *mock_plugin.MockPreScorePluginExtender) {
				se.EXPECT().PreScore(gomock.Any(), gomock.Any(), testPod, testNodeInfos).Return(framework.NewStatus(framework.Success))
				se.EXPECT().Name().Return("name")
				s.EXPECT().AddPreScoreResult("namespace", "pod", "name", resultstore.SuccessMessage)
			},
			noExtender: true,
			want:       framework.NewStatus(framework.Success),
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			ctrl := gomock.NewController(t)
			s := mock_plugin.NewMockStore(ctrl)
			p := mock_plugin.NewMockPreScorePlugin(ctrl)
			ex := mock_plugin.NewMockPreScorePluginExtender(ctrl)
			tt.prepareMocksFn(s, p, ex)

			w := &wrappedPlugin{
				store:                  s,
				originalPreScorePlugin: p,
			}
			if !tt.noExtender {
				w.preScorePluginExtender = ex
			}
			assert.Equalf(t, tt.want, w.PreScore(context.Background(), framework.NewCycleState(), testPod, testNodeInfos), "PreScore(ctx, cyclestate, %v, %v)", testPod, testNodes)
		})
	}
}

func Test_wrappedPlugin_PreFilter(t *testing.T) {
	t.Parallel()
	testPod := &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "pod", Namespace: "namespace"}}

	tests := []struct {
		name           string
		prepareMocksFn func(s *mock_plugin.MockStore, se *mock_plugin.MockPreFilterPlugin, extender *mock_plugin.MockPreFilterPluginExtender)
		noExtender     bool
		want           *framework.PreFilterResult
		want1          *framework.Status
	}{
		{
			name: "happy with extender",
			prepareMocksFn: func(s *mock_plugin.MockStore, se *mock_plugin.MockPreFilterPlugin, extender *mock_plugin.MockPreFilterPluginExtender) {
				extender.EXPECT().BeforePreFilter(gomock.Any(), gomock.Any(), testPod).Return(nil, framework.NewStatus(framework.Success))
				se.EXPECT().PreFilter(gomock.Any(), gomock.Any(), testPod).Return(&framework.PreFilterResult{NodeNames: sets.New("hoge")}, framework.NewStatus(framework.Success))
				se.EXPECT().Name().Return("name")
				s.EXPECT().AddPreFilterResult("namespace", "pod", "name", resultstore.SuccessMessage, &framework.PreFilterResult{NodeNames: sets.New("hoge")})
				extender.EXPECT().AfterPreFilter(gomock.Any(), gomock.Any(), testPod, &framework.PreFilterResult{NodeNames: sets.New("hoge")}, framework.NewStatus(framework.Success)).Return(&framework.PreFilterResult{NodeNames: sets.New("hoge")}, framework.NewStatus(framework.Success))
			},
			want:  &framework.PreFilterResult{NodeNames: sets.New("hoge")},
			want1: framework.NewStatus(framework.Success),
		},
		{
			name: "unhappy: BeforePreFilter returns non-success",
			prepareMocksFn: func(_ *mock_plugin.MockStore, _ *mock_plugin.MockPreFilterPlugin, extender *mock_plugin.MockPreFilterPluginExtender) {
				extender.EXPECT().BeforePreFilter(gomock.Any(), gomock.Any(), testPod).Return(&framework.PreFilterResult{NodeNames: sets.New("hoge")}, framework.NewStatus(framework.Unschedulable))
			},
			want:  &framework.PreFilterResult{NodeNames: sets.New("hoge")},
			want1: framework.NewStatus(framework.Unschedulable),
		},
		{
			name: "unhappy: AfterPreFilter returns non-success",
			prepareMocksFn: func(s *mock_plugin.MockStore, se *mock_plugin.MockPreFilterPlugin, extender *mock_plugin.MockPreFilterPluginExtender) {
				extender.EXPECT().BeforePreFilter(gomock.Any(), gomock.Any(), testPod).Return(nil, framework.NewStatus(framework.Success))
				se.EXPECT().PreFilter(gomock.Any(), gomock.Any(), testPod).Return(&framework.PreFilterResult{NodeNames: sets.New("hoge")}, framework.NewStatus(framework.Success))
				se.EXPECT().Name().Return("name")
				s.EXPECT().AddPreFilterResult("namespace", "pod", "name", resultstore.SuccessMessage, &framework.PreFilterResult{NodeNames: sets.New("hoge")})
				extender.EXPECT().AfterPreFilter(gomock.Any(), gomock.Any(), testPod, &framework.PreFilterResult{NodeNames: sets.New("hoge")}, framework.NewStatus(framework.Success)).Return(&framework.PreFilterResult{NodeNames: sets.New("hoge")}, framework.NewStatus(framework.Unschedulable))
			},
			want:  &framework.PreFilterResult{NodeNames: sets.New("hoge")},
			want1: framework.NewStatus(framework.Unschedulable),
		},
		{
			name: "unhappy: PreFilter and AfterPreFilter return non-success",
			prepareMocksFn: func(s *mock_plugin.MockStore, se *mock_plugin.MockPreFilterPlugin, extender *mock_plugin.MockPreFilterPluginExtender) {
				extender.EXPECT().BeforePreFilter(gomock.Any(), gomock.Any(), testPod).Return(nil, framework.NewStatus(framework.Success))
				se.EXPECT().PreFilter(gomock.Any(), gomock.Any(), testPod).Return(&framework.PreFilterResult{NodeNames: sets.New("hoge")}, framework.NewStatus(framework.Unschedulable, "error"))
				se.EXPECT().Name().Return("name")
				s.EXPECT().AddPreFilterResult("namespace", "pod", "name", "error", &framework.PreFilterResult{NodeNames: sets.New("hoge")})
				extender.EXPECT().AfterPreFilter(gomock.Any(), gomock.Any(), testPod, &framework.PreFilterResult{NodeNames: sets.New("hoge")}, framework.NewStatus(framework.Unschedulable, "error")).Return(&framework.PreFilterResult{NodeNames: sets.New("hoge")}, framework.NewStatus(framework.Unschedulable))
			},
			want:  &framework.PreFilterResult{NodeNames: sets.New("hoge")},
			want1: framework.NewStatus(framework.Unschedulable),
		},
		{
			name: "happy: PreFilter returns non-success, but AfterPreFilter return success",
			prepareMocksFn: func(s *mock_plugin.MockStore, se *mock_plugin.MockPreFilterPlugin, extender *mock_plugin.MockPreFilterPluginExtender) {
				extender.EXPECT().BeforePreFilter(gomock.Any(), gomock.Any(), testPod).Return(nil, framework.NewStatus(framework.Success))
				se.EXPECT().PreFilter(gomock.Any(), gomock.Any(), testPod).Return(&framework.PreFilterResult{NodeNames: sets.New("hoge")}, framework.NewStatus(framework.Unschedulable, "error"))
				se.EXPECT().Name().Return("name")
				s.EXPECT().AddPreFilterResult("namespace", "pod", "name", "error", &framework.PreFilterResult{NodeNames: sets.New("hoge")})
				extender.EXPECT().AfterPreFilter(gomock.Any(), gomock.Any(), testPod, &framework.PreFilterResult{NodeNames: sets.New("hoge")}, framework.NewStatus(framework.Unschedulable, "error")).Return(&framework.PreFilterResult{NodeNames: sets.New("hoge2")}, framework.NewStatus(framework.Success))
			},
			want:  &framework.PreFilterResult{NodeNames: sets.New("hoge2")},
			want1: framework.NewStatus(framework.Success),
		},
		{
			name: "happy without extender",
			prepareMocksFn: func(s *mock_plugin.MockStore, se *mock_plugin.MockPreFilterPlugin, _ *mock_plugin.MockPreFilterPluginExtender) {
				se.EXPECT().PreFilter(gomock.Any(), gomock.Any(), testPod).Return(&framework.PreFilterResult{NodeNames: sets.New("hoge")}, framework.NewStatus(framework.Success))
				se.EXPECT().Name().Return("name")
				s.EXPECT().AddPreFilterResult("namespace", "pod", "name", resultstore.SuccessMessage, &framework.PreFilterResult{NodeNames: sets.New("hoge")})
			},
			noExtender: true,
			want:       &framework.PreFilterResult{NodeNames: sets.New("hoge")},
			want1:      framework.NewStatus(framework.Success),
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			ctrl := gomock.NewController(t)
			s := mock_plugin.NewMockStore(ctrl)
			p := mock_plugin.NewMockPreFilterPlugin(ctrl)
			ex := mock_plugin.NewMockPreFilterPluginExtender(ctrl)
			tt.prepareMocksFn(s, p, ex)

			w := &wrappedPlugin{
				store:                   s,
				originalPreFilterPlugin: p,
			}

			if !tt.noExtender {
				w.preFilterPluginExtender = ex
			}

			got, got1 := w.PreFilter(context.Background(), framework.NewCycleState(), testPod)
			assert.Equalf(t, tt.want, got, "PreFilter(ctx, cyclestate, %v)", testPod)
			assert.Equalf(t, tt.want1, got1, "PreFilter(ctx, cyclestate, %v)", testPod)
		})
	}
}

func Test_wrappedPlugin_Permit(t *testing.T) {
	t.Parallel()
	testPod := &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "pod", Namespace: "namespace"}}
	testNodeName := "node"

	tests := []struct {
		name           string
		prepareMocksFn func(s *mock_plugin.MockStore, se *mock_plugin.MockPermitPlugin, extender *mock_plugin.MockPermitPluginExtender)
		noExtender     bool
		want           *framework.Status
		want1          time.Duration
	}{
		{
			name: "happy with extender",
			prepareMocksFn: func(s *mock_plugin.MockStore, se *mock_plugin.MockPermitPlugin, extender *mock_plugin.MockPermitPluginExtender) {
				extender.EXPECT().BeforePermit(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Success), time.Duration(1))
				se.EXPECT().Permit(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Success), time.Duration(1))
				se.EXPECT().Name().Return("name")
				s.EXPECT().AddPermitResult("namespace", "pod", "name", resultstore.SuccessMessage, time.Duration(1))
				extender.EXPECT().AfterPermit(gomock.Any(), gomock.Any(), testPod, testNodeName, framework.NewStatus(framework.Success), time.Duration(1)).Return(framework.NewStatus(framework.Success), time.Duration(1))
			},
			want:  framework.NewStatus(framework.Success),
			want1: time.Duration(1),
		},
		{
			name: "unhappy: BeforePermit returns non-success",
			prepareMocksFn: func(_ *mock_plugin.MockStore, _ *mock_plugin.MockPermitPlugin, extender *mock_plugin.MockPermitPluginExtender) {
				extender.EXPECT().BeforePermit(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Unschedulable), time.Duration(1))
			},
			want:  framework.NewStatus(framework.Unschedulable),
			want1: time.Duration(1),
		},
		{
			name: "unhappy: AfterPermit returns non-success",
			prepareMocksFn: func(s *mock_plugin.MockStore, se *mock_plugin.MockPermitPlugin, extender *mock_plugin.MockPermitPluginExtender) {
				extender.EXPECT().BeforePermit(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Success), time.Duration(1))
				se.EXPECT().Permit(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Success), time.Duration(1))
				se.EXPECT().Name().Return("name")
				s.EXPECT().AddPermitResult("namespace", "pod", "name", resultstore.SuccessMessage, time.Duration(1))
				extender.EXPECT().AfterPermit(gomock.Any(), gomock.Any(), testPod, testNodeName, framework.NewStatus(framework.Success), time.Duration(1)).Return(framework.NewStatus(framework.Unschedulable), time.Duration(2))
			},
			want:  framework.NewStatus(framework.Unschedulable),
			want1: time.Duration(2),
		},
		{
			name: "unhappy: Permit and AfterPermit return non-success",
			prepareMocksFn: func(s *mock_plugin.MockStore, se *mock_plugin.MockPermitPlugin, extender *mock_plugin.MockPermitPluginExtender) {
				extender.EXPECT().BeforePermit(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Success), time.Duration(1))
				se.EXPECT().Permit(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Unschedulable, "error"), time.Duration(1))
				se.EXPECT().Name().Return("name")
				s.EXPECT().AddPermitResult("namespace", "pod", "name", "error", time.Duration(1))
				extender.EXPECT().AfterPermit(gomock.Any(), gomock.Any(), testPod, testNodeName, framework.NewStatus(framework.Unschedulable, "error"), time.Duration(1)).Return(framework.NewStatus(framework.Unschedulable), time.Duration(2))
			},
			want:  framework.NewStatus(framework.Unschedulable),
			want1: time.Duration(2),
		},
		{
			name: "happy: Permit returns non-success, but AfterPermit return success",
			prepareMocksFn: func(s *mock_plugin.MockStore, se *mock_plugin.MockPermitPlugin, extender *mock_plugin.MockPermitPluginExtender) {
				extender.EXPECT().BeforePermit(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Success), time.Duration(1))
				se.EXPECT().Permit(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Unschedulable, "error"), time.Duration(1))
				se.EXPECT().Name().Return("name")
				s.EXPECT().AddPermitResult("namespace", "pod", "name", "error", time.Duration(1))
				extender.EXPECT().AfterPermit(gomock.Any(), gomock.Any(), testPod, testNodeName, framework.NewStatus(framework.Unschedulable, "error"), time.Duration(1)).Return(framework.NewStatus(framework.Success), time.Duration(2))
			},
			want:  framework.NewStatus(framework.Success),
			want1: time.Duration(2),
		},
		{
			name: "happy without extender",
			prepareMocksFn: func(s *mock_plugin.MockStore, se *mock_plugin.MockPermitPlugin, _ *mock_plugin.MockPermitPluginExtender) {
				se.EXPECT().Permit(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Success), time.Duration(1))
				se.EXPECT().Name().Return("name")
				s.EXPECT().AddPermitResult("namespace", "pod", "name", resultstore.SuccessMessage, time.Duration(1))
			},
			noExtender: true,
			want:       framework.NewStatus(framework.Success),
			want1:      time.Duration(1),
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			ctrl := gomock.NewController(t)
			s := mock_plugin.NewMockStore(ctrl)
			p := mock_plugin.NewMockPermitPlugin(ctrl)
			ex := mock_plugin.NewMockPermitPluginExtender(ctrl)
			tt.prepareMocksFn(s, p, ex)

			w := &wrappedPlugin{
				store:                s,
				originalPermitPlugin: p,
			}
			if !tt.noExtender {
				w.permitPluginExtender = ex
			}
			got, got1 := w.Permit(context.Background(), framework.NewCycleState(), testPod, testNodeName)
			assert.Equalf(t, tt.want, got, "Permit(ctx, cyclestate, %v, %v)", testPod, testNodeName)
			assert.Equalf(t, tt.want1, got1, "Permit(ctx, cyclestate, %v, %v)", testPod, testNodeName)
		})
	}
}

func Test_wrappedPlugin_Reserve(t *testing.T) {
	t.Parallel()
	testPod := &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "pod", Namespace: "namespace"}}
	testNodeName := "node"

	tests := []struct {
		name           string
		prepareMocksFn func(s *mock_plugin.MockStore, se *mock_plugin.MockReservePlugin, extender *mock_plugin.MockReservePluginExtender)
		want           *framework.Status
		noExtender     bool
	}{
		{
			name: "happy with extnder",
			prepareMocksFn: func(s *mock_plugin.MockStore, se *mock_plugin.MockReservePlugin, extender *mock_plugin.MockReservePluginExtender) {
				s.EXPECT().AddSelectedNode("namespace", "pod", "node")
				extender.EXPECT().BeforeReserve(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Success))
				se.EXPECT().Reserve(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Success))
				se.EXPECT().Name().Return("name")
				s.EXPECT().AddReserveResult("namespace", "pod", "name", resultstore.SuccessMessage)
				extender.EXPECT().AfterReserve(gomock.Any(), gomock.Any(), testPod, testNodeName, framework.NewStatus(framework.Success)).Return(framework.NewStatus(framework.Success))
			},
			want: framework.NewStatus(framework.Success),
		},
		{
			name: "unhappy: BeforeReserve returns non-success",
			prepareMocksFn: func(s *mock_plugin.MockStore, _ *mock_plugin.MockReservePlugin, extender *mock_plugin.MockReservePluginExtender) {
				s.EXPECT().AddSelectedNode("namespace", "pod", "node")
				extender.EXPECT().BeforeReserve(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Unschedulable))
			},
			want: framework.NewStatus(framework.Unschedulable),
		},
		{
			name: "unhappy: AfterReserve returns non-success",
			prepareMocksFn: func(s *mock_plugin.MockStore, se *mock_plugin.MockReservePlugin, extender *mock_plugin.MockReservePluginExtender) {
				s.EXPECT().AddSelectedNode("namespace", "pod", "node")
				extender.EXPECT().BeforeReserve(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Success))
				se.EXPECT().Reserve(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Success))
				se.EXPECT().Name().Return("name")
				s.EXPECT().AddReserveResult("namespace", "pod", "name", resultstore.SuccessMessage)
				extender.EXPECT().AfterReserve(gomock.Any(), gomock.Any(), testPod, testNodeName, framework.NewStatus(framework.Success)).Return(framework.NewStatus(framework.Unschedulable))
			},
			want: framework.NewStatus(framework.Unschedulable),
		},
		{
			name: "unhappy: Reserve and AfterReserve return non-success",
			prepareMocksFn: func(s *mock_plugin.MockStore, se *mock_plugin.MockReservePlugin, extender *mock_plugin.MockReservePluginExtender) {
				s.EXPECT().AddSelectedNode("namespace", "pod", "node")
				extender.EXPECT().BeforeReserve(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Success))
				se.EXPECT().Reserve(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Unschedulable, "error"))
				se.EXPECT().Name().Return("name")
				s.EXPECT().AddReserveResult("namespace", "pod", "name", "error")
				extender.EXPECT().AfterReserve(gomock.Any(), gomock.Any(), testPod, testNodeName, framework.NewStatus(framework.Unschedulable, "error")).Return(framework.NewStatus(framework.Unschedulable))
			},
			want: framework.NewStatus(framework.Unschedulable),
		},
		{
			name: "happy: Reserve returns non-success, but AfterReserve return success",
			prepareMocksFn: func(s *mock_plugin.MockStore, se *mock_plugin.MockReservePlugin, extender *mock_plugin.MockReservePluginExtender) {
				s.EXPECT().AddSelectedNode("namespace", "pod", "node")
				extender.EXPECT().BeforeReserve(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Success))
				se.EXPECT().Reserve(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Unschedulable, "error"))
				se.EXPECT().Name().Return("name")
				s.EXPECT().AddReserveResult("namespace", "pod", "name", "error")
				extender.EXPECT().AfterReserve(gomock.Any(), gomock.Any(), testPod, testNodeName, framework.NewStatus(framework.Unschedulable, "error")).Return(framework.NewStatus(framework.Success))
			},
			want: framework.NewStatus(framework.Success),
		},
		{
			name: "happy without extnder",
			prepareMocksFn: func(s *mock_plugin.MockStore, se *mock_plugin.MockReservePlugin, _ *mock_plugin.MockReservePluginExtender) {
				s.EXPECT().AddSelectedNode("namespace", "pod", "node")
				se.EXPECT().Reserve(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Success))
				se.EXPECT().Name().Return("name")
				s.EXPECT().AddReserveResult("namespace", "pod", "name", resultstore.SuccessMessage)
			},
			noExtender: true,
			want:       framework.NewStatus(framework.Success),
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			ctrl := gomock.NewController(t)
			s := mock_plugin.NewMockStore(ctrl)
			p := mock_plugin.NewMockReservePlugin(ctrl)
			ex := mock_plugin.NewMockReservePluginExtender(ctrl)
			tt.prepareMocksFn(s, p, ex)

			w := &wrappedPlugin{
				store:                 s,
				originalReservePlugin: p,
			}
			if !tt.noExtender {
				w.reservePluginExtender = ex
			}
			assert.Equalf(t, tt.want, w.Reserve(context.Background(), framework.NewCycleState(), testPod, testNodeName), "Reserve(ctx, cyclestate, %v, %v)", testPod, testNodeName)
		})
	}
}

func Test_wrappedPlugin_Unreserve(t *testing.T) {
	t.Parallel()
	testPod := &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "pod", Namespace: "namespace"}}
	testNodeName := "node"

	tests := []struct {
		name           string
		prepareMocksFn func(s *mock_plugin.MockStore, se *mock_plugin.MockReservePlugin, extender *mock_plugin.MockReservePluginExtender)
		noExtender     bool
	}{
		{
			name: "happy with extender",
			prepareMocksFn: func(_ *mock_plugin.MockStore, se *mock_plugin.MockReservePlugin, extender *mock_plugin.MockReservePluginExtender) {
				extender.EXPECT().BeforeUnreserve(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Success))
				se.EXPECT().Unreserve(gomock.Any(), gomock.Any(), testPod, testNodeName)
				extender.EXPECT().AfterUnreserve(gomock.Any(), gomock.Any(), testPod, testNodeName)
			},
		},
		{
			name: "unhappy: BeforeUnreserve returns non-success",
			prepareMocksFn: func(_ *mock_plugin.MockStore, se *mock_plugin.MockReservePlugin, extender *mock_plugin.MockReservePluginExtender) {
				extender.EXPECT().BeforeUnreserve(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Unschedulable))
				se.EXPECT().Name().Return("hoge")
			},
		},
		{
			name: "happy without extender",
			prepareMocksFn: func(_ *mock_plugin.MockStore, se *mock_plugin.MockReservePlugin, _ *mock_plugin.MockReservePluginExtender) {
				se.EXPECT().Unreserve(gomock.Any(), gomock.Any(), testPod, testNodeName)
			},
			noExtender: true,
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			ctrl := gomock.NewController(t)
			s := mock_plugin.NewMockStore(ctrl)
			p := mock_plugin.NewMockReservePlugin(ctrl)
			ex := mock_plugin.NewMockReservePluginExtender(ctrl)
			tt.prepareMocksFn(s, p, ex)

			w := &wrappedPlugin{
				store:                 s,
				originalReservePlugin: p,
			}
			if !tt.noExtender {
				w.reservePluginExtender = ex
			}
			w.Unreserve(context.Background(), framework.NewCycleState(), testPod, testNodeName)
		})
	}
}

func Test_wrappedPlugin_PreBind(t *testing.T) {
	t.Parallel()
	testPod := &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "pod", Namespace: "namespace"}}
	testNodeName := "node"

	tests := []struct {
		name           string
		prepareMocksFn func(s *mock_plugin.MockStore, se *mock_plugin.MockPreBindPlugin, extender *mock_plugin.MockPreBindPluginExtender)
		want           *framework.Status
		noExtender     bool
	}{
		{
			name: "happy with extnder",
			prepareMocksFn: func(s *mock_plugin.MockStore, se *mock_plugin.MockPreBindPlugin, extender *mock_plugin.MockPreBindPluginExtender) {
				extender.EXPECT().BeforePreBind(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Success))
				se.EXPECT().PreBind(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Success))
				se.EXPECT().Name().Return("name")
				s.EXPECT().AddPreBindResult("namespace", "pod", "name", resultstore.SuccessMessage)
				extender.EXPECT().AfterPreBind(gomock.Any(), gomock.Any(), testPod, testNodeName, framework.NewStatus(framework.Success)).Return(framework.NewStatus(framework.Success))
			},
			want: framework.NewStatus(framework.Success),
		},
		{
			name: "unhappy: BeforePreBind returns non-success",
			prepareMocksFn: func(_ *mock_plugin.MockStore, _ *mock_plugin.MockPreBindPlugin, extender *mock_plugin.MockPreBindPluginExtender) {
				extender.EXPECT().BeforePreBind(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Unschedulable))
			},
			want: framework.NewStatus(framework.Unschedulable),
		},
		{
			name: "unhappy: AfterPreBind returns non-success",
			prepareMocksFn: func(s *mock_plugin.MockStore, se *mock_plugin.MockPreBindPlugin, extender *mock_plugin.MockPreBindPluginExtender) {
				extender.EXPECT().BeforePreBind(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Success))
				se.EXPECT().PreBind(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Success))
				se.EXPECT().Name().Return("name")
				s.EXPECT().AddPreBindResult("namespace", "pod", "name", resultstore.SuccessMessage)
				extender.EXPECT().AfterPreBind(gomock.Any(), gomock.Any(), testPod, testNodeName, framework.NewStatus(framework.Success)).Return(framework.NewStatus(framework.Unschedulable))
			},
			want: framework.NewStatus(framework.Unschedulable),
		},
		{
			name: "unhappy: PreBind and AfterPreBind return non-success",
			prepareMocksFn: func(s *mock_plugin.MockStore, se *mock_plugin.MockPreBindPlugin, extender *mock_plugin.MockPreBindPluginExtender) {
				extender.EXPECT().BeforePreBind(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Success))
				se.EXPECT().PreBind(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Unschedulable, "error"))
				se.EXPECT().Name().Return("name")
				s.EXPECT().AddPreBindResult("namespace", "pod", "name", "error")
				extender.EXPECT().AfterPreBind(gomock.Any(), gomock.Any(), testPod, testNodeName, framework.NewStatus(framework.Unschedulable, "error")).Return(framework.NewStatus(framework.Unschedulable))
			},
			want: framework.NewStatus(framework.Unschedulable),
		},
		{
			name: "happy: PreBind returns non-success, but AfterPreBind return success",
			prepareMocksFn: func(s *mock_plugin.MockStore, se *mock_plugin.MockPreBindPlugin, extender *mock_plugin.MockPreBindPluginExtender) {
				extender.EXPECT().BeforePreBind(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Success))
				se.EXPECT().PreBind(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Unschedulable, "error"))
				se.EXPECT().Name().Return("name")
				s.EXPECT().AddPreBindResult("namespace", "pod", "name", "error")
				extender.EXPECT().AfterPreBind(gomock.Any(), gomock.Any(), testPod, testNodeName, framework.NewStatus(framework.Unschedulable, "error")).Return(framework.NewStatus(framework.Success))
			},
			want: framework.NewStatus(framework.Success),
		},
		{
			name: "happy without extnder",
			prepareMocksFn: func(s *mock_plugin.MockStore, se *mock_plugin.MockPreBindPlugin, _ *mock_plugin.MockPreBindPluginExtender) {
				se.EXPECT().PreBind(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Success))
				se.EXPECT().Name().Return("name")
				s.EXPECT().AddPreBindResult("namespace", "pod", "name", resultstore.SuccessMessage)
			},
			noExtender: true,
			want:       framework.NewStatus(framework.Success),
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			ctrl := gomock.NewController(t)
			s := mock_plugin.NewMockStore(ctrl)
			p := mock_plugin.NewMockPreBindPlugin(ctrl)
			ex := mock_plugin.NewMockPreBindPluginExtender(ctrl)
			tt.prepareMocksFn(s, p, ex)

			w := &wrappedPlugin{
				store:                 s,
				originalPreBindPlugin: p,
			}
			if !tt.noExtender {
				w.preBindPluginExtender = ex
			}
			assert.Equalf(t, tt.want, w.PreBind(context.Background(), framework.NewCycleState(), testPod, testNodeName), "PreBind(ctx, cyclestate, %v, %v)", testPod, testNodeName)
		})
	}
}

func Test_wrappedPlugin_Bind(t *testing.T) {
	t.Parallel()
	testPod := &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "pod", Namespace: "namespace"}}
	testNodeName := "node"

	tests := []struct {
		name           string
		prepareMocksFn func(s *mock_plugin.MockStore, se *mock_plugin.MockBindPlugin, extender *mock_plugin.MockBindPluginExtender)
		want           *framework.Status
		noExtender     bool
	}{
		{
			name: "happy with extnder",
			prepareMocksFn: func(s *mock_plugin.MockStore, se *mock_plugin.MockBindPlugin, extender *mock_plugin.MockBindPluginExtender) {
				extender.EXPECT().BeforeBind(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Success))
				se.EXPECT().Bind(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Success))
				se.EXPECT().Name().Return("name")
				s.EXPECT().AddBindResult("namespace", "pod", "name", resultstore.SuccessMessage)
				extender.EXPECT().AfterBind(gomock.Any(), gomock.Any(), testPod, testNodeName, framework.NewStatus(framework.Success)).Return(framework.NewStatus(framework.Success))
			},
			want: framework.NewStatus(framework.Success),
		},
		{
			name: "unhappy: BeforeBind returns non-success",
			prepareMocksFn: func(_ *mock_plugin.MockStore, _ *mock_plugin.MockBindPlugin, extender *mock_plugin.MockBindPluginExtender) {
				extender.EXPECT().BeforeBind(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Unschedulable))
			},
			want: framework.NewStatus(framework.Unschedulable),
		},
		{
			name: "unhappy: AfterBind returns non-success",
			prepareMocksFn: func(s *mock_plugin.MockStore, se *mock_plugin.MockBindPlugin, extender *mock_plugin.MockBindPluginExtender) {
				extender.EXPECT().BeforeBind(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Success))
				se.EXPECT().Bind(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Success))
				se.EXPECT().Name().Return("name")
				s.EXPECT().AddBindResult("namespace", "pod", "name", resultstore.SuccessMessage)
				extender.EXPECT().AfterBind(gomock.Any(), gomock.Any(), testPod, testNodeName, framework.NewStatus(framework.Success)).Return(framework.NewStatus(framework.Unschedulable))
			},
			want: framework.NewStatus(framework.Unschedulable),
		},
		{
			name: "unhappy: Bind and AfterBind return non-success",
			prepareMocksFn: func(s *mock_plugin.MockStore, se *mock_plugin.MockBindPlugin, extender *mock_plugin.MockBindPluginExtender) {
				extender.EXPECT().BeforeBind(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Success))
				se.EXPECT().Bind(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Unschedulable, "error"))
				se.EXPECT().Name().Return("name")
				s.EXPECT().AddBindResult("namespace", "pod", "name", "error")
				extender.EXPECT().AfterBind(gomock.Any(), gomock.Any(), testPod, testNodeName, framework.NewStatus(framework.Unschedulable, "error")).Return(framework.NewStatus(framework.Unschedulable))
			},
			want: framework.NewStatus(framework.Unschedulable),
		},
		{
			name: "happy: Bind returns non-success, but AfterBind return success",
			prepareMocksFn: func(s *mock_plugin.MockStore, se *mock_plugin.MockBindPlugin, extender *mock_plugin.MockBindPluginExtender) {
				extender.EXPECT().BeforeBind(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Success))
				se.EXPECT().Bind(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Unschedulable, "error"))
				se.EXPECT().Name().Return("name")
				s.EXPECT().AddBindResult("namespace", "pod", "name", "error")
				extender.EXPECT().AfterBind(gomock.Any(), gomock.Any(), testPod, testNodeName, framework.NewStatus(framework.Unschedulable, "error")).Return(framework.NewStatus(framework.Success))
			},
			want: framework.NewStatus(framework.Success),
		},
		{
			name: "happy without extnder",
			prepareMocksFn: func(s *mock_plugin.MockStore, se *mock_plugin.MockBindPlugin, _ *mock_plugin.MockBindPluginExtender) {
				se.EXPECT().Bind(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Success))
				se.EXPECT().Name().Return("name")
				s.EXPECT().AddBindResult("namespace", "pod", "name", resultstore.SuccessMessage)
			},
			noExtender: true,
			want:       framework.NewStatus(framework.Success),
		},
		{
			name:       "unhappy: it is not bind plugin",
			noExtender: true,
			want:       framework.NewStatus(framework.Skip, "called wrapped bind plugin is nil"),
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			ctrl := gomock.NewController(t)

			w := &wrappedPlugin{}
			if tt.prepareMocksFn != nil {
				s := mock_plugin.NewMockStore(ctrl)
				p := mock_plugin.NewMockBindPlugin(ctrl)
				ex := mock_plugin.NewMockBindPluginExtender(ctrl)
				tt.prepareMocksFn(s, p, ex)
				w = &wrappedPlugin{
					store:              s,
					originalBindPlugin: p,
				}
				if !tt.noExtender {
					w.bindPluginExtender = ex
				}
			}

			assert.Equalf(t, tt.want, w.Bind(context.Background(), framework.NewCycleState(), testPod, testNodeName), "Bind(ctx, cyclestate, %v, %v)", testPod, testNodeName)
		})
	}
}

func Test_wrappedPlugin_PostBind(t *testing.T) {
	t.Parallel()
	testPod := &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: "pod", Namespace: "namespace"}}
	testNodeName := "node"

	tests := []struct {
		name           string
		prepareMocksFn func(s *mock_plugin.MockStore, se *mock_plugin.MockPostBindPlugin, extender *mock_plugin.MockPostBindPluginExtender)
		noExtender     bool
	}{
		{
			name: "happy with extender",
			prepareMocksFn: func(_ *mock_plugin.MockStore, se *mock_plugin.MockPostBindPlugin, extender *mock_plugin.MockPostBindPluginExtender) {
				extender.EXPECT().BeforePostBind(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Success))
				se.EXPECT().PostBind(gomock.Any(), gomock.Any(), testPod, testNodeName)
				extender.EXPECT().AfterPostBind(gomock.Any(), gomock.Any(), testPod, testNodeName)
			},
		},
		{
			name: "unhappy: BeforePostBind returns non-success",
			prepareMocksFn: func(_ *mock_plugin.MockStore, se *mock_plugin.MockPostBindPlugin, extender *mock_plugin.MockPostBindPluginExtender) {
				extender.EXPECT().BeforePostBind(gomock.Any(), gomock.Any(), testPod, testNodeName).Return(framework.NewStatus(framework.Unschedulable))
				se.EXPECT().Name().Return("hoge")
			},
		},
		{
			name: "happy without extender",
			prepareMocksFn: func(_ *mock_plugin.MockStore, se *mock_plugin.MockPostBindPlugin, _ *mock_plugin.MockPostBindPluginExtender) {
				se.EXPECT().PostBind(gomock.Any(), gomock.Any(), testPod, testNodeName)
			},
			noExtender: true,
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			ctrl := gomock.NewController(t)
			s := mock_plugin.NewMockStore(ctrl)
			p := mock_plugin.NewMockPostBindPlugin(ctrl)
			ex := mock_plugin.NewMockPostBindPluginExtender(ctrl)
			tt.prepareMocksFn(s, p, ex)

			w := &wrappedPlugin{
				store:                  s,
				originalPostBindPlugin: p,
			}
			if !tt.noExtender {
				w.postBindPluginExtender = ex
			}
			w.PostBind(context.Background(), framework.NewCycleState(), testPod, testNodeName)
		})
	}
}

// fake plugins for test

type fakeFilterPlugin struct{}

func (fakeFilterPlugin) Name() string { return "fakeFilterPlugin" }
func (fakeFilterPlugin) Filter(_ context.Context, _ *framework.CycleState, _ *v1.Pod, _ *framework.NodeInfo) *framework.Status {
	return nil
}

type fakePostFilterPlugin struct{}

func (fakePostFilterPlugin) Name() string { return "fakePostFilterPlugin" }
func (fakePostFilterPlugin) PostFilter(_ context.Context, _ *framework.CycleState, _ *v1.Pod, _ framework.NodeToStatusMap) (*framework.PostFilterResult, *framework.Status) {
	return &framework.PostFilterResult{NominatingInfo: &framework.NominatingInfo{NominatedNodeName: "node1"}}, framework.NewStatus(framework.Success, "postfilter success")
}

type fakeScorePlugin struct{}

func (fakeScorePlugin) Name() string { return "fakeScorePlugin" }
func (pl fakeScorePlugin) ScoreExtensions() framework.ScoreExtensions {
	return pl
}

func (fakeScorePlugin) NormalizeScore(_ context.Context, _ *framework.CycleState, _ *v1.Pod, _ framework.NodeScoreList) *framework.Status {
	return nil
}

func (fakeScorePlugin) Score(_ context.Context, _ *framework.CycleState, _ *v1.Pod, _ string) (int64, *framework.Status) {
	return 1, nil
}

type fakeWrappedPlugin struct{}

func (fakeWrappedPlugin) Name() string { return "fakeWrappedPlugin" }
func (fakeWrappedPlugin) Filter(_ context.Context, _ *framework.CycleState, _ *v1.Pod, _ *framework.NodeInfo) *framework.Status {
	return nil
}

func (fakeWrappedPlugin) PostFilter(_ context.Context, _ *framework.CycleState, _ *v1.Pod, _ framework.NodeToStatusMap) (*framework.PostFilterResult, *framework.Status) {
	return new(framework.PostFilterResult), nil
}

func (pl fakeWrappedPlugin) ScoreExtensions() framework.ScoreExtensions {
	return pl
}

func (fakeWrappedPlugin) NormalizeScore(_ context.Context, _ *framework.CycleState, _ *v1.Pod, _ framework.NodeScoreList) *framework.Status {
	return nil
}

func (fakeWrappedPlugin) Score(_ context.Context, _ *framework.CycleState, _ *v1.Pod, _ string) (int64, *framework.Status) {
	return 0, nil
}

// all method on this plugin will fail.
type fakeMustFailWrappedPlugin struct{}

var (
	errFilter    = errors.New("filter failed")
	errPost      = errors.New("postFilter failed")
	errNormalize = errors.New("normalize failed")
	errScore     = errors.New("score failed")
)

func (fakeMustFailWrappedPlugin) Name() string { return "fakeMustFailWrappedPlugin" }
func (fakeMustFailWrappedPlugin) Filter(_ context.Context, _ *framework.CycleState, _ *v1.Pod, _ *framework.NodeInfo) *framework.Status {
	return framework.AsStatus(errFilter)
}

func (fakeMustFailWrappedPlugin) PostFilter(_ context.Context, _ *framework.CycleState, _ *v1.Pod, _ framework.NodeToStatusMap) (*framework.PostFilterResult, *framework.Status) {
	return nil, framework.AsStatus(errPost)
}

func (pl fakeMustFailWrappedPlugin) ScoreExtensions() framework.ScoreExtensions {
	return pl
}

func (fakeMustFailWrappedPlugin) NormalizeScore(_ context.Context, _ *framework.CycleState, _ *v1.Pod, _ framework.NodeScoreList) *framework.Status {
	return framework.AsStatus(errNormalize)
}

func (fakeMustFailWrappedPlugin) Score(_ context.Context, _ *framework.CycleState, _ *v1.Pod, _ string) (int64, *framework.Status) {
	return 0, framework.AsStatus(errScore)
}
</file>

<file path="kube-scheduler-simulator/simulator/scheduler/plugin/wrappedplugin.go">
package plugin

import (
	"context"
	"time"

	v1 "k8s.io/api/core/v1"
	"k8s.io/klog/v2"
	"k8s.io/kubernetes/pkg/scheduler/framework"

	schedulingresultstore "sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler/plugin/resultstore"
)

//go:generate mockgen -destination=./mock/$GOFILE -package=plugin . Store,PreFilterPluginExtender,FilterPluginExtender,PostFilterPluginExtender,PreScorePluginExtender,ScorePluginExtender,NormalizeScorePluginExtender,ReservePluginExtender,PermitPluginExtender,PreBindPluginExtender,BindPluginExtender,PostBindPluginExtender
//go:generate mockgen -destination=./mock/framework.go -package=plugin k8s.io/kubernetes/pkg/scheduler/framework PreFilterPlugin,FilterPlugin,PostFilterPlugin,PreScorePlugin,ScorePlugin,ScoreExtensions,PermitPlugin,BindPlugin,PreBindPlugin,PostBindPlugin,ReservePlugin
type Store interface {
	AddNormalizedScoreResult(namespace, podName, nodeName, pluginName string, normalizedscore int64)
	AddPreFilterResult(namespace, podName, pluginName, reason string, preFilterResult *framework.PreFilterResult)
	AddFilterResult(namespace, podName, nodeName, pluginName, reason string)
	AddPreScoreResult(namespace, podName, pluginName, reason string)
	AddScoreResult(namespace, podName, nodeName, pluginName string, score int64)
	AddPostFilterResult(namespace, podName, nominatedNodeName, pluginName string, nodeNames []string)
	AddPermitResult(namespace, podName, pluginName, status string, timeout time.Duration)
	AddReserveResult(namespace, podName, pluginName, status string)
	AddSelectedNode(namespace, podName, nodeName string)
	AddBindResult(namespace, podName, pluginName, status string)
	AddPreBindResult(namespace, podName, pluginName, status string)
	// AddCustomResult is intended to be used from outside of simulator.
	AddCustomResult(namespace, podName, annotationKey, result string)
}

//nolint:revive
type PluginExtenderInitializer func(handle SimulatorHandle) PluginExtenders

type SimulatorHandle interface {
	// AddCustomResult adds user defined data.
	// The results added through this func is reflected on the Pod's annotation eventually like other scheduling results.
	// This function is intended to be called from the plugin.PluginExtender; allow users to export some internal state on Pods for debugging purpose.
	// For example,
	// Calling AddCustomResult in NodeAffinity's PreFilterPluginExtender:
	// AddCustomResult("namespace", "incomingPod", "node-affinity-filter-internal-state-anno-key", "internal-state")
	// Then, "incomingPod" Pod will get {"node-affinity-filter-internal-state-anno-key": "internal-state"} annotation after scheduling.
	AddCustomResult(namespace, podName, annotationKey, result string)
}

// PreFilterPluginExtender is the extender for PreFilter plugin.
type PreFilterPluginExtender interface {
	// BeforePreFilter is a function that runs before the PreFilter method of the original plugin.
	// If BeforePreFilter returns non-success status, the simulator plugin doesn't run the PreFilter method of the original plugin and return that status.
	BeforePreFilter(ctx context.Context, state *framework.CycleState, pod *v1.Pod) (*framework.PreFilterResult, *framework.Status)
	// AfterPreFilter is a function that is run after the PreFilter method of the original plugin.
	// A PreFilter of the simulator plugin finally returns the PreFilterResult and the status returned from AfterPreFilter.
	AfterPreFilter(ctx context.Context, state *framework.CycleState, pod *v1.Pod, preFilterResult *framework.PreFilterResult, preFilterStatus *framework.Status) (*framework.PreFilterResult, *framework.Status)
}

// FilterPluginExtender is the extender for Filter plugin.
type FilterPluginExtender interface {
	// BeforeFilter is a function that runs before the Filter method of the original plugin.
	// If BeforeFilter returns non-success status, the simulator plugin doesn't run the Filter method of the original plugin and return that status.
	BeforeFilter(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo) *framework.Status
	// AfterFilter is a function that is run after the Filter method of the original plugin.
	// A Filter of the simulator plugin finally returns the status returned from AfterFilter.
	AfterFilter(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo, filterResult *framework.Status) *framework.Status
}

type PostFilterPluginExtender interface {
	// BeforePostFilter is a function that is run before the PostFilter method of the original plugin.
	// If BeforePostFilter return non-success status, the simulator plugin doesn't run the PostFilter method of the original plugin and return that status.
	BeforePostFilter(ctx context.Context, state *framework.CycleState, pod *v1.Pod, filteredNodeStatusMap framework.NodeToStatusMap) (*framework.PostFilterResult, *framework.Status)
	// AfterPostFilter is a function that is run after the PostFilter method of the original plugin.
	// A PostFilter of the simulator plugin finally returns the status returned from PostFilter.
	AfterPostFilter(ctx context.Context, state *framework.CycleState, pod *v1.Pod, filteredNodeStatusMap framework.NodeToStatusMap, postFilterResult *framework.PostFilterResult, status *framework.Status) (*framework.PostFilterResult, *framework.Status)
}
type PreScorePluginExtender interface {
	// BeforePreScore is a function that runs before the PreFilter method of the original plugin.
	// If BeforePreScore returns non-success status, the simulator plugin doesn't run the PreScore method of the original plugin and return that status.
	BeforePreScore(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodes []*framework.NodeInfo) *framework.Status
	// AfterPreScore is a function that is run after the PreScore method of the original plugin.
	// A PreScore of the simulator plugin finally returns the status returned from AfterPreScore.
	AfterPreScore(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodes []*framework.NodeInfo, preScoreStatus *framework.Status) *framework.Status
}

// ScorePluginExtender is the extender for Score plugin.
type ScorePluginExtender interface {
	// BeforeScore is a function that runs before the Score method of the original plugin.
	// If BeforeScore returns non-success status, the simulator plugin doesn't run the Score method of the original plugin and return that score & status.
	BeforeScore(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeName string) (int64, *framework.Status)
	// AfterScore is a function that runs after the Score method of the original plugin.
	// A Score of the simulator plugin finally returns the score & status returned from AfterScore.
	AfterScore(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeName string, score int64, scoreResult *framework.Status) (int64, *framework.Status)
}

// NormalizeScorePluginExtender is the extender for NormalizeScore plugin.
type NormalizeScorePluginExtender interface {
	// BeforeNormalizeScore is a function that runs before the NormalizeScore method of the original plugin.
	// If BeforeNormalizeScore returns non-success status, the simulator plugin doesn't run the NormalizeScore method of the original plugin and return that status.
	BeforeNormalizeScore(ctx context.Context, state *framework.CycleState, pod *v1.Pod, scores framework.NodeScoreList) *framework.Status
	// AfterNormalizeScore is a function that runs after the NormalizeScore method of the original plugin.
	// A NormalizeScore of the simulator plugins finally returns the status returned from AfterNormalizeScore.
	AfterNormalizeScore(ctx context.Context, state *framework.CycleState, pod *v1.Pod, scores framework.NodeScoreList, normalizeScoreResult *framework.Status) *framework.Status
}

// ReservePluginExtender is the extender for Reserve plugin.
type ReservePluginExtender interface {
	// BeforeReserve is a function that runs before the Reserve method of the original plugin.
	// If BeforeReserve returns non-success status, the simulator plugin doesn't run the Reserve method of the original plugin and return that status.
	BeforeReserve(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodename string) *framework.Status
	// AfterReserve is a function that is run after the Reserve method of the original plugin.
	// A Reserve of the simulator plugin finally returns the status returned from AfterReserve.
	AfterReserve(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodename string, reserveStatus *framework.Status) *framework.Status
	// BeforeUnreserve is a function that runs before the Reserve method of the original plugin.
	// If BeforeUnreserve returns non-success status, the simulator plugin doesn't run the Reserve method of the original plugin.
	BeforeUnreserve(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodename string) *framework.Status
	// AfterUnreserve is a function that is run after the Unreserve method of the original plugin.
	AfterUnreserve(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodename string)
}

// PermitPluginExtender is the extender for Permit plugin.
type PermitPluginExtender interface {
	// BeforePermit is a function that runs before the Permit method of the original plugin.
	// If BeforePermit returns non-success status, the simulator plugin doesn't run the Permit method of the original plugin and return that status.
	BeforePermit(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeName string) (*framework.Status, time.Duration)
	// AfterPermit is a function that runs after the Permit method of the original plugin.
	// A Permit of the simulator plugins finally returns the status returned from AfterPermit.
	AfterPermit(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeName string, permitResult *framework.Status, timeout time.Duration) (*framework.Status, time.Duration)
}

type PreBindPluginExtender interface {
	// BeforePreBind is a function that runs before the PreBind method of the original plugin.
	// If BeforePreBind returns non-success status, the simulator plugin doesn't run the PreBind method of the original plugin and return that status.
	BeforePreBind(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodename string) *framework.Status
	// AfterPreBind is a function that is run after the Bind method of the original plugin.
	// A PreBind of the simulator plugin finally returns the status returned from AfterBind.
	AfterPreBind(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodename string, bindResult *framework.Status) *framework.Status
}

type BindPluginExtender interface {
	// BeforeBind is a function that runs before the Bind method of the original plugin.
	// If BeforeBind returns non-success status, the simulator plugin doesn't run the Bind method of the original plugin and return that status.
	BeforeBind(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodename string) *framework.Status
	// AfterBind is a function that is run after the Bind method of the original plugin.
	// A Bind of the simulator plugin finally returns the status returned from AfterBind.
	AfterBind(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodename string, bindResult *framework.Status) *framework.Status
}

type PostBindPluginExtender interface {
	// BeforePostBind is a function that runs before the PostBind method of the original plugin.
	// If BeforePostBind returns non-success status, the simulator plugin doesn't run the PostBind method of the original plugin and return that status.
	BeforePostBind(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodename string) *framework.Status
	// AfterPostBind is a function that is run after the PostBind method of the original plugin.
	AfterPostBind(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodename string)
}

// PluginExtenders contains XXXXPluginExtenders.
// Each extender will intercept a calling to target method call of scheduler plugins,
// and you can check/modify requests and/or results.
//
//nolint:revive // intended to name it PluginExtenders to distinguish from the scheduler's extender.
type PluginExtenders struct {
	PreFilterPluginExtender      PreFilterPluginExtender
	FilterPluginExtender         FilterPluginExtender
	PostFilterPluginExtender     PostFilterPluginExtender
	PreScorePluginExtender       PreScorePluginExtender
	ScorePluginExtender          ScorePluginExtender
	NormalizeScorePluginExtender NormalizeScorePluginExtender
	PermitPluginExtender         PermitPluginExtender
	ReservePluginExtender        ReservePluginExtender
	PreBindPluginExtender        PreBindPluginExtender
	BindPluginExtender           BindPluginExtender
	PostBindPluginExtender       PostBindPluginExtender
}

type options struct {
	extenderInitializerOption PluginExtenderInitializer
	pluginNameOption          string
}

type (
	extendersOption  PluginExtenderInitializer
	pluginNameOption string
)

type Option interface {
	apply(*options)
}

func (e extendersOption) apply(opts *options) {
	opts.extenderInitializerOption = PluginExtenderInitializer(e)
}

func (p pluginNameOption) apply(opts *options) {
	opts.pluginNameOption = string(p)
}

// WithExtendersOption provides an easy way to extend the behavior of the plugin.
// These containing functions in PluginExtenders should be run before and after the original plugin of Scheduler Framework.
func WithExtendersOption(opt PluginExtenderInitializer) Option {
	return extendersOption(opt)
}

// WithPluginNameOption contains configuration options for the name field of a wrappedPlugin.
func WithPluginNameOption(opt *string) Option {
	return pluginNameOption(*opt)
}

// wrappedPlugin behaves as if it is original plugin, but it records result of plugin.
type wrappedPlugin struct {
	// name is plugin's name returned by Name() method.
	// This name is default to original plugin name + pluginSuffix.
	// You can change this name by WithPluginNameOption.
	name string
	// store records plugin's result.
	// TODO: move store's logic to plugin extender.
	store Store

	originalPreEnqueuePlugin framework.PreEnqueuePlugin
	originalPreFilterPlugin  framework.PreFilterPlugin
	originalFilterPlugin     framework.FilterPlugin
	originalPreScorePlugin   framework.PreScorePlugin
	originalPostFilterPlugin framework.PostFilterPlugin
	originalScorePlugin      framework.ScorePlugin
	originalPermitPlugin     framework.PermitPlugin
	originalReservePlugin    framework.ReservePlugin
	originalPreBindPlugin    framework.PreBindPlugin
	originalBindPlugin       framework.BindPlugin
	originalPostBindPlugin   framework.PostBindPlugin

	// plugin extenders
	preFilterPluginExtender      PreFilterPluginExtender
	filterPluginExtender         FilterPluginExtender
	postFilterPluginExtender     PostFilterPluginExtender
	scorePluginExtender          ScorePluginExtender
	preScorePluginExtender       PreScorePluginExtender
	normalizeScorePluginExtender NormalizeScorePluginExtender
	permitPluginExtender         PermitPluginExtender
	reservePluginExtender        ReservePluginExtender
	preBindPluginExtender        PreBindPluginExtender
	bindPluginExtender           BindPluginExtender
	postBindPluginExtender       PostBindPluginExtender
}

const (
	pluginSuffix = "Wrapped"
)

func pluginName(pluginName string) string {
	return pluginName + pluginSuffix
}

// NewWrappedPlugin makes wrappedPlugin from score or/and filter plugin.
//
//nolint:funlen,cyclop
func NewWrappedPlugin(s Store, p framework.Plugin, opts ...Option) framework.Plugin {
	options := options{
		// default value to create empty extenders.
		extenderInitializerOption: func(_ SimulatorHandle) PluginExtenders { return PluginExtenders{} },
	}
	for _, o := range opts {
		o.apply(&options)
	}
	pName := pluginName(p.Name())
	if options.pluginNameOption != "" {
		pName = options.pluginNameOption
	}

	plg := &wrappedPlugin{
		name:  pName,
		store: s,
	}

	extender := options.extenderInitializerOption(s)

	if extender.PreFilterPluginExtender != nil {
		plg.preFilterPluginExtender = extender.PreFilterPluginExtender
	}
	if extender.FilterPluginExtender != nil {
		plg.filterPluginExtender = extender.FilterPluginExtender
	}
	if extender.PostFilterPluginExtender != nil {
		plg.postFilterPluginExtender = extender.PostFilterPluginExtender
	}
	if extender.ScorePluginExtender != nil {
		plg.scorePluginExtender = extender.ScorePluginExtender
	}
	if extender.PreScorePluginExtender != nil {
		plg.preScorePluginExtender = extender.PreScorePluginExtender
	}
	if extender.NormalizeScorePluginExtender != nil {
		plg.normalizeScorePluginExtender = extender.NormalizeScorePluginExtender
	}
	if extender.PermitPluginExtender != nil {
		plg.permitPluginExtender = extender.PermitPluginExtender
	}
	if extender.ReservePluginExtender != nil {
		plg.reservePluginExtender = extender.ReservePluginExtender
	}
	if extender.PreBindPluginExtender != nil {
		plg.preBindPluginExtender = extender.PreBindPluginExtender
	}
	if extender.BindPluginExtender != nil {
		plg.bindPluginExtender = extender.BindPluginExtender
	}
	if extender.PostBindPluginExtender != nil {
		plg.postBindPluginExtender = extender.PostBindPluginExtender
	}

	peqp, ok := p.(framework.PreEnqueuePlugin)
	if ok {
		plg.originalPreEnqueuePlugin = peqp
	}
	prefp, ok := p.(framework.PreFilterPlugin)
	if ok {
		plg.originalPreFilterPlugin = prefp
	}
	fp, ok := p.(framework.FilterPlugin)
	if ok {
		plg.originalFilterPlugin = fp
	}
	pfp, ok := p.(framework.PostFilterPlugin)
	if ok {
		plg.originalPostFilterPlugin = pfp
	}
	presp, ok := p.(framework.PreScorePlugin)
	if ok {
		plg.originalPreScorePlugin = presp
	}
	sp, ok := p.(framework.ScorePlugin)
	if ok {
		plg.originalScorePlugin = sp
	}
	pp, ok := p.(framework.PermitPlugin)
	if ok {
		plg.originalPermitPlugin = pp
	}
	rp, ok := p.(framework.ReservePlugin)
	if ok {
		plg.originalReservePlugin = rp
	}

	bp, ok := p.(framework.BindPlugin)
	if ok {
		plg.originalBindPlugin = bp
	}

	prebp, ok := p.(framework.PreBindPlugin)
	if ok {
		plg.originalPreBindPlugin = prebp
	}

	postbp, ok := p.(framework.PostBindPlugin)
	if ok {
		plg.originalPostBindPlugin = postbp
	}

	queuesortp, ok := p.(framework.QueueSortPlugin)
	if ok {
		// There must be only one in each profile for which the QueueSortPlugin interface is implemented.
		newplug := &wrappedPluginWithQueueSort{wrappedPlugin: *plg}
		newplug.originalQueueSortPlugin = queuesortp
		return newplug
	}

	return plg
}

func (w *wrappedPlugin) Name() string { return w.name }
func (w *wrappedPlugin) ScoreExtensions() framework.ScoreExtensions {
	if w.originalScorePlugin != nil && w.originalScorePlugin.ScoreExtensions() != nil {
		return w
	}
	return nil
}

// PreEnqueue wraps original PreEnqueue plugin of Scheduler Framework.
// TODO: Implements before/after PreEnqueue function.
func (w *wrappedPlugin) PreEnqueue(ctx context.Context, p *v1.Pod) *framework.Status {
	if w.originalPreEnqueuePlugin == nil {
		// return nil not to affect queuing
		return nil
	}

	return w.originalPreEnqueuePlugin.PreEnqueue(ctx, p)
}

// NormalizeScore wraps original NormalizeScore plugin of Scheduler Framework.
// You can run your function before and/or after the execution of original NormalizeScore plugin
// by configuring with WithExtendersOption.
func (w *wrappedPlugin) NormalizeScore(ctx context.Context, state *framework.CycleState, pod *v1.Pod, scores framework.NodeScoreList) *framework.Status {
	if w.originalScorePlugin == nil || w.originalScorePlugin.ScoreExtensions() == nil {
		// return nil not to affect scoring
		return nil
	}

	if w.normalizeScorePluginExtender != nil {
		if s := w.normalizeScorePluginExtender.BeforeNormalizeScore(ctx, state, pod, scores); !s.IsSuccess() {
			return s
		}
	}

	s := w.originalScorePlugin.ScoreExtensions().NormalizeScore(ctx, state, pod, scores)
	if !s.IsSuccess() {
		klog.Errorf("failed to run normalize score. Normalized scores won't be recorded on Pod annotation: %v, %v", s.Code(), s.Message())
	} else {
		// TODO: move to AfterNormalizeScore.
		for _, s := range scores {
			w.store.AddNormalizedScoreResult(pod.Namespace, pod.Name, s.Name, w.originalScorePlugin.Name(), s.Score)
		}
	}

	if w.normalizeScorePluginExtender != nil {
		return w.normalizeScorePluginExtender.AfterNormalizeScore(ctx, state, pod, scores, s)
	}

	return s
}

// Score wraps original Score plugin of Scheduler Framework.
// You can run your function before and/or after the execution of original Score plugin
// by configuring with WithExtendersOption.
func (w *wrappedPlugin) Score(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeName string) (int64, *framework.Status) {
	if w.originalScorePlugin == nil {
		// return zero-score and nil not to affect scoring
		return 0, nil
	}

	if w.scorePluginExtender != nil {
		score, s := w.scorePluginExtender.BeforeScore(ctx, state, pod, nodeName)
		if !s.IsSuccess() {
			return score, s
		}
	}

	score, s := w.originalScorePlugin.Score(ctx, state, pod, nodeName)
	if !s.IsSuccess() {
		klog.Errorf("failed to run score plugin. Scores won't be recorded on Pod annotation: %v, %v", s.Code(), s.Message())
	} else {
		// TODO: move to AfterScore.
		w.store.AddScoreResult(pod.Namespace, pod.Name, nodeName, w.originalScorePlugin.Name(), score)
	}

	if w.scorePluginExtender != nil {
		return w.scorePluginExtender.AfterScore(ctx, state, pod, nodeName, score, s)
	}
	return score, s
}

func (w *wrappedPlugin) PreFilterExtensions() framework.PreFilterExtensions {
	if w.originalPreFilterPlugin == nil {
		// return nils not to affect scoring
		return nil
	}

	return w.originalPreFilterPlugin.PreFilterExtensions()
}

// PreScore wraps original PreScore plugin of Scheduler Framework.
// You can run your function before and/or after the execution of original PreScore plugin
// by configuring with WithExtendersOption.
func (w *wrappedPlugin) PreScore(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodes []*framework.NodeInfo) *framework.Status {
	if w.originalPreScorePlugin == nil {
		// return nil not to affect scoring
		return nil
	}

	if w.preScorePluginExtender != nil {
		s := w.preScorePluginExtender.BeforePreScore(ctx, state, pod, nodes)
		if !s.IsSuccess() {
			return s
		}
	}

	s := w.originalPreScorePlugin.PreScore(ctx, state, pod, nodes)
	var msg string
	if s.IsSuccess() {
		msg = schedulingresultstore.SuccessMessage
	} else {
		msg = s.Message()
	}
	w.store.AddPreScoreResult(pod.Namespace, pod.Name, w.originalPreScorePlugin.Name(), msg)

	if w.preScorePluginExtender != nil {
		return w.preScorePluginExtender.AfterPreScore(ctx, state, pod, nodes, s)
	}

	return s
}

// PreFilter wraps original PreFilter plugin of Scheduler Framework.
// You can run your function before and/or after the execution of original PreFilter plugin
// by configuring with WithExtendersOption.
func (w *wrappedPlugin) PreFilter(ctx context.Context, state *framework.CycleState, p *v1.Pod) (*framework.PreFilterResult, *framework.Status) {
	if w.originalPreFilterPlugin == nil {
		// return nils not to affect scoring
		return nil, nil
	}

	if w.preFilterPluginExtender != nil {
		r, s := w.preFilterPluginExtender.BeforePreFilter(ctx, state, p)
		if !s.IsSuccess() {
			return r, s
		}
	}

	result, s := w.originalPreFilterPlugin.PreFilter(ctx, state, p)
	var msg string
	if s.IsSuccess() {
		msg = schedulingresultstore.SuccessMessage
	} else {
		msg = s.Message()
	}
	w.store.AddPreFilterResult(p.Namespace, p.Name, w.originalPreFilterPlugin.Name(), msg, result)

	if w.preFilterPluginExtender != nil {
		return w.preFilterPluginExtender.AfterPreFilter(ctx, state, p, result, s)
	}

	return result, s
}

// Filter wraps original Filter plugin of Scheduler Framework.
// You can run your function before and/or after the execution of original Filter plugin
// by configuring with WithExtendersOption.
func (w *wrappedPlugin) Filter(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo) *framework.Status {
	if w.originalFilterPlugin == nil {
		// return nil not to affect filtering
		return nil
	}

	if w.filterPluginExtender != nil {
		if s := w.filterPluginExtender.BeforeFilter(ctx, state, pod, nodeInfo); !s.IsSuccess() {
			return s
		}
	}

	s := w.originalFilterPlugin.Filter(ctx, state, pod, nodeInfo)
	var msg string
	if s.IsSuccess() {
		msg = schedulingresultstore.PassedFilterMessage
	} else {
		msg = s.Message()
	}
	w.store.AddFilterResult(pod.Namespace, pod.Name, nodeInfo.Node().Name, w.originalFilterPlugin.Name(), msg)

	if w.filterPluginExtender != nil {
		return w.filterPluginExtender.AfterFilter(ctx, state, pod, nodeInfo, s)
	}
	return s
}

func (w *wrappedPlugin) PostFilter(ctx context.Context, state *framework.CycleState, pod *v1.Pod, filteredNodeStatusMap framework.NodeToStatusMap) (*framework.PostFilterResult, *framework.Status) {
	if w.originalPostFilterPlugin == nil {
		// return Unschedulable not to affect post filtering.
		// (If return Unschedulable, the scheduler will execute next PostFilter plugin.)
		return nil, framework.NewStatus(framework.Unschedulable)
	}
	if w.postFilterPluginExtender != nil {
		r, s := w.postFilterPluginExtender.BeforePostFilter(ctx, state, pod, filteredNodeStatusMap)
		if !s.IsSuccess() {
			return r, s
		}
	}
	r, s := w.originalPostFilterPlugin.PostFilter(ctx, state, pod, filteredNodeStatusMap)
	var nominatedNodeName string
	if s.IsSuccess() {
		nominatedNodeName = r.NominatedNodeName
	}
	nodeNames := make([]string, 0, len(filteredNodeStatusMap))
	for k := range filteredNodeStatusMap {
		nodeNames = append(nodeNames, k)
	}
	w.store.AddPostFilterResult(pod.Namespace, pod.Name, nominatedNodeName, w.originalPostFilterPlugin.Name(), nodeNames)

	if w.postFilterPluginExtender != nil {
		return w.postFilterPluginExtender.AfterPostFilter(ctx, state, pod, filteredNodeStatusMap, r, s)
	}
	return r, s
}

// Permit wraps original Permit plugin of Scheduler Framework.
// You can run your function before and/or after the execution of original Permit plugin
// by configuring with WithExtendersOption.
func (w *wrappedPlugin) Permit(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeName string) (*framework.Status, time.Duration) {
	if w.originalPermitPlugin == nil {
		// return zero-score and nil not to affect scoring
		return nil, 0
	}

	if w.permitPluginExtender != nil {
		s, d := w.permitPluginExtender.BeforePermit(ctx, state, pod, nodeName)
		if !s.IsSuccess() {
			return s, d
		}
	}

	s, timeout := w.originalPermitPlugin.Permit(ctx, state, pod, nodeName)
	msg := s.Message()
	if s.IsSuccess() {
		msg = schedulingresultstore.SuccessMessage
	}
	if s.IsWait() {
		msg = schedulingresultstore.WaitMessage
	}

	w.store.AddPermitResult(pod.Namespace, pod.Name, w.originalPermitPlugin.Name(), msg, timeout)

	if w.permitPluginExtender != nil {
		return w.permitPluginExtender.AfterPermit(ctx, state, pod, nodeName, s, timeout)
	}

	return s, timeout
}

// Reserve wraps original Reserve plugin of Scheduler Framework.
// You can run your function before and/or after the execution of original Reserve plugin
// by configuring with WithExtendersOption.
func (w *wrappedPlugin) Reserve(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodename string) *framework.Status {
	w.store.AddSelectedNode(pod.Namespace, pod.Name, nodename)

	if w.originalReservePlugin == nil {
		// return nil not to affect scoring
		return nil
	}

	if w.reservePluginExtender != nil {
		s := w.reservePluginExtender.BeforeReserve(ctx, state, pod, nodename)
		if !s.IsSuccess() {
			return s
		}
	}

	s := w.originalReservePlugin.Reserve(ctx, state, pod, nodename)
	var msg string
	if s.IsSuccess() {
		msg = schedulingresultstore.SuccessMessage
	} else {
		msg = s.Message()
	}
	w.store.AddReserveResult(pod.Namespace, pod.Name, w.originalReservePlugin.Name(), msg)

	if w.reservePluginExtender != nil {
		return w.reservePluginExtender.AfterReserve(ctx, state, pod, nodename, s)
	}

	return s
}

// Unreserve wraps original Unreserve plugin of Scheduler Framework.
// You can run your function before and/or after the execution of original Unreserve plugin
// by configuring with WithExtendersOption.
func (w *wrappedPlugin) Unreserve(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodename string) {
	if w.originalReservePlugin == nil {
		return
	}

	if w.reservePluginExtender != nil {
		s := w.reservePluginExtender.BeforeUnreserve(ctx, state, pod, nodename)
		if !s.IsSuccess() {
			klog.ErrorS(nil, "reservePluginExtender.BeforeUnreserve returned non success status, won't run Unreserve", "status_message", s.Message(), "plugin", w.originalReservePlugin.Name())
			return
		}
	}

	w.originalReservePlugin.Unreserve(ctx, state, pod, nodename)

	if w.reservePluginExtender != nil {
		w.reservePluginExtender.AfterUnreserve(ctx, state, pod, nodename)
	}
}

func (w *wrappedPlugin) PreBind(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodename string) *framework.Status {
	if w.originalPreBindPlugin == nil {
		// return nil not to affect scoring
		return nil
	}

	if w.preBindPluginExtender != nil {
		s := w.preBindPluginExtender.BeforePreBind(ctx, state, pod, nodename)
		if !s.IsSuccess() {
			return s
		}
	}

	s := w.originalPreBindPlugin.PreBind(ctx, state, pod, nodename)
	var msg string
	if s.IsSuccess() {
		msg = schedulingresultstore.SuccessMessage
	} else {
		msg = s.Message()
	}
	w.store.AddPreBindResult(pod.Namespace, pod.Name, w.originalPreBindPlugin.Name(), msg)

	if w.preBindPluginExtender != nil {
		return w.preBindPluginExtender.AfterPreBind(ctx, state, pod, nodename, s)
	}

	return s
}

func (w *wrappedPlugin) Bind(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodename string) *framework.Status {
	if w.originalBindPlugin == nil {
		// return skip not to affect other bind plugins.
		return framework.NewStatus(framework.Skip, "called wrapped bind plugin is nil")
	}

	if w.bindPluginExtender != nil {
		s := w.bindPluginExtender.BeforeBind(ctx, state, pod, nodename)
		if !s.IsSuccess() {
			return s
		}
	}

	s := w.originalBindPlugin.Bind(ctx, state, pod, nodename)
	var msg string
	if s.IsSuccess() {
		msg = schedulingresultstore.SuccessMessage
	} else {
		msg = s.Message()
	}
	w.store.AddBindResult(pod.Namespace, pod.Name, w.originalBindPlugin.Name(), msg)

	if w.bindPluginExtender != nil {
		return w.bindPluginExtender.AfterBind(ctx, state, pod, nodename, s)
	}

	return s
}

func (w *wrappedPlugin) PostBind(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodename string) {
	if w.originalPostBindPlugin == nil {
		return
	}

	if w.postBindPluginExtender != nil {
		s := w.postBindPluginExtender.BeforePostBind(ctx, state, pod, nodename)
		if !s.IsSuccess() {
			klog.ErrorS(nil, "postBindPluginExtender.BeforePostBind returned non success status, won't run PostBind", "status_message", s.Message(), "plugin", w.originalPostBindPlugin.Name())
			return
		}
	}

	w.originalPostBindPlugin.PostBind(ctx, state, pod, nodename)

	if w.postBindPluginExtender != nil {
		w.postBindPluginExtender.AfterPostBind(ctx, state, pod, nodename)
	}
}

// wrappedPluginWithQueueSort behaves as if it is original plugin and QueueSort plugin.
// To support MultiPoint field, we are required to separate WrappedPlugin and the implementation of QueueSort interface.
type wrappedPluginWithQueueSort struct {
	wrappedPlugin

	originalQueueSortPlugin framework.QueueSortPlugin
}

func (w *wrappedPluginWithQueueSort) Name() string { return w.wrappedPlugin.Name() }

// Less  wraps original Less plugin of Scheduler Framework.
func (w *wrappedPluginWithQueueSort) Less(pod1 *framework.QueuedPodInfo, pod2 *framework.QueuedPodInfo) bool {
	if w.originalQueueSortPlugin == nil {
		return false
	}

	return w.originalQueueSortPlugin.Less(pod1, pod2)
}
</file>

<file path="kube-scheduler-simulator/simulator/scheduler/storereflector/mock_storereflector/resultstore.go">
// Code generated by MockGen. DO NOT EDIT.
// Source: sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler/storereflector (interfaces: ResultStore)
//
// Generated by this command:
//
//	mockgen -destination=./mock_storereflector/resultstore.go . ResultStore
//

// Package mock_storereflector is a generated GoMock package.
package mock_storereflector

import (
	reflect "reflect"

	gomock "go.uber.org/mock/gomock"
	v1 "k8s.io/api/core/v1"
)

// MockResultStore is a mock of ResultStore interface.
type MockResultStore struct {
	ctrl     *gomock.Controller
	recorder *MockResultStoreMockRecorder
	isgomock struct{}
}

// MockResultStoreMockRecorder is the mock recorder for MockResultStore.
type MockResultStoreMockRecorder struct {
	mock *MockResultStore
}

// NewMockResultStore creates a new mock instance.
func NewMockResultStore(ctrl *gomock.Controller) *MockResultStore {
	mock := &MockResultStore{ctrl: ctrl}
	mock.recorder = &MockResultStoreMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockResultStore) EXPECT() *MockResultStoreMockRecorder {
	return m.recorder
}

// DeleteData mocks base method.
func (m *MockResultStore) DeleteData(key v1.Pod) {
	m.ctrl.T.Helper()
	m.ctrl.Call(m, "DeleteData", key)
}

// DeleteData indicates an expected call of DeleteData.
func (mr *MockResultStoreMockRecorder) DeleteData(key any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "DeleteData", reflect.TypeOf((*MockResultStore)(nil).DeleteData), key)
}

// GetStoredResult mocks base method.
func (m *MockResultStore) GetStoredResult(pod *v1.Pod) map[string]string {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "GetStoredResult", pod)
	ret0, _ := ret[0].(map[string]string)
	return ret0
}

// GetStoredResult indicates an expected call of GetStoredResult.
func (mr *MockResultStoreMockRecorder) GetStoredResult(pod any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "GetStoredResult", reflect.TypeOf((*MockResultStore)(nil).GetStoredResult), pod)
}
</file>

<file path="kube-scheduler-simulator/simulator/scheduler/storereflector/annotation.go">
package storereflector

// ResultsHistoryAnnotation has the all results including the past ones.
const ResultsHistoryAnnotation = "kube-scheduler-simulator.sigs.k8s.io/result-history"
</file>

<file path="kube-scheduler-simulator/simulator/scheduler/storereflector/storereflector_test.go">
package storereflector

import (
	"context"
	"fmt"
	"testing"

	"github.com/google/go-cmp/cmp"
	"github.com/stretchr/testify/assert"
	"go.uber.org/mock/gomock"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/client-go/kubernetes/fake"

	"sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler/storereflector/mock_storereflector"
)

const (
	ExtenderFilterResultAnnotationKey = "kube-scheduler-simulator.sigs.k8s.io/extender-filter-result"
	ResultStoreKey                    = "ExtenderResultStoreKey"
)

func TestReflector_storeAllResultToPodFunc(t *testing.T) {
	t.Parallel()
	tests := []struct {
		name                        string
		podName                     string
		podNamespace                string
		prepareMockResultStoreSetFn func(m *mock_storereflector.MockResultStore)
		prepareFakeClientSetFn      func() *fake.Clientset
		wantAnnotation              map[string]string
	}{
		{
			name:         "success",
			podName:      "pod1",
			podNamespace: "default",
			prepareMockResultStoreSetFn: func(m *mock_storereflector.MockResultStore) {
				m.EXPECT().GetStoredResult(gomock.Any()).Return(map[string]string{ExtenderFilterResultAnnotationKey: "some results"})
				m.EXPECT().DeleteData(gomock.Any())
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				c.CoreV1().Pods("default").Create(context.Background(), &corev1.Pod{
					ObjectMeta: metav1.ObjectMeta{
						Name:      "pod1",
						Namespace: "default",
					},
				}, metav1.CreateOptions{})
				return c
			},
			wantAnnotation: map[string]string{ExtenderFilterResultAnnotationKey: "some results", ResultsHistoryAnnotation: "[{\"kube-scheduler-simulator.sigs.k8s.io/extender-filter-result\":\"some results\"}]"},
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			c := tt.prepareFakeClientSetFn()
			ctrl := gomock.NewController(t)
			rs := mock_storereflector.NewMockResultStore(ctrl)
			tt.prepareMockResultStoreSetFn(rs)
			r := &reflector{
				resultStores: map[string]ResultStore{ResultStoreKey: rs},
			}
			fn := r.storeAllResultToPodFunc(c)
			p, _ := c.CoreV1().Pods(tt.podNamespace).Get(context.Background(), tt.podName, metav1.GetOptions{})
			original := p.DeepCopy()
			fn(corev1.Pod{}, p)

			// Check that the function doesn't mutate the input object,
			// which is shared with other event handlers.
			assert.Equal(t, original, p)

			updatedPod, _ := c.CoreV1().Pods(tt.podNamespace).Get(context.Background(), tt.podName, metav1.GetOptions{})

			assert.Equal(t, tt.wantAnnotation, updatedPod.Annotations)
		})
	}
}

func Test_updateResultHistory(t *testing.T) {
	t.Parallel()
	tests := []struct {
		name    string
		p       *corev1.Pod
		m       map[string]string
		wantErr assert.ErrorAssertionFunc
		wantPod *corev1.Pod
	}{
		{
			name: "success: Pod doesn't have annotation yet",
			p: &corev1.Pod{
				ObjectMeta: metav1.ObjectMeta{
					Annotations: nil,
				},
			},
			m: map[string]string{
				"result1": "fuga",
				"result2": "hoge",
			},
			wantPod: &corev1.Pod{
				ObjectMeta: metav1.ObjectMeta{
					Annotations: map[string]string{
						ResultsHistoryAnnotation: `[{"result1":"fuga","result2":"hoge"}]`,
					},
				},
			},
			wantErr: assert.NoError,
		},
		{
			name: "success: Pod already has annotation",
			p: &corev1.Pod{
				ObjectMeta: metav1.ObjectMeta{
					Annotations: map[string]string{
						ResultsHistoryAnnotation: `[{"result1":"fuga","result2":"hoge"}]`,
					},
				},
			},
			m: map[string]string{
				"result1": "fuga2",
				"result2": "hoge2",
			},
			wantPod: &corev1.Pod{
				ObjectMeta: metav1.ObjectMeta{
					Annotations: map[string]string{
						ResultsHistoryAnnotation: `[{"result1":"fuga","result2":"hoge"},{"result1":"fuga2","result2":"hoge2"}]`,
					},
				},
			},
			wantErr: assert.NoError,
		},
		{
			name: "fail: Pod has broken value on annotation",
			p: &corev1.Pod{
				ObjectMeta: metav1.ObjectMeta{
					Annotations: map[string]string{
						ResultsHistoryAnnotation: `broken`,
					},
				},
			},
			m: map[string]string{
				"result1": "fuga2",
				"result2": "hoge2",
			},
			wantErr: assert.Error,
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			p := tt.p
			tt.wantErr(t, updateResultHistory(p, tt.m), fmt.Sprintf("updateResultHistory(%v, %v)", p, tt.m))
			if d := cmp.Diff(p, tt.wantPod); d != "" && tt.wantPod != nil {
				t.Fatalf("unexpected Pod: %v", d)
			}
		})
	}
}
</file>

<file path="kube-scheduler-simulator/simulator/scheduler/storereflector/storereflector.go">
package storereflector

//go:generate mockgen -destination=./mock_$GOPACKAGE/resultstore.go . ResultStore

import (
	"context"
	"encoding/json"

	"golang.org/x/xerrors"
	corev1 "k8s.io/api/core/v1"
	apierrors "k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	clientset "k8s.io/client-go/kubernetes"
	"k8s.io/client-go/tools/cache"
	"k8s.io/klog/v2"
	"k8s.io/kubernetes/pkg/scheduler"

	"sigs.k8s.io/kube-scheduler-simulator/simulator/util"
)

type Reflector interface {
	AddResultStore(store ResultStore, key string)
	ResisterResultSavingToInformer(client clientset.Interface, stopCh <-chan struct{}) error
}

// ResultStore represents the store which is stores data and shared with simulator and scheduler.
// Fulfilling this interface will allow the stored results to be saved as data in that Pod
// when the Pod's schedule is complete.
type ResultStore interface {
	// GetStoredResult get all stored result of a given Pod.
	GetStoredResult(pod *corev1.Pod) map[string]string
	// DeleteData deletes all data corresponding to the pod.
	DeleteData(key corev1.Pod)
}

// store manages any ResultStore.
// ResultStore stores any result that should be reflected to the Pod.
type reflector struct {
	resultStores map[string]ResultStore
}

func New() Reflector {
	return &reflector{
		resultStores: map[string]ResultStore{},
	}
}

// AddResultStore adds the ResultStore to the map.
func (s *reflector) AddResultStore(store ResultStore, key string) {
	s.resultStores[key] = store
}

// ResisterResultSavingToInformer registers the event handler to the informerFactory
// to reflects all results on the pod annotation when the scheduling is finished.
func (s *reflector) ResisterResultSavingToInformer(client clientset.Interface, stopCh <-chan struct{}) error {
	informerFactory := scheduler.NewInformerFactory(client, 0)
	// Reflector adds scheduling results when pod is updating.
	// This is because Extenders doesn't have any phase to hook scheduling finished. (both successfully and non-successfully)
	_, err := informerFactory.Core().V1().Pods().Informer().AddEventHandler(
		cache.ResourceEventHandlerFuncs{
			UpdateFunc: s.storeAllResultToPodFunc(client),
		},
	)
	if err != nil {
		return xerrors.Errorf("failed to AddEventHandler of Informer: %w", err)
	}

	informerFactory.Start(stopCh)
	informerFactory.WaitForCacheSync(stopCh)

	return nil
}

// storeAllResultToPodFunc returns the function that reflects all results on the pod annotation when the scheduling is finished.
// It will be used as the even handler of resource updating.
//
//nolint:gocognit,cyclop
func (s *reflector) storeAllResultToPodFunc(client clientset.Interface) func(interface{}, interface{}) {
	return func(_, newObj interface{}) {
		ctx := context.Background()
		pod, ok := newObj.(*corev1.Pod)
		if !ok {
			klog.ErrorS(nil, "Cannot convert to *corev1.Pod", "obj", newObj)
			return
		}

		updateFunc := func() (bool, error) {
			// Fetch the latest Pod object and apply changes to it. Otherwise, our update may be
			// rejected due to our copy being stale. This also ensures we don't modify the copy from
			// the shared informer.
			newPod, err := client.CoreV1().Pods(pod.Namespace).Get(ctx, pod.Name, metav1.GetOptions{})
			if err != nil {
				return false, xerrors.Errorf("get pod: %w", err)
			}
			if newPod.UID != pod.UID {
				return false, xerrors.Errorf("pod UID is different: %s != %s", newPod.UID, pod.UID)
			}
			// overwrite the Pod object so that we won't modify the copy from the shared informer.
			pod = newPod

			// Call GetStoredResult of all ResultStore which is kept on the map
			// to reflect all results to the pod annotation.
			resultSet := map[string]string{}
			for k := range s.resultStores {
				m := s.resultStores[k].GetStoredResult(pod)
				for k, v := range m {
					resultSet[k] = v
					if pod.ObjectMeta.Annotations == nil {
						pod.ObjectMeta.Annotations = map[string]string{}
					}
					pod.ObjectMeta.Annotations[k] = v
				}
			}
			if len(resultSet) == 0 {
				// no need to update anything on the Pod.
				return true, nil
			}

			if err := updateResultHistory(pod, resultSet); err != nil {
				klog.ErrorS(err, "cannot update "+ResultsHistoryAnnotation, "pod", klog.KObj(pod))
				// just log error and update other annotation values.
			}

			_, err = client.CoreV1().Pods(pod.Namespace).Update(ctx, pod, metav1.UpdateOptions{})
			if err != nil {
				// Even though we fetched the latest Pod object, we still might get a conflict
				// because of a concurrent update. Retrying these conflict errors will usually help
				// as long as we re-fetch the latest Pod object each time.
				if apierrors.IsConflict(err) {
					return false, nil
				}
				return false, xerrors.Errorf("update pod: %w", err)
			}
			return true, nil
		}
		if err := util.RetryWithExponentialBackOff(updateFunc); err != nil {
			klog.Errorf("failed to update the pod with retry to record store: %+v", err)
			return
		}

		for k := range s.resultStores {
			// Delete the data from the Reflector only if it is successfully added on the pod's annotations.
			s.resultStores[k].DeleteData(*pod)
		}
	}
}

func updateResultHistory(p *corev1.Pod, m map[string]string) error {
	a, ok := p.GetAnnotations()[ResultsHistoryAnnotation]
	if !ok {
		a = "[]"
	}
	results := []map[string]string{}
	if err := json.Unmarshal([]byte(a), &results); err != nil {
		return err
	}

	results = append(results, m)

	r, err := json.Marshal(results)
	if err != nil {
		return xerrors.Errorf("encode all results: %w", err)
	}
	metav1.SetMetaDataAnnotation(&p.ObjectMeta, ResultsHistoryAnnotation, string(r))

	return nil
}
</file>

<file path="kube-scheduler-simulator/simulator/scheduler/scheduler_test.go">
package scheduler

import (
	"sort"
	"testing"

	"github.com/stretchr/testify/assert"
	v1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	configv1 "k8s.io/kube-scheduler/config/v1"
	"k8s.io/utils/ptr"

	schedConfig "sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler/config"
)

var (
	weight1 int32 = 1
	weight2 int32 = 2
	weight3 int32 = 3
)

//nolint:gocognit // For test case.
func Test_convertConfigurationForSimulator(t *testing.T) {
	t.Parallel()

	var nondefaultParallelism int32 = 3
	defaultschedulername := v1.DefaultSchedulerName
	nondefaultschedulername := v1.DefaultSchedulerName + "2"

	var minCandidateNodesPercentage int32 = 20
	var minCandidateNodesAbsolute int32 = 100
	var hardPodAffinityWeight int32 = 2

	type args struct {
		versioned *configv1.KubeSchedulerConfiguration
	}
	tests := []struct {
		name    string
		args    args
		want    *configv1.KubeSchedulerConfiguration
		wantErr bool
	}{
		{
			name: "success with empty-configuration",
			args: args{
				versioned: &configv1.KubeSchedulerConfiguration{},
			},
			want: func() *configv1.KubeSchedulerConfiguration {
				cfg := configGeneratedFromDefault()
				return &cfg
			}(),
		},
		{
			name: "success with no-disabled plugin",
			args: args{
				versioned: &configv1.KubeSchedulerConfiguration{
					Profiles: []configv1.KubeSchedulerProfile{
						{
							SchedulerName: &defaultschedulername,
							Plugins:       &configv1.Plugins{},
						},
					},
				},
			},
			want: func() *configv1.KubeSchedulerConfiguration {
				cfg := configGeneratedFromDefault()
				return &cfg
			}(),
		},
		{
			name: "success with empty Profiles",
			args: args{
				versioned: &configv1.KubeSchedulerConfiguration{},
			},
			want: func() *configv1.KubeSchedulerConfiguration {
				cfg := configGeneratedFromDefault()
				return &cfg
			}(),
		},
		{
			name: "changes of field other than Profiles and Extenders does not affects result",
			args: args{
				versioned: &configv1.KubeSchedulerConfiguration{
					Parallelism: &nondefaultParallelism,
					Profiles: []configv1.KubeSchedulerProfile{
						{
							SchedulerName: &defaultschedulername,
							Plugins:       &configv1.Plugins{},
						},
					},
				},
			},
			want: func() *configv1.KubeSchedulerConfiguration {
				cfg := configGeneratedFromDefault()
				return &cfg
			}(),
		},
		{
			name: "changes of field other than Profiles.Plugins and Extenders does not affects result",
			args: args{
				versioned: &configv1.KubeSchedulerConfiguration{
					Parallelism: &nondefaultParallelism,
					Profiles: []configv1.KubeSchedulerProfile{
						{
							SchedulerName: &defaultschedulername,
							Plugins:       &configv1.Plugins{},
							PluginConfig:  nil,
						},
					},
				},
			},
			want: func() *configv1.KubeSchedulerConfiguration {
				cfg := configGeneratedFromDefault()
				return &cfg
			}(),
		},
		{
			name: "success with multiple profiles/applied disabled setting",
			args: args{
				versioned: &configv1.KubeSchedulerConfiguration{
					Parallelism: &nondefaultParallelism,
					Profiles: []configv1.KubeSchedulerProfile{
						{
							SchedulerName: &defaultschedulername,
						},
						{
							SchedulerName: &nondefaultschedulername,
							Plugins: &configv1.Plugins{
								MultiPoint: configv1.PluginSet{
									Disabled: []configv1.Plugin{
										{
											Name: "ImageLocality",
										},
										{
											Name: "NodeResourcesFit",
										},
									},
								},
							},
						},
					},
				},
			},
			want: func() *configv1.KubeSchedulerConfiguration {
				cfg := configGeneratedFromDefault()
				profile2 := cfg.Profiles[0].DeepCopy()
				profile2.SchedulerName = ptr.To(nondefaultschedulername)
				profile2.Plugins.MultiPoint.Enabled = []configv1.Plugin{
					{Name: "SchedulingGatesWrapped"},
					{Name: "PrioritySortWrapped"},
					{Name: "NodeUnschedulableWrapped"},
					{Name: "NodeNameWrapped"},
					{Name: "TaintTolerationWrapped", Weight: &weight3},
					{Name: "NodeAffinityWrapped", Weight: &weight2},
					{Name: "NodePortsWrapped"},
					{Name: "VolumeRestrictionsWrapped"},
					{Name: "EBSLimitsWrapped"},
					{Name: "GCEPDLimitsWrapped"},
					{Name: "NodeVolumeLimitsWrapped"},
					{Name: "AzureDiskLimitsWrapped"},
					{Name: "VolumeBindingWrapped"},
					{Name: "VolumeZoneWrapped"},
					{Name: "PodTopologySpreadWrapped", Weight: &weight2},
					{Name: "InterPodAffinityWrapped", Weight: &weight2},
					{Name: "DefaultPreemptionWrapped"},
					{Name: "NodeResourcesBalancedAllocationWrapped", Weight: &weight1},
					{Name: "DefaultBinderWrapped"},
				}
				cfg.Profiles = append(cfg.Profiles, *profile2)
				return &cfg
			}(),
		},
		{
			name: "success with multiple profiles and custom-pluginconfig",
			args: args{
				versioned: &configv1.KubeSchedulerConfiguration{
					Parallelism: &nondefaultParallelism,
					Profiles: []configv1.KubeSchedulerProfile{
						{
							SchedulerName: &defaultschedulername,
							PluginConfig: []configv1.PluginConfig{
								{
									Name: "DefaultPreemption",
									Args: runtime.RawExtension{
										Object: &configv1.DefaultPreemptionArgs{
											TypeMeta: metav1.TypeMeta{
												Kind:       "DefaultPreemptionArgs",
												APIVersion: "kubescheduler.config.k8s.io/v1",
											},
											MinCandidateNodesPercentage: &minCandidateNodesPercentage,
											MinCandidateNodesAbsolute:   &minCandidateNodesAbsolute,
										},
									},
								},
							},
						},
						{
							SchedulerName: &nondefaultschedulername,
							PluginConfig: []configv1.PluginConfig{
								{
									Name: "InterPodAffinity",
									Args: runtime.RawExtension{
										Object: &configv1.InterPodAffinityArgs{
											TypeMeta: metav1.TypeMeta{
												Kind:       "InterPodAffinityArgs",
												APIVersion: "kubescheduler.config.k8s.io/v1",
											},
											HardPodAffinityWeight: &hardPodAffinityWeight,
										},
									},
								},
							},
						},
					},
				},
			},
			want: func() *configv1.KubeSchedulerConfiguration {
				cfg := configGeneratedFromDefault()
				profile2 := cfg.Profiles[0].DeepCopy()
				profile2.SchedulerName = &nondefaultschedulername
				for i := range cfg.Profiles[0].PluginConfig {
					if cfg.Profiles[0].PluginConfig[i].Name == "DefaultPreemption" {
						cfg.Profiles[0].PluginConfig[i] = configv1.PluginConfig{
							Name: "DefaultPreemption",
							Args: runtime.RawExtension{
								Object: &configv1.DefaultPreemptionArgs{
									TypeMeta: metav1.TypeMeta{
										Kind:       "DefaultPreemptionArgs",
										APIVersion: "kubescheduler.config.k8s.io/v1",
									},
									MinCandidateNodesPercentage: &minCandidateNodesPercentage,
									MinCandidateNodesAbsolute:   &minCandidateNodesAbsolute,
								},
							},
						}
					}
					if cfg.Profiles[0].PluginConfig[i].Name == "DefaultPreemptionWrapped" {
						cfg.Profiles[0].PluginConfig[i] = configv1.PluginConfig{
							Name: "DefaultPreemptionWrapped",
							Args: runtime.RawExtension{
								Object: &configv1.DefaultPreemptionArgs{
									TypeMeta: metav1.TypeMeta{
										Kind:       "DefaultPreemptionArgs",
										APIVersion: "kubescheduler.config.k8s.io/v1",
									},
									MinCandidateNodesPercentage: &minCandidateNodesPercentage,
									MinCandidateNodesAbsolute:   &minCandidateNodesAbsolute,
								},
							},
						}
					}
				}

				for i := range profile2.PluginConfig {
					if profile2.PluginConfig[i].Name == "InterPodAffinity" {
						profile2.PluginConfig[i] = configv1.PluginConfig{
							Name: "InterPodAffinity",
							Args: runtime.RawExtension{
								Object: &configv1.InterPodAffinityArgs{
									TypeMeta: metav1.TypeMeta{
										Kind:       "InterPodAffinityArgs",
										APIVersion: "kubescheduler.config.k8s.io/v1",
									},
									HardPodAffinityWeight: &hardPodAffinityWeight,
								},
							},
						}
					}
					if profile2.PluginConfig[i].Name == "InterPodAffinityWrapped" {
						profile2.PluginConfig[i] = configv1.PluginConfig{
							Name: "InterPodAffinityWrapped",
							Args: runtime.RawExtension{
								Object: &configv1.InterPodAffinityArgs{
									TypeMeta: metav1.TypeMeta{
										Kind:       "InterPodAffinityArgs",
										APIVersion: "kubescheduler.config.k8s.io/v1",
									},
									HardPodAffinityWeight: &hardPodAffinityWeight,
								},
							},
						}
					}
				}

				cfg.Profiles = append(cfg.Profiles, *profile2)
				return &cfg
			}(),
		},
		{
			name: "success with multiplugin plugin setting/manual setting weights have priority.",
			args: args{
				versioned: &configv1.KubeSchedulerConfiguration{
					Parallelism: &nondefaultParallelism,
					Profiles: []configv1.KubeSchedulerProfile{
						{
							SchedulerName: &defaultschedulername,
							Plugins: &configv1.Plugins{
								MultiPoint: configv1.PluginSet{
									Enabled: []configv1.Plugin{
										{
											Name:   "NodeResourcesFit",
											Weight: &weight3,
										},
									},
								},
							},
						},
					},
				},
			},
			want: func() *configv1.KubeSchedulerConfiguration {
				cfg := configGeneratedFromDefault()
				cfg.Profiles[0].Plugins.MultiPoint.Disabled = []configv1.Plugin{
					{Name: "*"},
				}
				cfg.Profiles[0].Plugins.MultiPoint.Enabled = []configv1.Plugin{
					{Name: "SchedulingGatesWrapped"},
					{Name: "PrioritySortWrapped"},
					{Name: "NodeUnschedulableWrapped"},
					{Name: "NodeNameWrapped"},
					{Name: "TaintTolerationWrapped", Weight: &weight3},
					{Name: "NodeAffinityWrapped", Weight: &weight2},
					{Name: "NodePortsWrapped"},
					{Name: "NodeResourcesFitWrapped", Weight: &weight3},
					{Name: "VolumeRestrictionsWrapped"},
					{Name: "EBSLimitsWrapped"},
					{Name: "GCEPDLimitsWrapped"},
					{Name: "NodeVolumeLimitsWrapped"},
					{Name: "AzureDiskLimitsWrapped"},
					{Name: "VolumeBindingWrapped"},
					{Name: "VolumeZoneWrapped"},
					{Name: "PodTopologySpreadWrapped", Weight: &weight2},
					{Name: "InterPodAffinityWrapped", Weight: &weight2},
					{Name: "DefaultPreemptionWrapped"},
					{Name: "NodeResourcesBalancedAllocationWrapped", Weight: &weight1},
					{Name: "ImageLocalityWrapped", Weight: &weight1},
					{Name: "DefaultBinderWrapped"},
				}
				return &cfg
			}(),
		},
		{
			name: "success with multiplugin plugin setting/multi manual setting weights have priority.",
			args: args{
				versioned: &configv1.KubeSchedulerConfiguration{
					Profiles: []configv1.KubeSchedulerProfile{
						{
							SchedulerName: &defaultschedulername,
							Plugins: &configv1.Plugins{
								MultiPoint: configv1.PluginSet{
									Enabled: []configv1.Plugin{
										{
											Name:   "NodeResourcesFit",
											Weight: &weight2,
										},
									},
								},
								Score: configv1.PluginSet{
									Enabled: []configv1.Plugin{
										{
											Name:   "NodeResourcesFit",
											Weight: &weight3,
										},
									},
								},
							},
						},
					},
				},
			},
			want: func() *configv1.KubeSchedulerConfiguration {
				cfg := configGeneratedFromDefault()
				cfg.Profiles[0].Plugins.Score.Enabled = []configv1.Plugin{
					{
						Name:   "NodeResourcesFitWrapped",
						Weight: &weight3,
					},
				}
				cfg.Profiles[0].Plugins.MultiPoint.Disabled = []configv1.Plugin{
					{Name: "*"},
				}
				cfg.Profiles[0].Plugins.MultiPoint.Enabled = []configv1.Plugin{
					{Name: "SchedulingGatesWrapped"},
					{Name: "PrioritySortWrapped"},
					{Name: "NodeUnschedulableWrapped"},
					{Name: "NodeNameWrapped"},
					{Name: "TaintTolerationWrapped", Weight: &weight3},
					{Name: "NodeAffinityWrapped", Weight: &weight2},
					{Name: "NodePortsWrapped"},
					{Name: "NodeResourcesFitWrapped", Weight: &weight2},
					{Name: "VolumeRestrictionsWrapped"},
					{Name: "EBSLimitsWrapped"},
					{Name: "GCEPDLimitsWrapped"},
					{Name: "NodeVolumeLimitsWrapped"},
					{Name: "AzureDiskLimitsWrapped"},
					{Name: "VolumeBindingWrapped"},
					{Name: "VolumeZoneWrapped"},
					{Name: "PodTopologySpreadWrapped", Weight: &weight2},
					{Name: "InterPodAffinityWrapped", Weight: &weight2},
					{Name: "DefaultPreemptionWrapped"},
					{Name: "NodeResourcesBalancedAllocationWrapped", Weight: &weight1},
					{Name: "ImageLocalityWrapped", Weight: &weight1},
					{Name: "DefaultBinderWrapped"},
				}
				return &cfg
			}(),
		},
		{
			name: "success with multiplugin plugin setting/disable a specific default multipoint plugin on a extension point",
			args: args{
				versioned: &configv1.KubeSchedulerConfiguration{
					Profiles: []configv1.KubeSchedulerProfile{
						{
							SchedulerName: &defaultschedulername,
							Plugins: &configv1.Plugins{
								Score: configv1.PluginSet{
									Disabled: []configv1.Plugin{
										{
											Name: "NodeResourcesFit",
										},
									},
								},
							},
						},
					},
				},
			},
			want: func() *configv1.KubeSchedulerConfiguration {
				cfg := configGeneratedFromDefault()
				cfg.Profiles[0].Plugins.Score.Disabled = []configv1.Plugin{
					{
						Name: "NodeResourcesFitWrapped",
					},
				}
				cfg.Profiles[0].Plugins.MultiPoint.Disabled = []configv1.Plugin{
					{Name: "*"},
				}
				return &cfg
			}(),
		},
		{
			name: "success with multiplugin plugin setting/disable a specific default multipoint plugin",
			args: args{
				versioned: &configv1.KubeSchedulerConfiguration{
					Parallelism: &nondefaultParallelism,
					Profiles: []configv1.KubeSchedulerProfile{
						{
							SchedulerName: &defaultschedulername,
							Plugins: &configv1.Plugins{
								MultiPoint: configv1.PluginSet{
									Disabled: []configv1.Plugin{
										{
											Name: "NodeResourcesFit",
										},
									},
								},
							},
						},
					},
				},
			},
			want: func() *configv1.KubeSchedulerConfiguration {
				cfg := configGeneratedFromDefault()
				cfg.Profiles[0].Plugins.MultiPoint.Disabled = []configv1.Plugin{
					{Name: "*"},
				}
				cfg.Profiles[0].Plugins.MultiPoint.Enabled = []configv1.Plugin{
					{Name: "SchedulingGatesWrapped"},
					{Name: "PrioritySortWrapped"},
					{Name: "NodeUnschedulableWrapped"},
					{Name: "NodeNameWrapped"},
					{Name: "TaintTolerationWrapped", Weight: &weight3},
					{Name: "NodeAffinityWrapped", Weight: &weight2},
					{Name: "NodePortsWrapped"},
					{Name: "VolumeRestrictionsWrapped"},
					{Name: "EBSLimitsWrapped"},
					{Name: "GCEPDLimitsWrapped"},
					{Name: "NodeVolumeLimitsWrapped"},
					{Name: "AzureDiskLimitsWrapped"},
					{Name: "VolumeBindingWrapped"},
					{Name: "VolumeZoneWrapped"},
					{Name: "PodTopologySpreadWrapped", Weight: &weight2},
					{Name: "InterPodAffinityWrapped", Weight: &weight2},
					{Name: "DefaultPreemptionWrapped"},
					{Name: "NodeResourcesBalancedAllocationWrapped", Weight: &weight1},
					{Name: "ImageLocalityWrapped", Weight: &weight1},
					{Name: "DefaultBinderWrapped"},
				}
				return &cfg
			}(),
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()

			got, err := ConvertConfigurationForSimulator(tt.args.versioned)
			if (err != nil) != tt.wantErr {
				t.Errorf("ConvertConfigurationForSimulator() error = %v, wantErr %v", err, tt.wantErr)
				return
			}
			if len(got.Profiles) != len(tt.want.Profiles) {
				t.Errorf("unmatch length of profiles, want: %v, got: %v", len(tt.want.Profiles), len(got.Profiles))
				return
			}
			if len(got.Extenders) != len(tt.want.Extenders) {
				t.Errorf("unmatch length of extenders, want: %v, got: %v", len(tt.want.Extenders), len(got.Extenders))
				return
			}

			for k := range got.Profiles {
				sort.SliceStable(got.Profiles[k].PluginConfig, func(i, j int) bool {
					return got.Profiles[k].PluginConfig[i].Name < got.Profiles[k].PluginConfig[j].Name
				})
				sort.SliceStable(tt.want.Profiles[k].PluginConfig, func(i, j int) bool {
					return tt.want.Profiles[k].PluginConfig[i].Name < tt.want.Profiles[k].PluginConfig[j].Name
				})
			}

			for i := range tt.want.Profiles {
				assert.Equal(t, tt.want.Profiles[i].Plugins, got.Profiles[i].Plugins)
				assert.Equal(t, tt.want.Profiles[i].PluginConfig, got.Profiles[i].PluginConfig)
			}
			assert.Equal(t, tt.want.Extenders, got.Extenders)
		})
	}
}

func configGeneratedFromDefault() configv1.KubeSchedulerConfiguration {
	versioned, _ := schedConfig.DefaultSchedulerConfig()
	cfg := versioned.DeepCopy()

	cfg.Profiles[0].Plugins.MultiPoint.Enabled = []configv1.Plugin{
		{Name: "SchedulingGatesWrapped"},
		{Name: "PrioritySortWrapped"},
		{Name: "NodeUnschedulableWrapped"},
		{Name: "NodeNameWrapped"},
		{Name: "TaintTolerationWrapped", Weight: &weight3},
		{Name: "NodeAffinityWrapped", Weight: &weight2},
		{Name: "NodePortsWrapped"},
		{Name: "NodeResourcesFitWrapped", Weight: &weight1},
		{Name: "VolumeRestrictionsWrapped"},
		{Name: "EBSLimitsWrapped"},
		{Name: "GCEPDLimitsWrapped"},
		{Name: "NodeVolumeLimitsWrapped"},
		{Name: "AzureDiskLimitsWrapped"},
		{Name: "VolumeBindingWrapped"},
		{Name: "VolumeZoneWrapped"},
		{Name: "PodTopologySpreadWrapped", Weight: &weight2},
		{Name: "InterPodAffinityWrapped", Weight: &weight2},
		{Name: "DefaultPreemptionWrapped"},
		{Name: "NodeResourcesBalancedAllocationWrapped", Weight: &weight1},
		{Name: "ImageLocalityWrapped", Weight: &weight1},
		{Name: "DefaultBinderWrapped"},
	}
	cfg.Profiles[0].Plugins.MultiPoint.Disabled = []configv1.Plugin{
		{Name: "*"},
	}

	pcMap := map[string]runtime.RawExtension{}
	for _, c := range cfg.Profiles[0].PluginConfig {
		pcMap[c.Name] = c.Args
	}

	var newpc []configv1.PluginConfig
	newpc = append(newpc, configv1.PluginConfig{
		Name: "NodeResourcesBalancedAllocationWrapped",
		Args: pcMap["NodeResourcesBalancedAllocation"],
	})
	newpc = append(newpc, configv1.PluginConfig{
		Name: "InterPodAffinityWrapped",
		Args: pcMap["InterPodAffinity"],
	})
	newpc = append(newpc, configv1.PluginConfig{
		Name: "NodeResourcesFitWrapped",
		Args: pcMap["NodeResourcesFit"],
	})
	newpc = append(newpc, configv1.PluginConfig{
		Name: "NodeAffinityWrapped",
		Args: pcMap["NodeAffinity"],
	})
	newpc = append(newpc, configv1.PluginConfig{
		Name: "PodTopologySpreadWrapped",
		Args: pcMap["PodTopologySpread"],
	})
	newpc = append(newpc, configv1.PluginConfig{
		Name: "VolumeBindingWrapped",
		Args: pcMap["VolumeBinding"],
	})
	newpc = append(newpc, configv1.PluginConfig{
		Name: "DefaultPreemptionWrapped",
		Args: pcMap["DefaultPreemption"],
	})

	cfg.Profiles[0].PluginConfig = append(cfg.Profiles[0].PluginConfig, newpc...)

	return *cfg
}
</file>

<file path="kube-scheduler-simulator/simulator/scheduler/scheduler.go">
package scheduler

import (
	"context"
	"errors"

	"github.com/docker/docker/api/types/container"
	"github.com/docker/docker/client"
	"golang.org/x/xerrors"
	v1 "k8s.io/api/core/v1"
	clientset "k8s.io/client-go/kubernetes"
	restclient "k8s.io/client-go/rest"
	"k8s.io/klog/v2"
	configv1 "k8s.io/kube-scheduler/config/v1"
	extenderv1 "k8s.io/kube-scheduler/extender/v1"
	"k8s.io/kubernetes/pkg/scheduler/apis/config"
	"k8s.io/kubernetes/pkg/scheduler/apis/config/scheme"
	apiconfigv1 "k8s.io/kubernetes/pkg/scheduler/apis/config/v1"

	simulatorconfig "sigs.k8s.io/kube-scheduler-simulator/simulator/config"
	simulatorschedconfig "sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler/config"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler/plugin"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler/storereflector"
)

// Service manages scheduler.
type Service struct {
	// function to shutdown scheduler.
	shutdownfn func()

	clientset           clientset.Interface
	restclientCfg       *restclient.Config
	initialSchedulerCfg *configv1.KubeSchedulerConfiguration
	currentSchedulerCfg *configv1.KubeSchedulerConfiguration
	extenderService     ExtenderService
	sharedStore         storereflector.Reflector
	simulatorPort       int
}

type ExtenderService interface {
	Filter(id int, args extenderv1.ExtenderArgs) (*extenderv1.ExtenderFilterResult, error)
	Prioritize(id int, args extenderv1.ExtenderArgs) (*extenderv1.HostPriorityList, error)
	Preempt(id int, args extenderv1.ExtenderPreemptionArgs) (*extenderv1.ExtenderPreemptionResult, error)
	Bind(id int, args extenderv1.ExtenderBindingArgs) (*extenderv1.ExtenderBindingResult, error)
}

var ErrServiceDisabled = errors.New("scheduler service is disabled")

// NewSchedulerService starts scheduler and return *Service.
func NewSchedulerService(client clientset.Interface, restclientCfg *restclient.Config, initialSchedulerCfg *configv1.KubeSchedulerConfiguration, simulatorPort int) *Service {
	// sharedStore has some resultstores which are referenced by Registry of Plugins and Extenders.
	sharedStore := storereflector.New()

	initCfg := initialSchedulerCfg.DeepCopy()
	return &Service{clientset: client, restclientCfg: restclientCfg, initialSchedulerCfg: initCfg, sharedStore: sharedStore, simulatorPort: simulatorPort}
}

func restartContainer(ctx context.Context, cli *client.Client, cfg *configv1.KubeSchedulerConfiguration) error {
	containers, err := cli.ContainerList(ctx, container.ListOptions{})
	if err != nil {
		return xerrors.Errorf("failed to get container list: %w", err)
	}
	for _, c := range containers {
		if c.Names[0] != "/simulator-scheduler" {
			continue
		}
		if err := simulatorschedconfig.UpdateSchedulerConfig(cfg); err != nil {
			return xerrors.Errorf("read old scheduler.yaml: %w", err)
		}

		if err := cli.ContainerRestart(ctx, c.ID, container.StopOptions{}); err != nil {
			return xerrors.Errorf("failed restart container: %w", err)
		}
		inspect, err := cli.ContainerInspect(ctx, c.ID)
		if err != nil {
			return xerrors.Errorf("failed get container inspect: %w", err)
		}
		if inspect.State.Status != "running" {
			return xerrors.Errorf("restart container status is not running")
		}
		return nil
	}

	return xerrors.New("can not find simulator-scheduler, are you running the debuggable scheduler along with this simulator container?")
}

// RestartScheduler restarts the debuggable scheduler with a new config.
// Specifically, it updates the config file, which is also mounted on the debuggable scheduler,
// and then restart the debuggable scheduler.
func (s *Service) RestartScheduler(cfg *configv1.KubeSchedulerConfiguration) error {
	ctx := context.Background()
	cli, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())
	if err != nil {
		return xerrors.Errorf("failed to create docker client: %w", err)
	}

	oldCfg, err := simulatorconfig.GetSchedulerCfg()
	if err != nil {
		return xerrors.Errorf("read old scheduler.yaml: %w", err)
	}

	if err := restartContainer(ctx, cli, cfg); err != nil {
		klog.Errorf("failed to apply new scheduler config: %v", err)
		// If failing restarting the container, we roll back to the old config.
		if err := restartContainer(ctx, cli, oldCfg); err != nil {
			return xerrors.Errorf("oldConfig restart failed: %w", err)
		}
	}
	s.SetSchedulerConfig(cfg)
	return nil
}

func (s *Service) ResetScheduler() error {
	return s.RestartScheduler(s.initialSchedulerCfg.DeepCopy())
}

func (s *Service) ShutdownScheduler() {
	if s.shutdownfn != nil {
		klog.Info("shutdown scheduler...")
		s.shutdownfn()
	}
}

func (s *Service) GetSchedulerConfig() (*configv1.KubeSchedulerConfiguration, error) {
	return s.currentSchedulerCfg, nil
}

func (s *Service) SetSchedulerConfig(cfg *configv1.KubeSchedulerConfiguration) {
	s.currentSchedulerCfg = cfg.DeepCopy()
}

// ExtenderService returns ExtenderService interface.
func (s *Service) ExtenderService() ExtenderService {
	return s.extenderService
}

// ConvertConfigurationForSimulator convert KubeSchedulerConfiguration to apply scheduler on simulator
// (1) It replaces all default-plugins with plugins for simulator.
// (2) It replaces Extenders config so that the connection is directed to the simulator server.
// (3) It converts KubeSchedulerConfiguration from configv1.KubeSchedulerConfiguration to config.KubeSchedulerConfiguration.
func ConvertConfigurationForSimulator(versioned *configv1.KubeSchedulerConfiguration) (*configv1.KubeSchedulerConfiguration, error) {
	if len(versioned.Profiles) == 0 {
		defaultSchedulerName := v1.DefaultSchedulerName
		versioned.Profiles = []configv1.KubeSchedulerProfile{
			{
				SchedulerName: &defaultSchedulerName,
				Plugins:       &configv1.Plugins{},
			},
		}
	}

	for i := range versioned.Profiles {
		if versioned.Profiles[i].Plugins == nil {
			versioned.Profiles[i].Plugins = &configv1.Plugins{}
		}

		plugins, err := plugin.ConvertForSimulator(versioned.Profiles[i].Plugins)
		if err != nil {
			return nil, xerrors.Errorf("convert plugins for simulator: %w", err)
		}
		versioned.Profiles[i].Plugins = plugins

		pluginConfigForSimulatorPlugins, err := plugin.NewPluginConfig(versioned.Profiles[i].PluginConfig)
		if err != nil {
			return nil, xerrors.Errorf("get plugin configs: %w", err)
		}
		versioned.Profiles[i].PluginConfig = pluginConfigForSimulatorPlugins
	}

	apiconfigv1.SetDefaults_KubeSchedulerConfiguration(versioned)

	return versioned, nil
}

func ConvertSchedulerConfigToInternalConfig(versioned *configv1.KubeSchedulerConfiguration) (*config.KubeSchedulerConfiguration, error) {
	cfg := config.KubeSchedulerConfiguration{}
	if err := scheme.Scheme.Convert(versioned, &cfg, nil); err != nil {
		return nil, xerrors.Errorf("convert configuration: %w", err)
	}
	cfg.SetGroupVersionKind(configv1.SchemeGroupVersion.WithKind("KubeSchedulerConfiguration"))

	return &cfg, nil
}
</file>

<file path="kube-scheduler-simulator/simulator/server/di/di.go">
// Package di organizes the dependencies.
// All services are only initialized on this package.
// di means dependency injection.
package di

import (
	clientv3 "go.etcd.io/etcd/client/v3"
	"golang.org/x/xerrors"
	"k8s.io/apimachinery/pkg/api/meta"
	"k8s.io/client-go/dynamic"
	clientset "k8s.io/client-go/kubernetes"
	restclient "k8s.io/client-go/rest"
	configv1 "k8s.io/kube-scheduler/config/v1"

	"sigs.k8s.io/kube-scheduler-simulator/simulator/oneshotimporter"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/reset"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/resourceapplier"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/resourcewatcher"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/snapshot"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/syncer"
)

// Container saves and provides dependencies.
type Container struct {
	schedulerService               SchedulerService
	snapshotService                SnapshotService
	resetService                   ResetService
	oneshotClusterResourceImporter OneShotClusterResourceImporter
	resourceSyncer                 ResourceSyncer
	resourceWatcherService         ResourceWatcherService
}

// NewDIContainer initializes Container.
// It initializes all service and puts to Container.
// Only when externalImportEnabled is true, the simulator uses externalClient and creates ImportClusterResourceService.
func NewDIContainer(
	client clientset.Interface,
	dynamicClient dynamic.Interface,
	restMapper meta.RESTMapper,
	etcdclient *clientv3.Client,
	restclientCfg *restclient.Config,
	initialSchedulerCfg *configv1.KubeSchedulerConfiguration,
	externalImportEnabled bool,
	resourceSyncEnabled bool,
	externalDynamicClient dynamic.Interface,
	simulatorPort int,
	resourceapplierOptions resourceapplier.Options,
) (*Container, error) {
	c := &Container{}

	// initializes each service
	c.schedulerService = scheduler.NewSchedulerService(client, restclientCfg, initialSchedulerCfg, simulatorPort)
	var err error
	c.resetService, err = reset.NewResetService(etcdclient, client, c.schedulerService)
	if err != nil {
		return nil, xerrors.Errorf("initialize reset service: %w", err)
	}
	snapshotSvc := snapshot.NewService(client, c.schedulerService)
	c.snapshotService = snapshotSvc
	resourceApplierService := resourceapplier.New(dynamicClient, restMapper, resourceapplierOptions)
	if externalImportEnabled {
		c.oneshotClusterResourceImporter = oneshotimporter.NewService(externalDynamicClient, resourceApplierService)
	}
	if resourceSyncEnabled {
		c.resourceSyncer = syncer.New(externalDynamicClient, resourceApplierService)
	}
	c.resourceWatcherService = resourcewatcher.NewService(client)

	return c, nil
}

// SchedulerService returns SchedulerService.
func (c *Container) SchedulerService() SchedulerService {
	return c.schedulerService
}

// ExportService returns ExportService.
func (c *Container) ExportService() SnapshotService {
	return c.snapshotService
}

// ResetService returns ResetService.
func (c *Container) ResetService() ResetService {
	return c.resetService
}

// OneshotClusterResourceImporter returns OneshotClusterResourceImporter.
// Note: this service will return nil when `externalImportEnabled` is false.
func (c *Container) OneshotClusterResourceImporter() OneShotClusterResourceImporter {
	return c.oneshotClusterResourceImporter
}

// ResourceSyncer returns ResourceSyncer.
func (c *Container) ResourceSyncer() ResourceSyncer {
	return c.resourceSyncer
}

// ResourceWatcherService returns ResourceWatcherService.
func (c *Container) ResourceWatcherService() ResourceWatcherService {
	return c.resourceWatcherService
}

// ExtenderService returns ExtenderService.
func (c *Container) ExtenderService() ExtenderService {
	return c.schedulerService.ExtenderService()
}
</file>

<file path="kube-scheduler-simulator/simulator/server/di/interface.go">
package di

import (
	"context"

	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	configv1 "k8s.io/kube-scheduler/config/v1"
	extenderv1 "k8s.io/kube-scheduler/extender/v1"

	"sigs.k8s.io/kube-scheduler-simulator/simulator/resourcewatcher"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/resourcewatcher/streamwriter"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/snapshot"
)

// SchedulerService represents service for manage scheduler.
type SchedulerService interface {
	GetSchedulerConfig() (*configv1.KubeSchedulerConfiguration, error)
	SetSchedulerConfig(cfg *configv1.KubeSchedulerConfiguration)
	RestartScheduler(cfg *configv1.KubeSchedulerConfiguration) error
	ResetScheduler() error
	ShutdownScheduler()
	ExtenderService() scheduler.ExtenderService
}

// SnapshotService represents a service for exporting/importing resources on the simulator.
type SnapshotService interface {
	Snap(ctx context.Context, opts ...snapshot.Option) (*snapshot.ResourcesForSnap, error)
	Load(ctx context.Context, resources *snapshot.ResourcesForLoad, opts ...snapshot.Option) error
	IgnoreErr() snapshot.Option
}

type ResetService interface {
	Reset(ctx context.Context) error
}

// OneShotClusterResourceImporter represents a service to import resources from an target cluster when starting the simulator.
type OneShotClusterResourceImporter interface {
	ImportClusterResources(ctx context.Context, labelSelector metav1.LabelSelector) error
}

// ResourceSyncer represents a service to constantly sync resources from an target cluster.
type ResourceSyncer interface {
	// Run starts the resource syncer.
	// It should be run until the context is canceled.
	Run(ctx context.Context) error
}

// ResourceWatcherService represents service for watch k8s resources.
type ResourceWatcherService interface {
	ListWatch(ctx context.Context, stream streamwriter.ResponseStream, lrVersions *resourcewatcher.LastResourceVersions) error
}

// ExtenderService represents service for the extender of scheduler.
type ExtenderService interface {
	Filter(id int, args extenderv1.ExtenderArgs) (*extenderv1.ExtenderFilterResult, error)
	Prioritize(id int, args extenderv1.ExtenderArgs) (*extenderv1.HostPriorityList, error)
	Preempt(id int, args extenderv1.ExtenderPreemptionArgs) (*extenderv1.ExtenderPreemptionResult, error)
	Bind(id int, args extenderv1.ExtenderBindingArgs) (*extenderv1.ExtenderBindingResult, error)
}
</file>

<file path="kube-scheduler-simulator/simulator/server/handler/extender.go">
package handler

import (
	"net/http"
	"strconv"

	"github.com/labstack/echo/v4"
	"k8s.io/klog/v2"
	extenderv1 "k8s.io/kube-scheduler/extender/v1"

	"sigs.k8s.io/kube-scheduler-simulator/simulator/server/di"
)

// ExtenderHandler is a handler about scheduling.
type ExtenderHandler struct {
	service di.ExtenderService
}

func NewExtenderHandler(s di.ExtenderService) *ExtenderHandler {
	return &ExtenderHandler{
		service: s,
	}
}

// Filter request the original extender server which is specified by user,
// and return the response as is.
func (h *ExtenderHandler) Filter(c echo.Context) error {
	id, err := strconv.Atoi(c.Param("id"))
	if err != nil {
		klog.Errorf("failed to convert id to integer: %+v", err)
		return echo.NewHTTPError(http.StatusBadRequest)
	}
	req := new(extenderv1.ExtenderArgs)
	if err = c.Bind(req); err != nil {
		klog.Errorf("failed to bind the Filter request: %+v", err)
		return echo.NewHTTPError(http.StatusBadRequest)
	}

	res, err := h.service.Filter(id, *req)
	if err != nil {
		klog.Errorf("failed to Filter request to the extender's actually host server: %+v", err)
		return echo.NewHTTPError(http.StatusInternalServerError)
	}
	return c.JSON(http.StatusOK, res)
}

// Prioritize request the original extender server which is specified by user,
// and return the response as is.
func (h *ExtenderHandler) Prioritize(c echo.Context) error {
	id, err := strconv.Atoi(c.Param("id"))
	if err != nil {
		klog.Errorf("failed to convert id to integer: %+v", err)
		return echo.NewHTTPError(http.StatusBadRequest)
	}
	req := new(extenderv1.ExtenderArgs)
	if err = c.Bind(req); err != nil {
		klog.Errorf("failed to bind the Prioritize request: %+v", err)
		return echo.NewHTTPError(http.StatusBadRequest)
	}

	res, err := h.service.Prioritize(id, *req)
	if err != nil {
		klog.Errorf("failed to Prioritize request to the extender's actually host server: %+v", err)
		return echo.NewHTTPError(http.StatusInternalServerError)
	}
	return c.JSON(http.StatusOK, res)
}

// Preempt request the original extender server which is specified by user,
// and return the response as is.
func (h *ExtenderHandler) Preempt(c echo.Context) error {
	id, err := strconv.Atoi(c.Param("id"))
	if err != nil {
		klog.Errorf("failed to convert id to integer: %+v", err)
		return echo.NewHTTPError(http.StatusBadRequest)
	}
	req := new(extenderv1.ExtenderPreemptionArgs)
	if err = c.Bind(req); err != nil {
		klog.Errorf("failed to bind the preempt request: %+v", err)
		return echo.NewHTTPError(http.StatusBadRequest)
	}

	res, err := h.service.Preempt(id, *req)
	if err != nil {
		klog.Errorf("failed to Preempt request to the extender's actually host server: %+v", err)
		return echo.NewHTTPError(http.StatusInternalServerError)
	}
	return c.JSON(http.StatusOK, res)
}

// Bind request the original extender server which is specified by user,
// and return the response as is.
func (h *ExtenderHandler) Bind(c echo.Context) error {
	id, err := strconv.Atoi(c.Param("id"))
	if err != nil {
		klog.Errorf("failed to convert id to integer: %+v", err)
		return echo.NewHTTPError(http.StatusBadRequest)
	}
	req := new(extenderv1.ExtenderBindingArgs)
	if err = c.Bind(req); err != nil {
		klog.Errorf("failed to bind the bind request: %+v", err)
		return echo.NewHTTPError(http.StatusBadRequest)
	}

	res, err := h.service.Bind(id, *req)
	if err != nil {
		klog.Errorf("failed to bind request to the extender's actually host server: %+v", err)
		return echo.NewHTTPError(http.StatusInternalServerError)
	}
	return c.JSON(http.StatusOK, res)
}
</file>

<file path="kube-scheduler-simulator/simulator/server/handler/reset.go">
package handler

import (
	"net/http"

	"github.com/labstack/echo/v4"
	"k8s.io/klog/v2"

	"sigs.k8s.io/kube-scheduler-simulator/simulator/server/di"
)

// ResetHandler is handler for clean up resources and scheduler configuration.
type ResetHandler struct {
	service di.ResetService
}

// NewResetHandler initializes ResetHandler.
func NewResetHandler(s di.ResetService) *ResetHandler {
	return &ResetHandler{service: s}
}

func (h *ResetHandler) Reset(c echo.Context) error {
	ctx := c.Request().Context()
	if err := h.service.Reset(ctx); err != nil {
		klog.Errorf("failed to reset all resources and schediler configuration: %+v", err)
		return echo.NewHTTPError(http.StatusInternalServerError)
	}
	return c.NoContent(http.StatusAccepted)
}
</file>

<file path="kube-scheduler-simulator/simulator/server/handler/schedulerconfig.go">
package handler

import (
	"errors"
	"net/http"

	"github.com/labstack/echo/v4"
	"k8s.io/klog/v2"
	configv1 "k8s.io/kube-scheduler/config/v1"

	"sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/server/di"
)

// SchedulerConfigHandler is handler for manage scheduler config.
type SchedulerConfigHandler struct {
	service di.SchedulerService
}

func NewSchedulerConfigHandler(s di.SchedulerService) *SchedulerConfigHandler {
	return &SchedulerConfigHandler{
		service: s,
	}
}

func (h *SchedulerConfigHandler) GetSchedulerConfig(c echo.Context) error {
	cfg, err := h.service.GetSchedulerConfig()
	if err != nil && !errors.Is(err, scheduler.ErrServiceDisabled) {
		klog.Errorf("failed to get scheduler config: %+v", err)
		return echo.NewHTTPError(http.StatusInternalServerError)
	}
	if errors.Is(err, scheduler.ErrServiceDisabled) {
		return c.JSON(http.StatusBadRequest, "When using an external scheduler, you cannot see and edit the scheduler configuration.")
	}

	return c.JSON(http.StatusOK, cfg)
}

// ApplySchedulerConfig currently only takes profiles and extenders from the
// posted payload and applies them.
func (h *SchedulerConfigHandler) ApplySchedulerConfig(c echo.Context) error {
	reqSchedulerCfg := new(configv1.KubeSchedulerConfiguration)
	if err := c.Bind(reqSchedulerCfg); err != nil {
		klog.Errorf("failed to bind scheduler config request: %+v", err)
		return echo.NewHTTPError(http.StatusInternalServerError)
	}
	cfg, err := h.service.GetSchedulerConfig()
	if err != nil {
		klog.Errorf("failed to get scheduler config: %+v", err)
		return echo.NewHTTPError(http.StatusInternalServerError)
	}

	cfg = cfg.DeepCopy()
	cfg.Profiles = reqSchedulerCfg.Profiles
	cfg.Extenders = reqSchedulerCfg.Extenders
	if err := h.service.RestartScheduler(cfg); err != nil {
		klog.Errorf("failed to restart scheduler: %+v", err)
		return echo.NewHTTPError(http.StatusInternalServerError)
	}

	return c.NoContent(http.StatusAccepted)
}
</file>

<file path="kube-scheduler-simulator/simulator/server/handler/snapshot.go">
package handler

import (
	"net/http"

	"github.com/labstack/echo/v4"
	v1 "k8s.io/client-go/applyconfigurations/core/v1"
	schedulingcfgv1 "k8s.io/client-go/applyconfigurations/scheduling/v1"
	confstoragev1 "k8s.io/client-go/applyconfigurations/storage/v1"
	"k8s.io/klog/v2"
	configv1 "k8s.io/kube-scheduler/config/v1"

	"sigs.k8s.io/kube-scheduler-simulator/simulator/server/di"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/snapshot"
)

type SnapshotHandler struct {
	service di.SnapshotService
}

type ResourcesForLoad struct {
	Pods            []v1.PodApplyConfiguration                        `json:"pods"`
	Nodes           []v1.NodeApplyConfiguration                       `json:"nodes"`
	Pvs             []v1.PersistentVolumeApplyConfiguration           `json:"pvs"`
	Pvcs            []v1.PersistentVolumeClaimApplyConfiguration      `json:"pvcs"`
	StorageClasses  []confstoragev1.StorageClassApplyConfiguration    `json:"storageClasses"`
	PriorityClasses []schedulingcfgv1.PriorityClassApplyConfiguration `json:"priorityClasses"`
	SchedulerConfig *configv1.KubeSchedulerConfiguration              `json:"schedulerConfig"`
	Namespaces      []v1.NamespaceApplyConfiguration                  `json:"namespaces"`
}

func NewSnapshotHandler(s di.SnapshotService) *SnapshotHandler {
	return &SnapshotHandler{service: s}
}

func (h *SnapshotHandler) Snap(c echo.Context) error {
	ctx := c.Request().Context()

	rs, err := h.service.Snap(ctx)
	if err != nil {
		klog.Errorf("failed to save all resources: %+v", err)
		return echo.NewHTTPError(http.StatusInternalServerError)
	}
	return c.JSON(http.StatusOK, rs)
}

func (h *SnapshotHandler) Load(c echo.Context) error {
	ctx := c.Request().Context()

	reqResources := new(ResourcesForLoad)
	if err := c.Bind(reqResources); err != nil {
		klog.Errorf("failed to bind request: %+v", err)
		return echo.NewHTTPError(http.StatusBadRequest)
	}

	err := h.service.Load(ctx, convertToResourcesApplyConfiguration(reqResources))
	if err != nil {
		klog.Errorf("failed to load all resources: %+v", err)
		return echo.NewHTTPError(http.StatusInternalServerError)
	}
	return c.NoContent(http.StatusOK)
}

// convertToResourcesApplyConfiguration converts from *ResourcesApplyConfiguration to *export.ResourcesApplyConfiguration.
func convertToResourcesApplyConfiguration(r *ResourcesForLoad) *snapshot.ResourcesForLoad {
	return &snapshot.ResourcesForLoad{
		Pods:            r.Pods,
		Nodes:           r.Nodes,
		Pvs:             r.Pvs,
		Pvcs:            r.Pvcs,
		StorageClasses:  r.StorageClasses,
		PriorityClasses: r.PriorityClasses,
		SchedulerConfig: r.SchedulerConfig,
		Namespaces:      r.Namespaces,
	}
}
</file>

<file path="kube-scheduler-simulator/simulator/server/handler/watcher.go">
package handler

import (
	"net/http"

	"github.com/labstack/echo/v4"
	"k8s.io/klog/v2"

	"sigs.k8s.io/kube-scheduler-simulator/simulator/resourcewatcher"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/server/di"
)

// ResourceWatcherHandler is a handler for watching the k8s resources in the simulator.
type ResourceWatcherHandler struct {
	service di.ResourceWatcherService
}

func NewResourceWatcherHandler(s di.ResourceWatcherService) *ResourceWatcherHandler {
	return &ResourceWatcherHandler{service: s}
}

// ListWatchResources provides resource updates using `server-sent events`.
func (h *ResourceWatcherHandler) ListWatchResources(c echo.Context) error {
	ctx := c.Request().Context()
	// If key is not present, FormValue returns the empty string.
	versions := &resourcewatcher.LastResourceVersions{
		Pods:       c.FormValue("podsLastResourceVersion"),
		Nodes:      c.FormValue("nodesLastResourceVersion"),
		Pvs:        c.FormValue("pvsLastResourceVersion"),
		Pvcs:       c.FormValue("pvcsLastResourceVersion"),
		Scs:        c.FormValue("scsLastResourceVersion"),
		Pcs:        c.FormValue("pcsLastResourceVersion"),
		Namespaces: c.FormValue("namespaceLastResourceVersion"),
	}
	c.Response().Header().Set(echo.HeaderContentType, echo.MIMEApplicationJSON)
	c.Response().WriteHeader(http.StatusOK)
	// Start to watch and do server push
	err := h.service.ListWatch(ctx, c.Response(), versions)
	if err != nil {
		klog.Errorf("terminated to watch resources: %+v", err)
		return echo.NewHTTPError(http.StatusInternalServerError)
	}
	// We expect this line will be called when the connection is canceled by the client.
	return c.NoContent(http.StatusOK)
}
</file>

<file path="kube-scheduler-simulator/simulator/server/server.go">
package server

import (
	"context"
	"errors"
	"net/http"
	"strconv"
	"time"

	"github.com/labstack/echo/v4"
	"github.com/labstack/echo/v4/middleware"
	"github.com/labstack/gommon/log"

	"sigs.k8s.io/kube-scheduler-simulator/simulator/config"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/server/di"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/server/handler"
)

// SimulatorServer is server for simulator.
type SimulatorServer struct {
	e *echo.Echo
}

// NewSimulatorServer initialize SimulatorServer.
func NewSimulatorServer(cfg *config.Config, dic *di.Container) *SimulatorServer {
	e := echo.New()

	e.Use(middleware.Logger())
	e.Use(middleware.CORSWithConfig(middleware.CORSConfig{
		AllowOrigins:     cfg.CorsAllowedOriginList,
		AllowCredentials: true,
	}))

	// initialize each handler
	schedulercfgHandler := handler.NewSchedulerConfigHandler(dic.SchedulerService())
	snapshotHandler := handler.NewSnapshotHandler(dic.ExportService())
	resetHandler := handler.NewResetHandler(dic.ResetService())
	resourcewatcherHandler := handler.NewResourceWatcherHandler(dic.ResourceWatcherService())
	extenderHandler := handler.NewExtenderHandler(dic.ExtenderService())

	// register apis
	v1 := e.Group("/api/v1")

	v1.GET("/schedulerconfiguration", schedulercfgHandler.GetSchedulerConfig)
	v1.POST("/schedulerconfiguration", schedulercfgHandler.ApplySchedulerConfig)

	v1.PUT("/reset", resetHandler.Reset)

	v1.GET("/export", snapshotHandler.Snap)
	v1.POST("/import", snapshotHandler.Load)

	v1.GET("/listwatchresources", resourcewatcherHandler.ListWatchResources)

	RouteExtender(v1, extenderHandler)

	// initialize SimulatorServer.
	s := &SimulatorServer{e: e}
	s.e.Logger.SetLevel(log.INFO)

	return s
}

// Start starts SimulatorServer.
func (s *SimulatorServer) Start(port int) (
	func(), // function for shutdown
	error,
) {
	e := s.e

	go func() {
		if err := e.Start(":" + strconv.Itoa(port)); err != nil && !errors.Is(err, http.ErrServerClosed) {
			e.Logger.Fatalf("failed to start server successfully: %v", err)
		}
	}()

	shutdownFn := func() {
		ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
		defer cancel()
		if err := e.Shutdown(ctx); err != nil {
			e.Logger.Warnf("failed to shutdown simulator server successfully: %v", err)
		}
	}

	return shutdownFn, nil
}

// RouteExtender routes request for extender to 4 endpoints.
func RouteExtender(v1 *echo.Group, handler *handler.ExtenderHandler) {
	v1.POST("/extender/filter/:id", handler.Filter)
	v1.POST("/extender/prioritize/:id", handler.Prioritize)
	v1.POST("/extender/preempt/:id", handler.Preempt)
	v1.POST("/extender/bind/:id", handler.Bind)
}
</file>

<file path="kube-scheduler-simulator/simulator/snapshot/mock_snapshot/scheduler.go">
// Code generated by MockGen. DO NOT EDIT.
// Source: sigs.k8s.io/kube-scheduler-simulator/simulator/snapshot (interfaces: SchedulerService)
//
// Generated by this command:
//
//	mockgen -destination=./mock_snapshot/scheduler.go . SchedulerService
//

// Package mock_snapshot is a generated GoMock package.
package mock_snapshot

import (
	reflect "reflect"

	gomock "go.uber.org/mock/gomock"
	v1 "k8s.io/kube-scheduler/config/v1"
)

// MockSchedulerService is a mock of SchedulerService interface.
type MockSchedulerService struct {
	ctrl     *gomock.Controller
	recorder *MockSchedulerServiceMockRecorder
	isgomock struct{}
}

// MockSchedulerServiceMockRecorder is the mock recorder for MockSchedulerService.
type MockSchedulerServiceMockRecorder struct {
	mock *MockSchedulerService
}

// NewMockSchedulerService creates a new mock instance.
func NewMockSchedulerService(ctrl *gomock.Controller) *MockSchedulerService {
	mock := &MockSchedulerService{ctrl: ctrl}
	mock.recorder = &MockSchedulerServiceMockRecorder{mock}
	return mock
}

// EXPECT returns an object that allows the caller to indicate expected use.
func (m *MockSchedulerService) EXPECT() *MockSchedulerServiceMockRecorder {
	return m.recorder
}

// GetSchedulerConfig mocks base method.
func (m *MockSchedulerService) GetSchedulerConfig() (*v1.KubeSchedulerConfiguration, error) {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "GetSchedulerConfig")
	ret0, _ := ret[0].(*v1.KubeSchedulerConfiguration)
	ret1, _ := ret[1].(error)
	return ret0, ret1
}

// GetSchedulerConfig indicates an expected call of GetSchedulerConfig.
func (mr *MockSchedulerServiceMockRecorder) GetSchedulerConfig() *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "GetSchedulerConfig", reflect.TypeOf((*MockSchedulerService)(nil).GetSchedulerConfig))
}

// RestartScheduler mocks base method.
func (m *MockSchedulerService) RestartScheduler(cfg *v1.KubeSchedulerConfiguration) error {
	m.ctrl.T.Helper()
	ret := m.ctrl.Call(m, "RestartScheduler", cfg)
	ret0, _ := ret[0].(error)
	return ret0
}

// RestartScheduler indicates an expected call of RestartScheduler.
func (mr *MockSchedulerServiceMockRecorder) RestartScheduler(cfg any) *gomock.Call {
	mr.mock.ctrl.T.Helper()
	return mr.mock.ctrl.RecordCallWithMethodType(mr.mock, "RestartScheduler", reflect.TypeOf((*MockSchedulerService)(nil).RestartScheduler), cfg)
}
</file>

<file path="kube-scheduler-simulator/simulator/snapshot/snapshot_test.go">
package snapshot

import (
	"context"
	"encoding/json"
	"strings"
	"testing"

	"github.com/google/go-cmp/cmp"
	"github.com/stretchr/testify/assert"
	"go.uber.org/mock/gomock"
	"golang.org/x/xerrors"
	corev1 "k8s.io/api/core/v1"
	schedulingv1 "k8s.io/api/scheduling/v1"
	storagev1 "k8s.io/api/storage/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	v1 "k8s.io/client-go/applyconfigurations/core/v1"
	schedulingcfgv1 "k8s.io/client-go/applyconfigurations/scheduling/v1"
	confstoragev1 "k8s.io/client-go/applyconfigurations/storage/v1"
	"k8s.io/client-go/kubernetes/fake"
	k8stesting "k8s.io/client-go/testing"
	configv1 "k8s.io/kube-scheduler/config/v1"

	"sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler"
	schedulerCfg "sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler/config"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/snapshot/mock_snapshot"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/util"
)

const (
	testNamespace1 = "default1"
	testNamespace2 = "default2"
)

type (
	resourceName         string
	SettingClientFunc    func(context.Context, *fake.Clientset)
	SettingClientFuncMap map[resourceName]SettingClientFunc
)

var (
	ns   resourceName = "namespace"
	node resourceName = "node"
	pod  resourceName = "pod"
	pv   resourceName = "persistentVolume"
	pvc  resourceName = "persistentVolumeClaim"
	sc   resourceName = "storageClass"
	pc   resourceName = "priorityClass"
	// order indicates calling order in the invokeResourcesFn method.
	order = []resourceName{
		ns, node, pod, pv, pvc, sc, pc,
	}
	// defaultResForSnapFn returns default expected ResourcesForSnap.
	defaultResForSnapFn = func() *ResourcesForSnap {
		return &ResourcesForSnap{
			Pods: []corev1.Pod{
				{
					ObjectMeta: metav1.ObjectMeta{
						Name:      "pod1",
						Namespace: testNamespace1,
					},
					Spec: corev1.PodSpec{
						NodeName: "node1",
					},
				},
			},
			Nodes: []corev1.Node{
				{
					ObjectMeta: metav1.ObjectMeta{
						Name: "node1",
					},
				},
			},
			Pvs: []corev1.PersistentVolume{
				{
					ObjectMeta: metav1.ObjectMeta{
						Name: "pv1",
					},
				},
			},
			Pvcs: []corev1.PersistentVolumeClaim{
				{
					ObjectMeta: metav1.ObjectMeta{
						Name:      "pvc1",
						Namespace: testNamespace1,
					},
					Spec: corev1.PersistentVolumeClaimSpec{
						VolumeName: "pv1",
					},
				},
			},
			StorageClasses: []storagev1.StorageClass{
				{
					ObjectMeta: metav1.ObjectMeta{
						Name: "sc1",
					},
				},
			},
			PriorityClasses: []schedulingv1.PriorityClass{
				{
					ObjectMeta: metav1.ObjectMeta{
						Name: "pc1",
					},
				},
			},
			SchedulerConfig: &configv1.KubeSchedulerConfiguration{},
			Namespaces: []corev1.Namespace{
				{
					ObjectMeta: metav1.ObjectMeta{
						Name: testNamespace1,
					},
				},
			},
		}
	}
	// defaultApplyFuncs returns default expected settings to fakeClientset for `Apply`.
	defaultApplyFuncs = SettingClientFuncMap{
		ns: func(_ context.Context, c *fake.Clientset) {
			c.PrependReactor("patch", "namespaces", func(_ k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
				return true, &corev1.Namespace{}, nil
			})
		},
		node: func(_ context.Context, c *fake.Clientset) {
			c.PrependReactor("patch", "nodes", func(_ k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
				return true, &corev1.Node{}, nil
			})
		},
		pod: func(_ context.Context, c *fake.Clientset) {
			c.PrependReactor("patch", "pods", func(_ k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
				return true, &corev1.Pod{}, nil
			})
		},
		pv: func(_ context.Context, c *fake.Clientset) {
			c.PrependReactor("patch", "persistentvolumes", func(_ k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
				return true, &corev1.PersistentVolume{}, nil
			})
		},
		pvc: func(_ context.Context, c *fake.Clientset) {
			c.PrependReactor("patch", "persistentvolumeclaims", func(_ k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
				return true, &corev1.PersistentVolumeClaim{}, nil
			})
		},
		sc: func(_ context.Context, c *fake.Clientset) {
			c.PrependReactor("patch", "storageclasses", func(_ k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
				return true, &storagev1.StorageClass{}, nil
			})
		},
		pc: func(_ context.Context, c *fake.Clientset) {
			c.PrependReactor("patch", "priorityclasses", func(_ k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
				return true, &schedulingv1.PriorityClass{}, nil
			})
		},
	}
	// defaultFuncs returns default methods setting to fakeClientset for create each resources.
	defaultFuncs = SettingClientFuncMap{
		ns: func(ctx context.Context, c *fake.Clientset) {
			c.CoreV1().Namespaces().Create(ctx, &corev1.Namespace{
				ObjectMeta: metav1.ObjectMeta{
					Name: testNamespace1,
				},
			}, metav1.CreateOptions{})
		},
		node: func(ctx context.Context, c *fake.Clientset) {
			c.CoreV1().Nodes().Create(ctx, &corev1.Node{
				ObjectMeta: metav1.ObjectMeta{
					Name: "node1",
				},
			}, metav1.CreateOptions{})
		},
		pod: func(ctx context.Context, c *fake.Clientset) {
			c.CoreV1().Pods(testNamespace1).Create(ctx, &corev1.Pod{
				ObjectMeta: metav1.ObjectMeta{
					Name: "pod1",
				},
				Spec: corev1.PodSpec{
					NodeName: "node1",
				},
			}, metav1.CreateOptions{})
		},
		pv: func(ctx context.Context, c *fake.Clientset) {
			c.CoreV1().PersistentVolumes().Create(ctx, &corev1.PersistentVolume{
				ObjectMeta: metav1.ObjectMeta{
					Name: "pv1",
				},
			}, metav1.CreateOptions{})
		},
		pvc: func(ctx context.Context, c *fake.Clientset) {
			c.CoreV1().PersistentVolumeClaims(testNamespace1).Create(ctx, &corev1.PersistentVolumeClaim{
				ObjectMeta: metav1.ObjectMeta{
					Name: "pvc1",
				},
				Spec: corev1.PersistentVolumeClaimSpec{
					VolumeName: "pv1",
				},
			}, metav1.CreateOptions{})
		},
		sc: func(ctx context.Context, c *fake.Clientset) {
			c.StorageV1().StorageClasses().Create(ctx, &storagev1.StorageClass{
				ObjectMeta: metav1.ObjectMeta{
					Name: "sc1",
				},
			}, metav1.CreateOptions{})
		},
		pc: func(ctx context.Context, c *fake.Clientset) {
			c.SchedulingV1().PriorityClasses().Create(ctx, &schedulingv1.PriorityClass{
				ObjectMeta: metav1.ObjectMeta{
					Name: "pc1",
				},
			}, metav1.CreateOptions{})
		},
	}
	// invokeResourcesFn invokes specified individual methods or baseFuncs.
	invokeResourcesFn = func(ctx context.Context, c *fake.Clientset, funcs SettingClientFuncMap, baseFuncs SettingClientFuncMap) {
		for _, n := range order {
			if fn, ok := funcs[n]; ok {
				fn(ctx, c)
				continue
			}
			baseFuncs[n](ctx, c)
		}
	}
)

func TestService_Snap(t *testing.T) {
	t.Parallel()

	tests := []struct {
		name                     string
		prepareEachServiceMockFn func(*mock_snapshot.MockSchedulerService)
		prepareFakeClientSetFn   func() *fake.Clientset
		wantReturn               func() *ResourcesForSnap
		wantErr                  error
	}{
		{
			name: "Snap all resources",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().GetSchedulerConfig().Return(&configv1.KubeSchedulerConfiguration{}, nil)
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				ctx := context.Background()
				// add test data.
				invokeResourcesFn(ctx, c, SettingClientFuncMap{}, defaultFuncs)
				return c
			},
			wantReturn: defaultResForSnapFn,
		},
		{
			name: "Snap failure on List Pod",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().GetSchedulerConfig().Return(&configv1.KubeSchedulerConfiguration{}, nil)
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				ctx := context.Background()
				// add test data.
				invokeResourcesFn(ctx, c, SettingClientFuncMap{
					pod: func(_ context.Context, c *fake.Clientset) {
						c.PrependReactor("list", "pods", func(_ k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
							return true, nil, xerrors.New("failed to list Pod")
						})
					},
				}, defaultFuncs)
				return c
			},
			wantErr: xerrors.New("failed to get(): get resources all: call list Pod: failed to list Pod"),
		},
		{
			name: "Snap failure on List Node",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().GetSchedulerConfig().Return(&configv1.KubeSchedulerConfiguration{}, nil)
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				ctx := context.Background()
				// add test data.
				invokeResourcesFn(ctx, c, SettingClientFuncMap{
					node: func(_ context.Context, c *fake.Clientset) {
						c.PrependReactor("list", "nodes", func(_ k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
							return true, nil, xerrors.New("failed to list Node")
						})
					},
				}, defaultFuncs)
				return c
			},
			wantErr: xerrors.New("failed to get(): get resources all: call list Node: failed to list Node"),
		},
		{
			name: "Snap failure on List PersistentVolume",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().GetSchedulerConfig().Return(&configv1.KubeSchedulerConfiguration{}, nil)
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				ctx := context.Background()
				// add test data.
				invokeResourcesFn(ctx, c, SettingClientFuncMap{
					pv: func(_ context.Context, c *fake.Clientset) {
						c.PrependReactor("list", "persistentvolumes", func(_ k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
							return true, nil, xerrors.New("failed to list PersistentVolume")
						})
					},
				}, defaultFuncs)
				return c
			},
			wantErr: xerrors.New("failed to get(): get resources all: call list PersistentVolume: failed to list PersistentVolume"),
		},
		{
			name: "Snap failure on List PersistentVolumeClaim",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().GetSchedulerConfig().Return(&configv1.KubeSchedulerConfiguration{}, nil)
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				ctx := context.Background()
				// add test data.
				invokeResourcesFn(ctx, c, SettingClientFuncMap{
					pvc: func(_ context.Context, c *fake.Clientset) {
						c.PrependReactor("list", "persistentvolumeclaims", func(_ k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
							return true, nil, xerrors.New("failed to list PersistentVolumeClaim")
						})
					},
				}, defaultFuncs)
				return c
			},
			wantErr: xerrors.New("failed to get(): get resources all: call list PersistentVolumeClaim: failed to list PersistentVolumeClaim"),
		},
		{
			name: "Snap failure on List of StorageClass",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().GetSchedulerConfig().Return(&configv1.KubeSchedulerConfiguration{}, nil)
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				ctx := context.Background()
				// add test data.
				invokeResourcesFn(ctx, c, SettingClientFuncMap{
					sc: func(_ context.Context, c *fake.Clientset) {
						c.PrependReactor("list", "storageclasses", func(_ k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
							return true, nil, xerrors.New("failed to list StorageClass")
						})
					},
				}, defaultFuncs)
				return c
			},
			wantErr: xerrors.New("failed to get(): get resources all: call list StorageClass: failed to list StorageClass"),
		},
		{
			name: "Snap failure on List PriorityClass",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().GetSchedulerConfig().Return(&configv1.KubeSchedulerConfiguration{}, nil)
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				ctx := context.Background()
				// add test data.
				invokeResourcesFn(ctx, c, SettingClientFuncMap{
					pc: func(_ context.Context, c *fake.Clientset) {
						c.PrependReactor("list", "priorityclasses", func(_ k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
							return true, nil, xerrors.New("failed to list PriorityClass")
						})
					},
				}, defaultFuncs)
				return c
			},
			wantErr: xerrors.New("failed to get(): get resources all: call list PriorityClass: failed to list PriorityClass"),
		},
		{
			name: "Snap failure on List Namespace",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().GetSchedulerConfig().Return(&configv1.KubeSchedulerConfiguration{}, nil)
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				ctx := context.Background()
				// add test data.
				invokeResourcesFn(ctx, c, SettingClientFuncMap{
					ns: func(_ context.Context, c *fake.Clientset) {
						c.PrependReactor("list", "namespaces", func(_ k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
							return true, nil, xerrors.New("failed to list Namespace")
						})
					},
				}, defaultFuncs)
				return c
			},
			wantErr: xerrors.New("failed to get(): get resources all: call list Namespace: failed to list Namespace"),
		},
		{
			name: "Snap failure on get scheduler config",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().GetSchedulerConfig().Return(&configv1.KubeSchedulerConfiguration{}, xerrors.New("failed to get config"))
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				ctx := context.Background()
				// add test data.
				invokeResourcesFn(ctx, c, SettingClientFuncMap{}, defaultFuncs)
				return c
			},
			wantErr: xerrors.New("failed to get(): get resources all: get scheduler config: failed to get config"),
		},
		{
			name: "get ErrServiceDisabled on get scheduler config, but it's ignored.",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().GetSchedulerConfig().Return(nil, scheduler.ErrServiceDisabled)
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				ctx := context.Background()
				// add test data.
				invokeResourcesFn(ctx, c, SettingClientFuncMap{}, defaultFuncs)
				return c
			},
			wantReturn: func() *ResourcesForSnap {
				r := defaultResForSnapFn()
				r.SchedulerConfig = nil
				return r
			},
		},
		{
			name: "Snap all Pods with different namespaces",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().GetSchedulerConfig().Return(&configv1.KubeSchedulerConfiguration{}, nil)
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				ctx := context.Background()
				// add test data.
				invokeResourcesFn(ctx, c, SettingClientFuncMap{
					pod: func(_ context.Context, c *fake.Clientset) {
						c.CoreV1().Pods(testNamespace1).Create(ctx, &corev1.Pod{
							ObjectMeta: metav1.ObjectMeta{
								Name: "pod1",
							},
							Spec: corev1.PodSpec{
								NodeName: "node1",
							},
						}, metav1.CreateOptions{})
						c.CoreV1().Pods(testNamespace2).Create(ctx, &corev1.Pod{
							ObjectMeta: metav1.ObjectMeta{
								Name: "pod2",
							},
							Spec: corev1.PodSpec{
								NodeName: "node1",
							},
						}, metav1.CreateOptions{})
					},
				}, defaultFuncs)
				return c
			},
			wantReturn: func() *ResourcesForSnap {
				r := defaultResForSnapFn()
				r.Pods = []corev1.Pod{
					{
						ObjectMeta: metav1.ObjectMeta{Name: "pod1", Namespace: testNamespace1},
						Spec:       corev1.PodSpec{NodeName: "node1"},
					},
					{
						ObjectMeta: metav1.ObjectMeta{Name: "pod2", Namespace: testNamespace2},
						Spec:       corev1.PodSpec{NodeName: "node1"},
					},
				}
				return r
			},
		},
		{
			name: "Snap all pvcs with different namespaces",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().GetSchedulerConfig().Return(&configv1.KubeSchedulerConfiguration{}, nil)
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				ctx := context.Background()
				// add test data.
				invokeResourcesFn(ctx, c, SettingClientFuncMap{
					pvc: func(_ context.Context, c *fake.Clientset) {
						c.CoreV1().PersistentVolumeClaims(testNamespace1).Create(ctx, &corev1.PersistentVolumeClaim{
							ObjectMeta: metav1.ObjectMeta{
								Name: "pvc1",
							},
							Spec: corev1.PersistentVolumeClaimSpec{
								VolumeName: "pv1",
							},
						}, metav1.CreateOptions{})
						c.CoreV1().PersistentVolumeClaims(testNamespace2).Create(ctx, &corev1.PersistentVolumeClaim{
							ObjectMeta: metav1.ObjectMeta{
								Name: "pvc2",
							},
							Spec: corev1.PersistentVolumeClaimSpec{
								VolumeName: "pv1",
							},
						}, metav1.CreateOptions{})
					},
				}, defaultFuncs)
				return c
			},
			wantReturn: func() *ResourcesForSnap {
				r := defaultResForSnapFn()
				r.Pvcs = []corev1.PersistentVolumeClaim{
					{
						ObjectMeta: metav1.ObjectMeta{
							Name:      "pvc1",
							Namespace: testNamespace1,
						},
						Spec: corev1.PersistentVolumeClaimSpec{
							VolumeName: "pv1",
						},
					},
					{
						ObjectMeta: metav1.ObjectMeta{
							Name:      "pvc2",
							Namespace: testNamespace2,
						},
						Spec: corev1.PersistentVolumeClaimSpec{
							VolumeName: "pv1",
						},
					},
				}
				return r
			},
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			ctrl := gomock.NewController(t)
			mockSchedulerSvc := mock_snapshot.NewMockSchedulerService(ctrl)
			fakeClientset := tt.prepareFakeClientSetFn()

			s := NewService(fakeClientset, mockSchedulerSvc)
			tt.prepareEachServiceMockFn(mockSchedulerSvc)
			r, err := s.Snap(context.Background())

			var diffResponse string
			if tt.wantReturn != nil {
				diffResponse = cmp.Diff(tt.wantReturn(), r)
			}

			if diffResponse != "" || (err != nil) != (tt.wantErr != nil) {
				t.Fatalf("Snap() %v test, \nerror = %v,\n%s", tt.name, err, diffResponse)
			}
			if tt.wantErr != nil {
				assert.EqualError(t, tt.wantErr, err.Error())
			}
		})
	}
}

func TestService_Snap_IgnoreErrOption(t *testing.T) {
	t.Parallel()
	tests := []struct {
		name                     string
		prepareEachServiceMockFn func(*mock_snapshot.MockSchedulerService)
		prepareFakeClientSetFn   func() *fake.Clientset
		wantReturn               func() *ResourcesForSnap
		wantErr                  error
	}{
		{
			name: "snap all resources",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().GetSchedulerConfig().Return(&configv1.KubeSchedulerConfiguration{}, nil)
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				ctx := context.Background()
				// add test data.
				invokeResourcesFn(ctx, c, SettingClientFuncMap{}, defaultFuncs)
				return c
			},
			wantReturn: defaultResForSnapFn,
		},
		{
			name: "no error if failure to list Pod",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().GetSchedulerConfig().Return(&configv1.KubeSchedulerConfiguration{}, nil)
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				ctx := context.Background()
				// add test data.
				invokeResourcesFn(ctx, c, SettingClientFuncMap{
					pod: func(_ context.Context, c *fake.Clientset) {
						c.PrependReactor("list", "pods", func(_ k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
							return true, nil, xerrors.New("failed to list Pod")
						})
					},
				}, defaultFuncs)
				return c
			},
			wantReturn: func() *ResourcesForSnap {
				r := defaultResForSnapFn()
				r.Pods = []corev1.Pod{}
				return r
			},
		},
		{
			name: "no error if failure to list Node",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().GetSchedulerConfig().Return(&configv1.KubeSchedulerConfiguration{}, nil)
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				ctx := context.Background()
				// add test data.
				invokeResourcesFn(ctx, c, SettingClientFuncMap{
					node: func(_ context.Context, c *fake.Clientset) {
						c.PrependReactor("list", "nodes", func(_ k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
							return true, nil, xerrors.New("failed to list Node")
						})
					},
				}, defaultFuncs)
				return c
			},
			wantReturn: func() *ResourcesForSnap {
				r := defaultResForSnapFn()
				r.Nodes = []corev1.Node{}
				return r
			},
		},
		{
			name: "no error if failure to list PersistentVolume",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().GetSchedulerConfig().Return(&configv1.KubeSchedulerConfiguration{}, nil)
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				ctx := context.Background()
				// add test data.
				invokeResourcesFn(ctx, c, SettingClientFuncMap{
					pv: func(_ context.Context, c *fake.Clientset) {
						c.PrependReactor("list", "persistentvolumes", func(_ k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
							return true, nil, xerrors.New("failed to list PersistentVolume")
						})
					},
				}, defaultFuncs)
				return c
			},
			wantReturn: func() *ResourcesForSnap {
				r := defaultResForSnapFn()
				r.Pvs = []corev1.PersistentVolume{}
				return r
			},
		},
		{
			name: "no error if failure to list PersistentVolumeClaims",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().GetSchedulerConfig().Return(&configv1.KubeSchedulerConfiguration{}, nil)
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				ctx := context.Background()
				// add test data.
				invokeResourcesFn(ctx, c, SettingClientFuncMap{
					pvc: func(_ context.Context, c *fake.Clientset) {
						c.PrependReactor("list", "persistentvolumeclaims", func(_ k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
							return true, nil, xerrors.New("failed to list PersistentVolumeClaim")
						})
					},
				}, defaultFuncs)
				return c
			},
			wantReturn: func() *ResourcesForSnap {
				r := defaultResForSnapFn()
				r.Pvcs = []corev1.PersistentVolumeClaim{}
				return r
			},
		},
		{
			name: "no error if failure to list storageClasses",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().GetSchedulerConfig().Return(&configv1.KubeSchedulerConfiguration{}, nil)
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				ctx := context.Background()
				// add test data.
				invokeResourcesFn(ctx, c, SettingClientFuncMap{
					sc: func(_ context.Context, c *fake.Clientset) {
						c.PrependReactor("list", "storageclasses", func(_ k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
							return true, nil, xerrors.New("failed to list StorageClass")
						})
					},
				}, defaultFuncs)
				return c
			},
			wantReturn: func() *ResourcesForSnap {
				r := defaultResForSnapFn()
				r.StorageClasses = []storagev1.StorageClass{}
				return r
			},
		},
		{
			name: "no error if failure to list priorityClasses",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().GetSchedulerConfig().Return(&configv1.KubeSchedulerConfiguration{}, nil)
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				ctx := context.Background()
				// add test data.
				invokeResourcesFn(ctx, c, SettingClientFuncMap{
					pc: func(_ context.Context, c *fake.Clientset) {
						c.PrependReactor("list", "priorityclasses", func(_ k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
							return true, nil, xerrors.New("failed to list PriorityClass")
						})
					},
				}, defaultFuncs)
				return c
			},
			wantReturn: func() *ResourcesForSnap {
				r := defaultResForSnapFn()
				r.PriorityClasses = []schedulingv1.PriorityClass{}
				return r
			},
		},
		{
			name: "no error if failure to list Namespace",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().GetSchedulerConfig().Return(&configv1.KubeSchedulerConfiguration{}, nil)
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				ctx := context.Background()
				// add test data.
				invokeResourcesFn(ctx, c, SettingClientFuncMap{
					ns: func(_ context.Context, c *fake.Clientset) {
						c.PrependReactor("list", "namespaces", func(_ k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
							return true, nil, xerrors.New("failed to list Namespace")
						})
					},
				}, defaultFuncs)
				return c
			},
			wantReturn: func() *ResourcesForSnap {
				r := defaultResForSnapFn()
				r.Namespaces = []corev1.Namespace{}
				return r
			},
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			ctrl := gomock.NewController(t)

			fakeClientset := tt.prepareFakeClientSetFn()
			mockSchedulerSvc := mock_snapshot.NewMockSchedulerService(ctrl)
			s := NewService(fakeClientset, mockSchedulerSvc)
			tt.prepareEachServiceMockFn(mockSchedulerSvc)
			r, err := s.Snap(context.Background(), s.IgnoreErr())

			var diffResponse string
			if tt.wantReturn != nil {
				diffResponse = cmp.Diff(tt.wantReturn(), r)
			}

			if diffResponse != "" || (err != nil) != (tt.wantErr != nil) {
				t.Fatalf("Snap() %v test, \nerror = %v,\n%s", tt.name, err, diffResponse)
			}
			if tt.wantErr != nil {
				assert.EqualError(t, tt.wantErr, err.Error())
			}
		})
	}
}

func TestService_Load(t *testing.T) {
	t.Parallel()
	tests := []struct {
		name                     string
		prepareEachServiceMockFn func(*mock_snapshot.MockSchedulerService)
		prepareFakeClientSetFn   func() *fake.Clientset
		applyConfiguration       func() *ResourcesForLoad
		wantErr                  error
	}{
		{
			name: "load all success",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().RestartScheduler(gomock.Any()).Return(nil)
			},
			applyConfiguration: func() *ResourcesForLoad {
				ns := []v1.NamespaceApplyConfiguration{
					*v1.Namespace(testNamespace1),
				}
				pods := []v1.PodApplyConfiguration{
					*v1.Pod("Pod1", testNamespace1),
				}
				nodes := []v1.NodeApplyConfiguration{
					*v1.Node("Node1"),
				}
				pvs := []v1.PersistentVolumeApplyConfiguration{
					*v1.PersistentVolume("PV1").WithStatus(v1.PersistentVolumeStatus().WithPhase("Pending")).WithUID("test"),
				}
				pvcs := []v1.PersistentVolumeClaimApplyConfiguration{
					*v1.PersistentVolumeClaim("PVC1", testNamespace1),
				}
				scs := []confstoragev1.StorageClassApplyConfiguration{
					*confstoragev1.StorageClass("StorageClass1"),
				}
				pcs := []schedulingcfgv1.PriorityClassApplyConfiguration{
					*schedulingcfgv1.PriorityClass("PriorityClass1"),
				}
				config, _ := schedulerCfg.DefaultSchedulerConfig()
				return &ResourcesForLoad{
					Pods:            pods,
					Nodes:           nodes,
					Pvs:             pvs,
					Pvcs:            pvcs,
					StorageClasses:  scs,
					PriorityClasses: pcs,
					SchedulerConfig: config,
					Namespaces:      ns,
				}
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				invokeResourcesFn(context.Background(), c, SettingClientFuncMap{}, defaultApplyFuncs)
				return c
			},
		},
		{
			name: "load all success (with external scheduler enabled.)",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().RestartScheduler(gomock.Any()).Return(scheduler.ErrServiceDisabled)
			},
			applyConfiguration: func() *ResourcesForLoad {
				ns := []v1.NamespaceApplyConfiguration{
					*v1.Namespace(testNamespace1),
				}
				pods := []v1.PodApplyConfiguration{
					*v1.Pod("Pod1", testNamespace1),
				}
				nodes := []v1.NodeApplyConfiguration{
					*v1.Node("Node1"),
				}
				pvs := []v1.PersistentVolumeApplyConfiguration{
					*v1.PersistentVolume("PV1").WithStatus(v1.PersistentVolumeStatus().WithPhase("Pending")).WithUID("test"),
				}
				pvcs := []v1.PersistentVolumeClaimApplyConfiguration{
					*v1.PersistentVolumeClaim("PVC1", testNamespace1),
				}
				scs := []confstoragev1.StorageClassApplyConfiguration{
					*confstoragev1.StorageClass("StorageClass1"),
				}
				pcs := []schedulingcfgv1.PriorityClassApplyConfiguration{
					*schedulingcfgv1.PriorityClass("PriorityClass1"),
				}
				config, _ := schedulerCfg.DefaultSchedulerConfig()
				return &ResourcesForLoad{
					Pods:            pods,
					Nodes:           nodes,
					Pvs:             pvs,
					Pvcs:            pvcs,
					StorageClasses:  scs,
					PriorityClasses: pcs,
					SchedulerConfig: config,
					Namespaces:      ns,
				}
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				invokeResourcesFn(context.Background(), c, SettingClientFuncMap{}, defaultApplyFuncs)
				return c
			},
		},
		{
			name: "load failure to apply Pod",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().RestartScheduler(gomock.Any()).Return(nil)
			},
			applyConfiguration: func() *ResourcesForLoad {
				ns := []v1.NamespaceApplyConfiguration{
					*v1.Namespace(testNamespace1),
				}
				pods := []v1.PodApplyConfiguration{
					*v1.Pod("something wrong", testNamespace1),
				}
				nodes := []v1.NodeApplyConfiguration{
					*v1.Node("Node1"),
				}
				pvs := []v1.PersistentVolumeApplyConfiguration{
					*v1.PersistentVolume("PV1").WithStatus(v1.PersistentVolumeStatus().WithPhase("Pending")).WithUID("test"),
				}
				pvcs := []v1.PersistentVolumeClaimApplyConfiguration{
					*v1.PersistentVolumeClaim("PVC1", testNamespace1),
				}
				scs := []confstoragev1.StorageClassApplyConfiguration{
					*confstoragev1.StorageClass("StorageClass1"),
				}
				pcs := []schedulingcfgv1.PriorityClassApplyConfiguration{
					*schedulingcfgv1.PriorityClass("PriorityClass1"),
				}
				config, _ := schedulerCfg.DefaultSchedulerConfig()
				return &ResourcesForLoad{
					Pods:            pods,
					Nodes:           nodes,
					Pvs:             pvs,
					Pvcs:            pvcs,
					StorageClasses:  scs,
					PriorityClasses: pcs,
					SchedulerConfig: config,
					Namespaces:      ns,
				}
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				invokeResourcesFn(context.Background(), c, SettingClientFuncMap{
					pod: func(_ context.Context, c *fake.Clientset) {
						c.PrependReactor("patch", "pods", func(_ k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
							return true, nil, xerrors.New("failed to apply Pod")
						})
					},
				}, defaultApplyFuncs)
				return c
			},
			wantErr: xerrors.New("failed to apply(): apply resources: apply Pod: failed to apply Pod"),
		},
		{
			name: "load failure to apply Node",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().RestartScheduler(gomock.Any()).Return(nil)
			},
			applyConfiguration: func() *ResourcesForLoad {
				ns := []v1.NamespaceApplyConfiguration{
					*v1.Namespace(testNamespace1),
				}
				pods := []v1.PodApplyConfiguration{
					*v1.Pod("Pod1", testNamespace1),
				}
				nodes := []v1.NodeApplyConfiguration{
					*v1.Node("something wrong"),
				}
				pvs := []v1.PersistentVolumeApplyConfiguration{
					*v1.PersistentVolume("PV1").WithStatus(v1.PersistentVolumeStatus().WithPhase("Pending")).WithUID("test"),
				}
				pvcs := []v1.PersistentVolumeClaimApplyConfiguration{
					*v1.PersistentVolumeClaim("PVC1", testNamespace1),
				}
				scs := []confstoragev1.StorageClassApplyConfiguration{
					*confstoragev1.StorageClass("StorageClass1"),
				}
				pcs := []schedulingcfgv1.PriorityClassApplyConfiguration{
					*schedulingcfgv1.PriorityClass("PriorityClass1"),
				}
				config, _ := schedulerCfg.DefaultSchedulerConfig()
				return &ResourcesForLoad{
					Pods:            pods,
					Nodes:           nodes,
					Pvs:             pvs,
					Pvcs:            pvcs,
					StorageClasses:  scs,
					PriorityClasses: pcs,
					SchedulerConfig: config,
					Namespaces:      ns,
				}
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				invokeResourcesFn(context.Background(), c, SettingClientFuncMap{
					node: func(_ context.Context, c *fake.Clientset) {
						c.PrependReactor("patch", "nodes", func(_ k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
							return true, nil, xerrors.New("failed to apply Node")
						})
					},
				}, defaultApplyFuncs)
				return c
			},
			wantErr: xerrors.New("failed to apply(): apply resources: apply Node: failed to apply Node"),
		},
		{
			name: "load failure to apply PersistentVolume",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().RestartScheduler(gomock.Any()).Return(nil)
			},
			applyConfiguration: func() *ResourcesForLoad {
				ns := []v1.NamespaceApplyConfiguration{
					*v1.Namespace(testNamespace1),
				}
				pods := []v1.PodApplyConfiguration{
					*v1.Pod("Pod1", testNamespace1),
				}
				nodes := []v1.NodeApplyConfiguration{
					*v1.Node("Node1"),
				}
				pvs := []v1.PersistentVolumeApplyConfiguration{
					*v1.PersistentVolume("something wrong").WithStatus(v1.PersistentVolumeStatus().WithPhase("Pending")).WithUID("test"),
				}
				pvcs := []v1.PersistentVolumeClaimApplyConfiguration{
					*v1.PersistentVolumeClaim("PVC1", testNamespace1),
				}
				scs := []confstoragev1.StorageClassApplyConfiguration{
					*confstoragev1.StorageClass("StorageClass1"),
				}
				pcs := []schedulingcfgv1.PriorityClassApplyConfiguration{
					*schedulingcfgv1.PriorityClass("PriorityClass1"),
				}
				config, _ := schedulerCfg.DefaultSchedulerConfig()
				return &ResourcesForLoad{
					Pods:            pods,
					Nodes:           nodes,
					Pvs:             pvs,
					Pvcs:            pvcs,
					StorageClasses:  scs,
					PriorityClasses: pcs,
					SchedulerConfig: config,
					Namespaces:      ns,
				}
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				invokeResourcesFn(context.Background(), c, SettingClientFuncMap{
					pv: func(_ context.Context, c *fake.Clientset) {
						c.PrependReactor("patch", "persistentvolumes", func(_ k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
							return true, nil, xerrors.New("failed to apply PersistentVolume")
						})
					},
				}, defaultApplyFuncs)
				return c
			},
			wantErr: xerrors.New("failed to apply(): apply PVs: apply PersistentVolume: failed to apply PersistentVolume"),
		},
		{
			name: "load failure to apply PersistentVolumeClaim",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().RestartScheduler(gomock.Any()).Return(nil)
			},
			applyConfiguration: func() *ResourcesForLoad {
				ns := []v1.NamespaceApplyConfiguration{
					*v1.Namespace(testNamespace1),
				}
				pods := []v1.PodApplyConfiguration{
					*v1.Pod("Pod1", testNamespace1),
				}
				nodes := []v1.NodeApplyConfiguration{
					*v1.Node("Node1"),
				}
				pvs := []v1.PersistentVolumeApplyConfiguration{
					*v1.PersistentVolume("PV1").WithStatus(v1.PersistentVolumeStatus().WithPhase("Pending")).WithUID("test"),
				}
				pvcs := []v1.PersistentVolumeClaimApplyConfiguration{
					*v1.PersistentVolumeClaim("something wrong", testNamespace1),
				}
				scs := []confstoragev1.StorageClassApplyConfiguration{
					*confstoragev1.StorageClass("StorageClass1"),
				}
				pcs := []schedulingcfgv1.PriorityClassApplyConfiguration{
					*schedulingcfgv1.PriorityClass("PriorityClass1"),
				}
				config, _ := schedulerCfg.DefaultSchedulerConfig()
				return &ResourcesForLoad{
					Pods:            pods,
					Nodes:           nodes,
					Pvs:             pvs,
					Pvcs:            pvcs,
					StorageClasses:  scs,
					PriorityClasses: pcs,
					SchedulerConfig: config,
					Namespaces:      ns,
				}
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				invokeResourcesFn(context.Background(), c, SettingClientFuncMap{
					pvc: func(_ context.Context, c *fake.Clientset) {
						c.PrependReactor("patch", "persistentvolumeclaims", func(_ k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
							return true, nil, xerrors.New("failed to apply PersistentVolumeClaim")
						})
					},
				}, defaultApplyFuncs)
				return c
			},
			wantErr: xerrors.New("failed to apply(): apply resources: apply PersistentVolumeClaims: failed to apply PersistentVolumeClaim"),
		},
		{
			name: "load failure to apply StorageClass",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().RestartScheduler(gomock.Any()).Return(nil)
			},
			applyConfiguration: func() *ResourcesForLoad {
				ns := []v1.NamespaceApplyConfiguration{
					*v1.Namespace(testNamespace1),
				}
				pods := []v1.PodApplyConfiguration{
					*v1.Pod("Pod1", testNamespace1),
				}
				nodes := []v1.NodeApplyConfiguration{
					*v1.Node("Node1"),
				}
				pvs := []v1.PersistentVolumeApplyConfiguration{
					*v1.PersistentVolume("PV1").WithStatus(v1.PersistentVolumeStatus().WithPhase("Pending")).WithUID("test"),
				}
				pvcs := []v1.PersistentVolumeClaimApplyConfiguration{
					*v1.PersistentVolumeClaim("PVC1", testNamespace1),
				}
				scs := []confstoragev1.StorageClassApplyConfiguration{
					*confstoragev1.StorageClass("something wrong"),
				}
				pcs := []schedulingcfgv1.PriorityClassApplyConfiguration{
					*schedulingcfgv1.PriorityClass("PriorityClass1"),
				}
				config, _ := schedulerCfg.DefaultSchedulerConfig()
				return &ResourcesForLoad{
					Pods:            pods,
					Nodes:           nodes,
					Pvs:             pvs,
					Pvcs:            pvcs,
					StorageClasses:  scs,
					PriorityClasses: pcs,
					SchedulerConfig: config,
					Namespaces:      ns,
				}
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				invokeResourcesFn(context.Background(), c, SettingClientFuncMap{
					sc: func(_ context.Context, c *fake.Clientset) {
						c.PrependReactor("patch", "storageclasses", func(_ k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
							return true, nil, xerrors.New("failed to apply StorageClass")
						})
					},
				}, defaultApplyFuncs)
				return c
			},
			wantErr: xerrors.New("failed to apply(): apply resources: apply StorageClass: failed to apply StorageClass"),
		},
		{
			name: "load failure to apply PriorityClass",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().RestartScheduler(gomock.Any()).Return(nil)
			},
			applyConfiguration: func() *ResourcesForLoad {
				ns := []v1.NamespaceApplyConfiguration{
					*v1.Namespace(testNamespace1),
				}
				pods := []v1.PodApplyConfiguration{
					*v1.Pod("Pod1", testNamespace1),
				}
				nodes := []v1.NodeApplyConfiguration{
					*v1.Node("Node1"),
				}
				pvs := []v1.PersistentVolumeApplyConfiguration{
					*v1.PersistentVolume("PV1").WithStatus(v1.PersistentVolumeStatus().WithPhase("Pending")).WithUID("test"),
				}
				pvcs := []v1.PersistentVolumeClaimApplyConfiguration{
					*v1.PersistentVolumeClaim("PVC1", testNamespace1),
				}
				scs := []confstoragev1.StorageClassApplyConfiguration{
					*confstoragev1.StorageClass("SC1"),
				}
				pcs := []schedulingcfgv1.PriorityClassApplyConfiguration{
					*schedulingcfgv1.PriorityClass("something wrong"),
				}
				config, _ := schedulerCfg.DefaultSchedulerConfig()
				return &ResourcesForLoad{
					Pods:            pods,
					Nodes:           nodes,
					Pvs:             pvs,
					Pvcs:            pvcs,
					StorageClasses:  scs,
					PriorityClasses: pcs,
					SchedulerConfig: config,
					Namespaces:      ns,
				}
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				invokeResourcesFn(context.Background(), c, SettingClientFuncMap{
					pc: func(_ context.Context, c *fake.Clientset) {
						c.PrependReactor("patch", "priorityclasses", func(_ k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
							return true, nil, xerrors.New("failed to apply PriorityClass")
						})
					},
				}, defaultApplyFuncs)
				return c
			},
			wantErr: xerrors.New("failed to apply(): apply resources: apply PriorityClass: failed to apply PriorityClass"),
		},
		{
			name: "load failure to apply Namespace",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().RestartScheduler(gomock.Any()).Return(nil)
			},
			applyConfiguration: func() *ResourcesForLoad {
				ns := []v1.NamespaceApplyConfiguration{
					*v1.Namespace("something wrong"),
				}
				pods := []v1.PodApplyConfiguration{
					*v1.Pod("Pod1", testNamespace1),
				}
				nodes := []v1.NodeApplyConfiguration{
					*v1.Node("Node1"),
				}
				pvs := []v1.PersistentVolumeApplyConfiguration{
					*v1.PersistentVolume("PV1").WithStatus(v1.PersistentVolumeStatus().WithPhase("Pending")).WithUID("test"),
				}
				pvcs := []v1.PersistentVolumeClaimApplyConfiguration{
					*v1.PersistentVolumeClaim("PVC1", testNamespace1),
				}
				scs := []confstoragev1.StorageClassApplyConfiguration{
					*confstoragev1.StorageClass("SC1"),
				}
				pcs := []schedulingcfgv1.PriorityClassApplyConfiguration{
					*schedulingcfgv1.PriorityClass("PC1"),
				}
				config, _ := schedulerCfg.DefaultSchedulerConfig()
				return &ResourcesForLoad{
					Pods:            pods,
					Nodes:           nodes,
					Pvs:             pvs,
					Pvcs:            pvcs,
					StorageClasses:  scs,
					PriorityClasses: pcs,
					SchedulerConfig: config,
					Namespaces:      ns,
				}
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				invokeResourcesFn(context.Background(), c, SettingClientFuncMap{
					ns: func(_ context.Context, c *fake.Clientset) {
						c.PrependReactor("patch", "namespaces", func(_ k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
							return true, nil, xerrors.New("failed to apply Namespace")
						})
					},
				}, defaultApplyFuncs)
				return c
			},
			wantErr: xerrors.New("failed to apply(): apply resources: apply Namespace: failed to apply Namespace"),
		},
		{
			name: "load success when PersistentVolumeClaim was not found (Get() return err)",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().RestartScheduler(gomock.Any()).Return(nil)
			},
			applyConfiguration: func() *ResourcesForLoad {
				ns := []v1.NamespaceApplyConfiguration{
					*v1.Namespace("something wrong"),
				}
				pods := []v1.PodApplyConfiguration{
					*v1.Pod("Pod1", testNamespace1),
				}
				nodes := []v1.NodeApplyConfiguration{
					*v1.Node("Node1"),
				}
				pvs := []v1.PersistentVolumeApplyConfiguration{
					*v1.PersistentVolume("PV1").WithStatus(v1.PersistentVolumeStatus().WithPhase("Bound")).WithSpec(v1.PersistentVolumeSpec().WithClaimRef(v1.ObjectReference().WithName("PVC1").WithNamespace(testNamespace1))),
				}
				pvcs := []v1.PersistentVolumeClaimApplyConfiguration{
					*v1.PersistentVolumeClaim("PVC1", testNamespace1),
				}
				scs := []confstoragev1.StorageClassApplyConfiguration{
					*confstoragev1.StorageClass("SC1"),
				}
				pcs := []schedulingcfgv1.PriorityClassApplyConfiguration{
					*schedulingcfgv1.PriorityClass("PC1"),
				}
				config, _ := schedulerCfg.DefaultSchedulerConfig()
				return &ResourcesForLoad{
					Pods:            pods,
					Nodes:           nodes,
					Pvs:             pvs,
					Pvcs:            pvcs,
					StorageClasses:  scs,
					PriorityClasses: pcs,
					SchedulerConfig: config,
					Namespaces:      ns,
				}
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				invokeResourcesFn(context.Background(), c, SettingClientFuncMap{
					pvc: func(_ context.Context, c *fake.Clientset) {
						c.PrependReactor("patch", "persistentvolumeclaims", func(_ k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
							return true, &corev1.PersistentVolumeClaim{}, nil
						})
						c.PrependReactor("get", "persistentvolumeclaims", func(action k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
							a, ok := action.(k8stesting.GetAction)
							assert.Equal(t, true, ok)
							assert.Equal(t, "PVC1", a.GetName())
							return true, nil, xerrors.New("pvc not found")
						})
					},
				}, defaultApplyFuncs)
				return c
			},
		},
		{
			name: "load success when PersistentVolumeClaim was found (Get() success)",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().RestartScheduler(gomock.Any()).Return(nil)
			},
			applyConfiguration: func() *ResourcesForLoad {
				ns := []v1.NamespaceApplyConfiguration{
					*v1.Namespace(testNamespace1),
				}
				pods := []v1.PodApplyConfiguration{
					*v1.Pod("Pod1", testNamespace1),
				}
				nodes := []v1.NodeApplyConfiguration{
					*v1.Node("Node1"),
				}
				pvs := []v1.PersistentVolumeApplyConfiguration{
					*v1.PersistentVolume("PV1").WithStatus(v1.PersistentVolumeStatus().WithPhase("Bound")).WithSpec(v1.PersistentVolumeSpec().WithClaimRef(v1.ObjectReference().WithName("PVC1").WithNamespace(testNamespace1))),
				}
				pvcs := []v1.PersistentVolumeClaimApplyConfiguration{
					*v1.PersistentVolumeClaim("PVC1", testNamespace1),
				}
				scs := []confstoragev1.StorageClassApplyConfiguration{
					*confstoragev1.StorageClass("SC1"),
				}
				pcs := []schedulingcfgv1.PriorityClassApplyConfiguration{
					*schedulingcfgv1.PriorityClass("PC1"),
				}
				config, _ := schedulerCfg.DefaultSchedulerConfig()
				return &ResourcesForLoad{
					Pods:            pods,
					Nodes:           nodes,
					Pvs:             pvs,
					Pvcs:            pvcs,
					StorageClasses:  scs,
					PriorityClasses: pcs,
					SchedulerConfig: config,
					Namespaces:      ns,
				}
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				invokeResourcesFn(context.Background(), c, SettingClientFuncMap{}, defaultApplyFuncs)
				c.PrependReactor("get", "persistentvolumeclaims", func(action k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
					a, ok := action.(k8stesting.GetAction)
					assert.Equal(t, true, ok)
					assert.Equal(t, "PVC1", a.GetName())
					o := &corev1.PersistentVolumeClaim{
						ObjectMeta: metav1.ObjectMeta{
							UID:  "testUID",
							Name: "PVC1",
						},
						Spec: corev1.PersistentVolumeClaimSpec{
							VolumeName: "PV1",
						},
					}
					return true, o, nil
				})
				return c
			},
		},
		{
			name: "success load when pv.Status is not exist",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().RestartScheduler(gomock.Any()).Return(nil)
			},
			applyConfiguration: func() *ResourcesForLoad {
				j := `{"pods":[],"nodes":[],`
				// delete status object
				j += `"pvs":[{"metadata":{"name":"pv1","uid":"b0184e68-5ba6-4533-b3fd-bde9416ad03d","resourceVersion":"565","creationTimestamp":"2021-12-28T01:06:35Z","annotations":{"pv.kubernetes.io/bound-by-controller":"yes"},"managedFields":[{"manager":"simulator","operation":"Apply","apiVersion":"v1","time":"2021-12-28T01:06:35Z","fieldsType":"FieldsV1","fieldsV1":{"f:spec":{"f:accessModes":{},"f:capacity":{"f:storage":{}},"f:hostPath":{"f:path":{},"f:type":{}},"f:persistentVolumeReclaimPolicy":{},"f:volumeMode":{}}}},{"manager":"simulator","operation":"Update","apiVersion":"v1","time":"2021-12-28T01:06:35Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:phase":{}}},"subresource":"status"},{"manager":"simulator","operation":"Update","apiVersion":"v1","time":"2021-12-28T01:06:36Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:pv.kubernetes.io/bound-by-controller":{}}},"f:spec":{"f:claimRef":{}}}}]},"spec":{"capacity":{"storage":"1Gi"},"hostPath":{"path":"/tmp/data","type":"DirectoryOrCreate"},"accessModes":["ReadWriteOnce"],"claimRef":{"kind":"PersistentVolumeClaim","namespace":"default","name":"pvc1","uid":"fb6d1964-41e3-4541-a200-4d76f62b2254","apiVersion":"v1","resourceVersion":"557"},"persistentVolumeReclaimPolicy":"Delete","volumeMode":"Filesystem"}}]`
				j += `,"pvcs":[{"metadata":{"name":"pvc1","namespace":"default","uid":"fb6d1964-41e3-4541-a200-4d76f62b2254","resourceVersion":"567","creationTimestamp":"2021-12-28T01:06:32Z","annotations":{"pv.kubernetes.io/bind-completed":"yes","pv.kubernetes.io/bound-by-controller":"yes"},"managedFields":[{"manager":"simulator","operation":"Apply","apiVersion":"v1","time":"2021-12-28T01:06:32Z","fieldsType":"FieldsV1","fieldsV1":{"f:spec":{"f:accessModes":{},"f:resources":{"f:requests":{"f:storage":{}}},"f:volumeMode":{}}}},{"manager":"simulator","operation":"Update","apiVersion":"v1","time":"2021-12-28T01:06:36Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:pv.kubernetes.io/bind-completed":{},"f:pv.kubernetes.io/bound-by-controller":{}}},"f:spec":{"f:volumeName":{}}}},{"manager":"simulator","operation":"Update","apiVersion":"v1","time":"2021-12-28T01:06:36Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:accessModes":{},"f:capacity":{".":{},"f:storage":{}},"f:phase":{}}},"subresource":"status"}]},"spec":{"accessModes":["ReadWriteOnce"],"resources":{"requests":{"storage":"1Gi"}},"volumeName":"pv1","volumeMode":"Filesystem"},"status":{"phase":"Bound","accessModes":["ReadWriteOnce"],"capacity":{"storage":"1Gi"}}}],"storageClasses":[],"priorityClasses":[],"schedulerConfig":{"parallelism":16,"leaderElection":{"leaderElect":true,"leaseDuration":"15s","renewDeadline":"10s","retryPeriod":"2s","resourceLock":"leases","resourceName":"kube-scheduler","resourceNamespace":"kube-system"},"clientConnection":{"kubeconfig":"","acceptContentTypes":"","contentType":"application/vnd.kubernetes.protobuf","qps":50,"burst":100},"healthzBindAddress":"0.0.0.0:10251","metricsBindAddress":"0.0.0.0:10251","enableProfiling":true,"enableContentionProfiling":true,"percentageOfNodesToScore":0,"podInitialBackoffSeconds":1,"podMaxBackoffSeconds":10,"profiles":[{"schedulerName":"default-scheduler","plugins":{"queueSort":{"enabled":[{"name":"PrioritySort"}]},"preFilter":{"enabled":[{"name":"NodeResourcesFit"},{"name":"NodePorts"},{"name":"VolumeRestrictions"},{"name":"PodTopologySpread"},{"name":"InterPodAffinity"},{"name":"VolumeBinding"},{"name":"NodeAffinity"}]},"filter":{"enabled":[{"name":"NodeUnschedulable"},{"name":"NodeName"},{"name":"TaintToleration"},{"name":"NodeAffinity"},{"name":"NodePorts"},{"name":"NodeResourcesFit"},{"name":"VolumeRestrictions"},{"name":"EBSLimits"},{"name":"GCEPDLimits"},{"name":"NodeVolumeLimits"},{"name":"AzureDiskLimits"},{"name":"VolumeBinding"},{"name":"VolumeZone"},{"name":"PodTopologySpread"},{"name":"InterPodAffinity"}]},"postFilter":{"enabled":[{"name":"DefaultPreemption"}]},"preScore":{"enabled":[{"name":"InterPodAffinity"},{"name":"PodTopologySpread"},{"name":"TaintToleration"},{"name":"NodeAffinity"}]},"score":{"enabled":[{"name":"NodeResourcesBalancedAllocation","weight":1},{"name":"ImageLocality","weight":1},{"name":"InterPodAffinity","weight":1},{"name":"NodeResourcesFit","weight":1},{"name":"NodeAffinity","weight":1},{"name":"PodTopologySpread","weight":2},{"name":"TaintToleration","weight":1}]},"reserve":{"enabled":[{"name":"VolumeBinding"}]},"permit":{},"preBind":{"enabled":[{"name":"VolumeBinding"}]},"bind":{"enabled":[{"name":"DefaultBinder"}]},"postBind":{}},"pluginConfig":[{"name":"DefaultPreemption","args":{"kind":"DefaultPreemptionArgs","apiVersion":"kubescheduler.config.k8s.io/v1","minCandidateNodesPercentage":10,"minCandidateNodesAbsolute":100}},{"name":"InterPodAffinity","args":{"kind":"InterPodAffinityArgs","apiVersion":"kubescheduler.config.k8s.io/v1","hardPodAffinityWeight":1}},{"name":"NodeAffinity","args":{"kind":"NodeAffinityArgs","apiVersion":"kubescheduler.config.k8s.io/v1"}},{"name":"NodeResourcesBalancedAllocation","args":{"kind":"NodeResourcesBalancedAllocationArgs","apiVersion":"kubescheduler.config.k8s.io/v1","resources":[{"name":"cpu","weight":1},{"name":"memory","weight":1}]}},{"name":"NodeResourcesFit","args":{"kind":"NodeResourcesFitArgs","apiVersion":"kubescheduler.config.k8s.io/v1","scoringStrategy":{"type":"LeastAllocated","resources":[{"name":"cpu","weight":1},{"name":"memory","weight":1}]}}},{"name":"PodTopologySpread","args":{"kind":"PodTopologySpreadArgs","apiVersion":"kubescheduler.config.k8s.io/v1","defaultingType":"System"}},{"name":"VolumeBinding","args":{"kind":"VolumeBindingArgs","apiVersion":"kubescheduler.config.k8s.io/v1","bindTimeoutSeconds":600}}]}]}}`
				b := []byte(j)
				r := ResourcesForLoad{}
				if err := json.Unmarshal(b, &r); err != nil {
					panic(err)
				}
				return &r
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				invokeResourcesFn(context.Background(), c, SettingClientFuncMap{}, defaultApplyFuncs)
				c.PrependReactor("get", "persistentvolumeclaims", func(_ k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
					t.Fatal("This will not be called.")
					return false, nil, nil
				})
				return c
			},
		},
		{
			name: "success load when pv.Status.Phase is not exist",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().RestartScheduler(gomock.Any()).Return(nil)
			},
			applyConfiguration: func() *ResourcesForLoad {
				j := `{"pods":[],"nodes":[],`
				// delete Phase key&value
				j += `"pvs":[{"metadata":{"name":"pv1","uid":"b0184e68-5ba6-4533-b3fd-bde9416ad03d","resourceVersion":"565","creationTimestamp":"2021-12-28T01:06:35Z","annotations":{"pv.kubernetes.io/bound-by-controller":"yes"},"managedFields":[{"manager":"simulator","operation":"Apply","apiVersion":"v1","time":"2021-12-28T01:06:35Z","fieldsType":"FieldsV1","fieldsV1":{"f:spec":{"f:accessModes":{},"f:capacity":{"f:storage":{}},"f:hostPath":{"f:path":{},"f:type":{}},"f:persistentVolumeReclaimPolicy":{},"f:volumeMode":{}}}},{"manager":"simulator","operation":"Update","apiVersion":"v1","time":"2021-12-28T01:06:35Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:phase":{}}},"subresource":"status"},{"manager":"simulator","operation":"Update","apiVersion":"v1","time":"2021-12-28T01:06:36Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:pv.kubernetes.io/bound-by-controller":{}}},"f:spec":{"f:claimRef":{}}}}]},"spec":{"capacity":{"storage":"1Gi"},"hostPath":{"path":"/tmp/data","type":"DirectoryOrCreate"},"accessModes":["ReadWriteOnce"],"claimRef":{"kind":"PersistentVolumeClaim","namespace":"default","name":"pvc1","uid":"fb6d1964-41e3-4541-a200-4d76f62b2254","apiVersion":"v1","resourceVersion":"557"},"persistentVolumeReclaimPolicy":"Delete","volumeMode":"Filesystem"},"status":{}}]`
				j += `,"pvcs":[{"metadata":{"name":"pvc1","namespace":"default","uid":"fb6d1964-41e3-4541-a200-4d76f62b2254","resourceVersion":"567","creationTimestamp":"2021-12-28T01:06:32Z","annotations":{"pv.kubernetes.io/bind-completed":"yes","pv.kubernetes.io/bound-by-controller":"yes"},"managedFields":[{"manager":"simulator","operation":"Apply","apiVersion":"v1","time":"2021-12-28T01:06:32Z","fieldsType":"FieldsV1","fieldsV1":{"f:spec":{"f:accessModes":{},"f:resources":{"f:requests":{"f:storage":{}}},"f:volumeMode":{}}}},{"manager":"simulator","operation":"Update","apiVersion":"v1","time":"2021-12-28T01:06:36Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:pv.kubernetes.io/bind-completed":{},"f:pv.kubernetes.io/bound-by-controller":{}}},"f:spec":{"f:volumeName":{}}}},{"manager":"simulator","operation":"Update","apiVersion":"v1","time":"2021-12-28T01:06:36Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:accessModes":{},"f:capacity":{".":{},"f:storage":{}},"f:phase":{}}},"subresource":"status"}]},"spec":{"accessModes":["ReadWriteOnce"],"resources":{"requests":{"storage":"1Gi"}},"volumeName":"pv1","volumeMode":"Filesystem"},"status":{"phase":"Bound","accessModes":["ReadWriteOnce"],"capacity":{"storage":"1Gi"}}}],"storageClasses":[],"priorityClasses":[],"schedulerConfig":{"parallelism":16,"leaderElection":{"leaderElect":true,"leaseDuration":"15s","renewDeadline":"10s","retryPeriod":"2s","resourceLock":"leases","resourceName":"kube-scheduler","resourceNamespace":"kube-system"},"clientConnection":{"kubeconfig":"","acceptContentTypes":"","contentType":"application/vnd.kubernetes.protobuf","qps":50,"burst":100},"healthzBindAddress":"0.0.0.0:10251","metricsBindAddress":"0.0.0.0:10251","enableProfiling":true,"enableContentionProfiling":true,"percentageOfNodesToScore":0,"podInitialBackoffSeconds":1,"podMaxBackoffSeconds":10,"profiles":[{"schedulerName":"default-scheduler","plugins":{"queueSort":{"enabled":[{"name":"PrioritySort"}]},"preFilter":{"enabled":[{"name":"NodeResourcesFit"},{"name":"NodePorts"},{"name":"VolumeRestrictions"},{"name":"PodTopologySpread"},{"name":"InterPodAffinity"},{"name":"VolumeBinding"},{"name":"NodeAffinity"}]},"filter":{"enabled":[{"name":"NodeUnschedulable"},{"name":"NodeName"},{"name":"TaintToleration"},{"name":"NodeAffinity"},{"name":"NodePorts"},{"name":"NodeResourcesFit"},{"name":"VolumeRestrictions"},{"name":"EBSLimits"},{"name":"GCEPDLimits"},{"name":"NodeVolumeLimits"},{"name":"AzureDiskLimits"},{"name":"VolumeBinding"},{"name":"VolumeZone"},{"name":"PodTopologySpread"},{"name":"InterPodAffinity"}]},"postFilter":{"enabled":[{"name":"DefaultPreemption"}]},"preScore":{"enabled":[{"name":"InterPodAffinity"},{"name":"PodTopologySpread"},{"name":"TaintToleration"},{"name":"NodeAffinity"}]},"score":{"enabled":[{"name":"NodeResourcesBalancedAllocation","weight":1},{"name":"ImageLocality","weight":1},{"name":"InterPodAffinity","weight":1},{"name":"NodeResourcesFit","weight":1},{"name":"NodeAffinity","weight":1},{"name":"PodTopologySpread","weight":2},{"name":"TaintToleration","weight":1}]},"reserve":{"enabled":[{"name":"VolumeBinding"}]},"permit":{},"preBind":{"enabled":[{"name":"VolumeBinding"}]},"bind":{"enabled":[{"name":"DefaultBinder"}]},"postBind":{}},"pluginConfig":[{"name":"DefaultPreemption","args":{"kind":"DefaultPreemptionArgs","apiVersion":"kubescheduler.config.k8s.io/v1","minCandidateNodesPercentage":10,"minCandidateNodesAbsolute":100}},{"name":"InterPodAffinity","args":{"kind":"InterPodAffinityArgs","apiVersion":"kubescheduler.config.k8s.io/v1","hardPodAffinityWeight":1}},{"name":"NodeAffinity","args":{"kind":"NodeAffinityArgs","apiVersion":"kubescheduler.config.k8s.io/v1"}},{"name":"NodeResourcesBalancedAllocation","args":{"kind":"NodeResourcesBalancedAllocationArgs","apiVersion":"kubescheduler.config.k8s.io/v1","resources":[{"name":"cpu","weight":1},{"name":"memory","weight":1}]}},{"name":"NodeResourcesFit","args":{"kind":"NodeResourcesFitArgs","apiVersion":"kubescheduler.config.k8s.io/v1","scoringStrategy":{"type":"LeastAllocated","resources":[{"name":"cpu","weight":1},{"name":"memory","weight":1}]}}},{"name":"PodTopologySpread","args":{"kind":"PodTopologySpreadArgs","apiVersion":"kubescheduler.config.k8s.io/v1","defaultingType":"System"}},{"name":"VolumeBinding","args":{"kind":"VolumeBindingArgs","apiVersion":"kubescheduler.config.k8s.io/v1","bindTimeoutSeconds":600}}]}]}}`
				b := []byte(j)
				r := ResourcesForLoad{}
				if err := json.Unmarshal(b, &r); err != nil {
					panic(err)
				}
				return &r
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				invokeResourcesFn(context.Background(), c, SettingClientFuncMap{}, defaultApplyFuncs)
				c.PrependReactor("get", "persistentvolumeclaims", func(_ k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
					t.Fatal("This will not be called.")
					return false, nil, nil
				})
				return c
			},
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			ctrl := gomock.NewController(t)

			mockSchedulerSve := mock_snapshot.NewMockSchedulerService(ctrl)
			c := tt.prepareFakeClientSetFn()

			s := NewService(c, mockSchedulerSve)
			tt.prepareEachServiceMockFn(mockSchedulerSve)

			err := s.Load(context.Background(), tt.applyConfiguration())
			if err != nil && tt.wantErr == nil {
				t.Fatalf("Expect no error, but Load() returns error = %+v", err)
			}
			if tt.wantErr != nil {
				assert.EqualError(t, err, tt.wantErr.Error(), "expected: %+v\nactual: %+v", tt.wantErr, err)
			}
		})
	}
}

func TestService_Load_WithIgnoreErrOption(t *testing.T) {
	t.Parallel()
	tests := []struct {
		name                     string
		prepareEachServiceMockFn func(*mock_snapshot.MockSchedulerService)
		prepareFakeClientSetFn   func() *fake.Clientset
		applyConfiguration       func() *ResourcesForLoad
		wantErr                  error
	}{
		{
			name: "load all success",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().RestartScheduler(gomock.Any()).Return(nil)
			},
			applyConfiguration: func() *ResourcesForLoad {
				ns := []v1.NamespaceApplyConfiguration{
					*v1.Namespace(testNamespace1),
				}
				pods := []v1.PodApplyConfiguration{
					*v1.Pod("Pod1", testNamespace1),
				}
				nodes := []v1.NodeApplyConfiguration{
					*v1.Node("Node1"),
				}
				pvs := []v1.PersistentVolumeApplyConfiguration{
					*v1.PersistentVolume("PV1").WithStatus(v1.PersistentVolumeStatus().WithPhase("Pending")).WithUID("test"),
				}
				pvcs := []v1.PersistentVolumeClaimApplyConfiguration{
					*v1.PersistentVolumeClaim("PVC1", testNamespace1),
				}
				scs := []confstoragev1.StorageClassApplyConfiguration{
					*confstoragev1.StorageClass("StorageClass1"),
				}
				pcs := []schedulingcfgv1.PriorityClassApplyConfiguration{
					*schedulingcfgv1.PriorityClass("PriorityClass1"),
				}
				config, _ := schedulerCfg.DefaultSchedulerConfig()
				return &ResourcesForLoad{
					Pods:            pods,
					Nodes:           nodes,
					Pvs:             pvs,
					Pvcs:            pvcs,
					StorageClasses:  scs,
					PriorityClasses: pcs,
					SchedulerConfig: config,
					Namespaces:      ns,
				}
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				invokeResourcesFn(context.Background(), c, SettingClientFuncMap{}, defaultApplyFuncs)
				return c
			},
		},
		{
			name: "no error if failure to apply Pod",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().RestartScheduler(gomock.Any()).Return(nil)
			},
			applyConfiguration: func() *ResourcesForLoad {
				ns := []v1.NamespaceApplyConfiguration{
					*v1.Namespace(testNamespace1),
				}
				pods := []v1.PodApplyConfiguration{
					*v1.Pod("something wrong", testNamespace1),
				}
				nodes := []v1.NodeApplyConfiguration{
					*v1.Node("Node1"),
				}
				pvs := []v1.PersistentVolumeApplyConfiguration{
					*v1.PersistentVolume("PV1").WithStatus(v1.PersistentVolumeStatus().WithPhase("Pending")).WithUID("test"),
				}
				pvcs := []v1.PersistentVolumeClaimApplyConfiguration{
					*v1.PersistentVolumeClaim("PVC1", testNamespace1),
				}
				scs := []confstoragev1.StorageClassApplyConfiguration{
					*confstoragev1.StorageClass("StorageClass1"),
				}
				pcs := []schedulingcfgv1.PriorityClassApplyConfiguration{
					*schedulingcfgv1.PriorityClass("PriorityClass1"),
				}
				config, _ := schedulerCfg.DefaultSchedulerConfig()
				return &ResourcesForLoad{
					Pods:            pods,
					Nodes:           nodes,
					Pvs:             pvs,
					Pvcs:            pvcs,
					StorageClasses:  scs,
					PriorityClasses: pcs,
					SchedulerConfig: config,
					Namespaces:      ns,
				}
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				invokeResourcesFn(context.Background(), c, SettingClientFuncMap{
					pod: func(_ context.Context, c *fake.Clientset) {
						c.PrependReactor("patch", "pods", func(_ k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
							return true, nil, xerrors.New("failed to apply Pod")
						})
					},
				}, defaultApplyFuncs)
				return c
			},
		},
		{
			name: "no error if failure to apply Node",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().RestartScheduler(gomock.Any()).Return(nil)
			},
			applyConfiguration: func() *ResourcesForLoad {
				ns := []v1.NamespaceApplyConfiguration{
					*v1.Namespace(testNamespace1),
				}
				pods := []v1.PodApplyConfiguration{
					*v1.Pod("Pod1", testNamespace1),
				}
				nodes := []v1.NodeApplyConfiguration{
					*v1.Node("something wrong"),
				}
				pvs := []v1.PersistentVolumeApplyConfiguration{
					*v1.PersistentVolume("PV1").WithStatus(v1.PersistentVolumeStatus().WithPhase("Pending")).WithUID("test"),
				}
				pvcs := []v1.PersistentVolumeClaimApplyConfiguration{
					*v1.PersistentVolumeClaim("PVC1", testNamespace1),
				}
				scs := []confstoragev1.StorageClassApplyConfiguration{
					*confstoragev1.StorageClass("StorageClass1"),
				}
				pcs := []schedulingcfgv1.PriorityClassApplyConfiguration{
					*schedulingcfgv1.PriorityClass("PriorityClass1"),
				}
				config, _ := schedulerCfg.DefaultSchedulerConfig()
				return &ResourcesForLoad{
					Pods:            pods,
					Nodes:           nodes,
					Pvs:             pvs,
					Pvcs:            pvcs,
					StorageClasses:  scs,
					PriorityClasses: pcs,
					SchedulerConfig: config,
					Namespaces:      ns,
				}
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				invokeResourcesFn(context.Background(), c, SettingClientFuncMap{
					node: func(_ context.Context, c *fake.Clientset) {
						c.PrependReactor("patch", "nodes", func(_ k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
							return true, nil, xerrors.New("failed to apply Node")
						})
					},
				}, defaultApplyFuncs)
				return c
			},
		},
		{
			name: "no error if failed to apply PersistentVolume",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().RestartScheduler(gomock.Any()).Return(nil)
			},
			applyConfiguration: func() *ResourcesForLoad {
				ns := []v1.NamespaceApplyConfiguration{
					*v1.Namespace(testNamespace1),
				}
				pods := []v1.PodApplyConfiguration{
					*v1.Pod("Pod1", testNamespace1),
				}
				nodes := []v1.NodeApplyConfiguration{
					*v1.Node("Node1"),
				}
				pvs := []v1.PersistentVolumeApplyConfiguration{
					*v1.PersistentVolume("something wrong").WithStatus(v1.PersistentVolumeStatus().WithPhase("Pending")).WithUID("test"),
				}
				pvcs := []v1.PersistentVolumeClaimApplyConfiguration{
					*v1.PersistentVolumeClaim("PVC1", testNamespace1),
				}
				scs := []confstoragev1.StorageClassApplyConfiguration{
					*confstoragev1.StorageClass("StorageClass1"),
				}
				pcs := []schedulingcfgv1.PriorityClassApplyConfiguration{
					*schedulingcfgv1.PriorityClass("PriorityClass1"),
				}
				config, _ := schedulerCfg.DefaultSchedulerConfig()
				return &ResourcesForLoad{
					Pods:            pods,
					Nodes:           nodes,
					Pvs:             pvs,
					Pvcs:            pvcs,
					StorageClasses:  scs,
					PriorityClasses: pcs,
					SchedulerConfig: config,
					Namespaces:      ns,
				}
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				invokeResourcesFn(context.Background(), c, SettingClientFuncMap{
					pv: func(_ context.Context, c *fake.Clientset) {
						c.PrependReactor("patch", "persistentvlumes", func(_ k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
							return true, nil, xerrors.New("failed to apply PersistentVolume")
						})
					},
				}, defaultApplyFuncs)
				return c
			},
		},
		{
			name: "no error if failed to apply PersistentVolumeClaim",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().RestartScheduler(gomock.Any()).Return(nil)
			},
			applyConfiguration: func() *ResourcesForLoad {
				ns := []v1.NamespaceApplyConfiguration{
					*v1.Namespace(testNamespace1),
				}
				pods := []v1.PodApplyConfiguration{
					*v1.Pod("Pod1", testNamespace1),
				}
				nodes := []v1.NodeApplyConfiguration{
					*v1.Node("Node1"),
				}
				pvs := []v1.PersistentVolumeApplyConfiguration{
					*v1.PersistentVolume("PV1").WithStatus(v1.PersistentVolumeStatus().WithPhase("Pending")).WithUID("test"),
				}
				pvcs := []v1.PersistentVolumeClaimApplyConfiguration{
					*v1.PersistentVolumeClaim("something wrong", testNamespace1),
				}
				scs := []confstoragev1.StorageClassApplyConfiguration{
					*confstoragev1.StorageClass("StorageClass1"),
				}
				pcs := []schedulingcfgv1.PriorityClassApplyConfiguration{
					*schedulingcfgv1.PriorityClass("PriorityClass1"),
				}
				config, _ := schedulerCfg.DefaultSchedulerConfig()
				return &ResourcesForLoad{
					Pods:            pods,
					Nodes:           nodes,
					Pvs:             pvs,
					Pvcs:            pvcs,
					StorageClasses:  scs,
					PriorityClasses: pcs,
					SchedulerConfig: config,
					Namespaces:      ns,
				}
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				invokeResourcesFn(context.Background(), c, SettingClientFuncMap{
					pvc: func(_ context.Context, c *fake.Clientset) {
						c.PrependReactor("patch", "persistentvlumeclaims", func(_ k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
							return true, nil, xerrors.New("failed to apply PersistentVolumeClaims")
						})
					},
				}, defaultApplyFuncs)
				return c
			},
		},
		{
			name: "no error if failed to apply StorageClass",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().RestartScheduler(gomock.Any()).Return(nil)
			},
			applyConfiguration: func() *ResourcesForLoad {
				ns := []v1.NamespaceApplyConfiguration{
					*v1.Namespace(testNamespace1),
				}
				pods := []v1.PodApplyConfiguration{
					*v1.Pod("Pod1", testNamespace1),
				}
				nodes := []v1.NodeApplyConfiguration{
					*v1.Node("Node1"),
				}
				pvs := []v1.PersistentVolumeApplyConfiguration{
					*v1.PersistentVolume("PV1").WithStatus(v1.PersistentVolumeStatus().WithPhase("Pending")).WithUID("test"),
				}
				pvcs := []v1.PersistentVolumeClaimApplyConfiguration{
					*v1.PersistentVolumeClaim("PVC1", testNamespace1),
				}
				scs := []confstoragev1.StorageClassApplyConfiguration{
					*confstoragev1.StorageClass("something wrong"),
				}
				pcs := []schedulingcfgv1.PriorityClassApplyConfiguration{
					*schedulingcfgv1.PriorityClass("PriorityClass1"),
				}
				config, _ := schedulerCfg.DefaultSchedulerConfig()
				return &ResourcesForLoad{
					Pods:            pods,
					Nodes:           nodes,
					Pvs:             pvs,
					Pvcs:            pvcs,
					StorageClasses:  scs,
					PriorityClasses: pcs,
					SchedulerConfig: config,
					Namespaces:      ns,
				}
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				invokeResourcesFn(context.Background(), c, SettingClientFuncMap{
					sc: func(_ context.Context, c *fake.Clientset) {
						c.PrependReactor("patch", "storageclasses", func(_ k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
							return true, nil, xerrors.New("failed to apply StorageClass")
						})
					},
				}, defaultApplyFuncs)
				return c
			},
		},
		{
			name: "no error if failed to apply PriorityClass",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().RestartScheduler(gomock.Any()).Return(nil)
			},
			applyConfiguration: func() *ResourcesForLoad {
				ns := []v1.NamespaceApplyConfiguration{
					*v1.Namespace(testNamespace1),
				}
				pods := []v1.PodApplyConfiguration{
					*v1.Pod("Pod1", testNamespace1),
				}
				nodes := []v1.NodeApplyConfiguration{
					*v1.Node("Node1"),
				}
				pvs := []v1.PersistentVolumeApplyConfiguration{
					*v1.PersistentVolume("PV1").WithStatus(v1.PersistentVolumeStatus().WithPhase("Pending")).WithUID("test"),
				}
				pvcs := []v1.PersistentVolumeClaimApplyConfiguration{
					*v1.PersistentVolumeClaim("PVC1", testNamespace1),
				}
				scs := []confstoragev1.StorageClassApplyConfiguration{
					*confstoragev1.StorageClass("something wrong"),
				}
				pcs := []schedulingcfgv1.PriorityClassApplyConfiguration{
					*schedulingcfgv1.PriorityClass("PriorityClass1"),
				}
				config, _ := schedulerCfg.DefaultSchedulerConfig()
				return &ResourcesForLoad{
					Pods:            pods,
					Nodes:           nodes,
					Pvs:             pvs,
					Pvcs:            pvcs,
					StorageClasses:  scs,
					PriorityClasses: pcs,
					SchedulerConfig: config,
					Namespaces:      ns,
				}
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				invokeResourcesFn(context.Background(), c, SettingClientFuncMap{
					pc: func(_ context.Context, c *fake.Clientset) {
						c.PrependReactor("patch", "priorityclasses", func(_ k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
							return true, nil, xerrors.New("failed to apply PriorityClass")
						})
					},
				}, defaultApplyFuncs)
				return c
			},
		},
		{
			name: "no error if failed to apply Namespace",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				ss.EXPECT().RestartScheduler(gomock.Any()).Return(nil)
			},
			applyConfiguration: func() *ResourcesForLoad {
				ns := []v1.NamespaceApplyConfiguration{
					*v1.Namespace("something wrong"),
				}
				pods := []v1.PodApplyConfiguration{
					*v1.Pod("Pod1", testNamespace1),
				}
				nodes := []v1.NodeApplyConfiguration{
					*v1.Node("Node1"),
				}
				pvs := []v1.PersistentVolumeApplyConfiguration{
					*v1.PersistentVolume("PV1").WithStatus(v1.PersistentVolumeStatus().WithPhase("Pending")).WithUID("test"),
				}
				pvcs := []v1.PersistentVolumeClaimApplyConfiguration{
					*v1.PersistentVolumeClaim("PVC1", testNamespace1),
				}
				scs := []confstoragev1.StorageClassApplyConfiguration{
					*confstoragev1.StorageClass("SC1"),
				}
				pcs := []schedulingcfgv1.PriorityClassApplyConfiguration{
					*schedulingcfgv1.PriorityClass("PriorityClass1"),
				}
				config, _ := schedulerCfg.DefaultSchedulerConfig()
				return &ResourcesForLoad{
					Pods:            pods,
					Nodes:           nodes,
					Pvs:             pvs,
					Pvcs:            pvcs,
					StorageClasses:  scs,
					PriorityClasses: pcs,
					SchedulerConfig: config,
					Namespaces:      ns,
				}
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				invokeResourcesFn(context.Background(), c, SettingClientFuncMap{
					ns: func(_ context.Context, c *fake.Clientset) {
						c.PrependReactor("patch", "namespaces", func(_ k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
							return true, nil, xerrors.New("failed to apply Namespace")
						})
					},
				}, defaultApplyFuncs)
				return c
			},
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			ctrl := gomock.NewController(t)

			mockSchedulerSve := mock_snapshot.NewMockSchedulerService(ctrl)
			c := tt.prepareFakeClientSetFn()

			s := NewService(c, mockSchedulerSve)
			tt.prepareEachServiceMockFn(mockSchedulerSve)

			err := s.Load(context.Background(), tt.applyConfiguration(), s.IgnoreErr())
			if err != nil && tt.wantErr == nil {
				t.Fatalf("Expect no error, but Load() returns error = %+v", err)
			}
			if tt.wantErr != nil {
				assert.EqualError(t, err, tt.wantErr.Error(), "expected: %+v\nactual: %+v", tt.wantErr, err)
			}
		})
	}
}

func TestFunction_listPcs(t *testing.T) {
	t.Parallel()
	tests := []struct {
		name                   string
		prepareFakeClientSetFn func() *fake.Clientset
		wantReturn             func() *ResourcesForSnap
		wantErr                bool
	}{
		{
			name: "all pc which have name prefixed with `system-` should filter out",
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				ctx := context.Background()
				// add test data.
				invokeResourcesFn(ctx, c, SettingClientFuncMap{
					pc: func(_ context.Context, c *fake.Clientset) {
						c.PrependReactor("list", "priorityclasses", func(_ k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
							return true, &schedulingv1.PriorityClassList{
								Items: []schedulingv1.PriorityClass{
									{
										ObjectMeta: metav1.ObjectMeta{
											Name: "system-cluster-critical",
										},
									},
									{
										ObjectMeta: metav1.ObjectMeta{
											Name: "priority-class1",
										},
									},
								},
							}, nil
						})
					},
				}, defaultFuncs)
				return c
			},
			wantReturn: func() *ResourcesForSnap {
				_pcs := []schedulingv1.PriorityClass{
					{
						ObjectMeta: metav1.ObjectMeta{
							Name: "priority-class1",
						},
					},
				}
				return &ResourcesForSnap{
					Pods:            []corev1.Pod{},
					Nodes:           []corev1.Node{},
					Pvs:             []corev1.PersistentVolume{},
					Pvcs:            []corev1.PersistentVolumeClaim{},
					StorageClasses:  []storagev1.StorageClass{},
					PriorityClasses: _pcs,
					SchedulerConfig: &configv1.KubeSchedulerConfiguration{},
				}
			},
			wantErr: false,
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			ctrl := gomock.NewController(t)

			mockSchedulerSve := mock_snapshot.NewMockSchedulerService(ctrl)
			c := tt.prepareFakeClientSetFn()

			s := NewService(c, mockSchedulerSve)

			errgrp := util.NewErrGroupWithSemaphore(context.Background())
			resources := &ResourcesForSnap{
				Pods:            []corev1.Pod{},
				Nodes:           []corev1.Node{},
				Pvs:             []corev1.PersistentVolume{},
				Pvcs:            []corev1.PersistentVolumeClaim{},
				StorageClasses:  []storagev1.StorageClass{},
				PriorityClasses: []schedulingv1.PriorityClass{},
				SchedulerConfig: &configv1.KubeSchedulerConfiguration{},
			}

			err := s.listPcs(context.Background(), resources, errgrp, options{})
			if err := errgrp.Wait(); err != nil {
				t.Fatalf("listPcs: %v", err)
			}
			diffResponse := cmp.Diff(resources, tt.wantReturn())
			if diffResponse != "" || (err != nil) != tt.wantErr {
				t.Fatalf("listPcs() %v test, \nerror = %v, wantErr %v\n%s", tt.name, err, tt.wantErr, diffResponse)
			}
		})
	}
}

func TestFunction_applyPcs(t *testing.T) {
	t.Parallel()
	tests := []struct {
		name                   string
		prepareFakeClientSetFn func() *fake.Clientset
		applyConfiguration     func() *ResourcesForLoad
		wantErr                bool
	}{
		{
			name: "all pc which have name prefixed with `system-` should filter out",
			applyConfiguration: func() *ResourcesForLoad {
				pods := []v1.PodApplyConfiguration{}
				nodes := []v1.NodeApplyConfiguration{}
				pvs := []v1.PersistentVolumeApplyConfiguration{}
				pvcs := []v1.PersistentVolumeClaimApplyConfiguration{}
				storageclasses := []confstoragev1.StorageClassApplyConfiguration{}
				pcs := []schedulingcfgv1.PriorityClassApplyConfiguration{}
				pcs = append(pcs, *schedulingcfgv1.PriorityClass("system-PriorityClass1"))
				pcs = append(pcs, *schedulingcfgv1.PriorityClass("PriorityClass1"))
				config, _ := schedulerCfg.DefaultSchedulerConfig()
				return &ResourcesForLoad{
					Pods:            pods,
					Nodes:           nodes,
					Pvs:             pvs,
					Pvcs:            pvcs,
					StorageClasses:  storageclasses,
					PriorityClasses: pcs,
					SchedulerConfig: config,
				}
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				c.PrependReactor("patch", "priorityclasses", func(action k8stesting.Action) (_ bool, _ runtime.Object, _ error) {
					a, ok := action.(k8stesting.PatchAction)
					assert.Equal(t, true, ok)
					// High priority PriorityClass will not come.
					assert.Equal(t, true, !strings.HasPrefix(a.GetName(), "system-"))
					return true, nil, nil
				})
				return c
			},
			wantErr: false,
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			ctx := context.Background()
			ctrl := gomock.NewController(t)

			c := tt.prepareFakeClientSetFn()
			mockSchedulerSve := mock_snapshot.NewMockSchedulerService(ctrl)
			s := NewService(c, mockSchedulerSve)

			errgrp := util.NewErrGroupWithSemaphore(ctx)
			err := s.applyPcs(ctx, tt.applyConfiguration(), errgrp, options{})
			if err := errgrp.Wait(); err != nil {
				t.Fatalf("applyPcs: %v", err)
			}

			if (err != nil) != tt.wantErr {
				t.Fatalf("applyPcs() %v test, \nerror = %v, wantErr %v", tt.name, err, tt.wantErr)
			}
		})
	}
}

func TestService_Load_WithIgnoreSchedulerConfigurationOption(t *testing.T) {
	t.Parallel()
	tests := []struct {
		name                     string
		prepareEachServiceMockFn func(*mock_snapshot.MockSchedulerService)
		prepareFakeClientSetFn   func() *fake.Clientset
		applyConfiguration       func() *ResourcesForLoad
		wantErr                  error
	}{
		{
			name: "Load does not call RestartScheduler",
			prepareEachServiceMockFn: func(ss *mock_snapshot.MockSchedulerService) {
				// If the Load function call this, it will return the error.
				// RestartScheduler must be called zero times.
				ss.EXPECT().RestartScheduler(gomock.Any()).Times(0)
			},
			applyConfiguration: func() *ResourcesForLoad {
				ns := []v1.NamespaceApplyConfiguration{
					*v1.Namespace(testNamespace1),
				}
				pods := []v1.PodApplyConfiguration{
					*v1.Pod("Pod1", testNamespace1),
				}
				nodes := []v1.NodeApplyConfiguration{
					*v1.Node("Node1"),
				}
				pvs := []v1.PersistentVolumeApplyConfiguration{
					*v1.PersistentVolume("PV1").WithStatus(v1.PersistentVolumeStatus().WithPhase("Pending")).WithUID("test"),
				}
				pvcs := []v1.PersistentVolumeClaimApplyConfiguration{
					*v1.PersistentVolumeClaim("PVC1", testNamespace1),
				}
				scs := []confstoragev1.StorageClassApplyConfiguration{
					*confstoragev1.StorageClass("StorageClass1"),
				}
				pcs := []schedulingcfgv1.PriorityClassApplyConfiguration{
					*schedulingcfgv1.PriorityClass("PriorityClass1"),
				}
				config, _ := schedulerCfg.DefaultSchedulerConfig()
				return &ResourcesForLoad{
					Pods:            pods,
					Nodes:           nodes,
					Pvs:             pvs,
					Pvcs:            pvcs,
					StorageClasses:  scs,
					PriorityClasses: pcs,
					SchedulerConfig: config,
					Namespaces:      ns,
				}
			},
			prepareFakeClientSetFn: func() *fake.Clientset {
				c := fake.NewSimpleClientset()
				invokeResourcesFn(context.Background(), c, SettingClientFuncMap{}, defaultApplyFuncs)
				return c
			},
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			ctrl := gomock.NewController(t)

			mockSchedulerSve := mock_snapshot.NewMockSchedulerService(ctrl)
			c := tt.prepareFakeClientSetFn()

			s := NewService(c, mockSchedulerSve)
			tt.prepareEachServiceMockFn(mockSchedulerSve)

			if err := s.Load(context.Background(), tt.applyConfiguration(), s.IgnoreSchedulerConfiguration()); (err != nil) != (tt.wantErr != nil) {
				t.Fatalf("Load() with ignoreSchedulerConfiguration option, %v test, \nerror = %v, wantErr %v", tt.name, err, tt.wantErr)
			}
		})
	}
}
</file>

<file path="kube-scheduler-simulator/simulator/snapshot/snapshot.go">
package snapshot

//go:generate mockgen -destination=./mock_$GOPACKAGE/scheduler.go . SchedulerService

import (
	"context"
	"errors"
	"strings"

	"golang.org/x/xerrors"
	corev1 "k8s.io/api/core/v1"
	schedulingv1 "k8s.io/api/scheduling/v1"
	storagev1 "k8s.io/api/storage/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	v1 "k8s.io/client-go/applyconfigurations/core/v1"
	schedulingcfgv1 "k8s.io/client-go/applyconfigurations/scheduling/v1"
	confstoragev1 "k8s.io/client-go/applyconfigurations/storage/v1"
	clientset "k8s.io/client-go/kubernetes"
	"k8s.io/klog/v2"
	configv1 "k8s.io/kube-scheduler/config/v1"

	"sigs.k8s.io/kube-scheduler-simulator/simulator/scheduler"
	"sigs.k8s.io/kube-scheduler-simulator/simulator/util"
)

type Service struct {
	client           clientset.Interface
	schedulerService SchedulerService
}

// ResourcesForSnap indicates all resources and scheduler configuration to be snapped.
type ResourcesForSnap struct {
	Pods            []corev1.Pod                         `json:"pods"`
	Nodes           []corev1.Node                        `json:"nodes"`
	Pvs             []corev1.PersistentVolume            `json:"pvs"`
	Pvcs            []corev1.PersistentVolumeClaim       `json:"pvcs"`
	StorageClasses  []storagev1.StorageClass             `json:"storageClasses"`
	PriorityClasses []schedulingv1.PriorityClass         `json:"priorityClasses"`
	SchedulerConfig *configv1.KubeSchedulerConfiguration `json:"schedulerConfig"`
	Namespaces      []corev1.Namespace                   `json:"namespaces"`
}

// ResourcesForLoad indicates all resources and scheduler configuration to be loaded.
type ResourcesForLoad struct {
	Pods            []v1.PodApplyConfiguration                        `json:"pods"`
	Nodes           []v1.NodeApplyConfiguration                       `json:"nodes"`
	Pvs             []v1.PersistentVolumeApplyConfiguration           `json:"pvs"`
	Pvcs            []v1.PersistentVolumeClaimApplyConfiguration      `json:"pvcs"`
	StorageClasses  []confstoragev1.StorageClassApplyConfiguration    `json:"storageClasses"`
	PriorityClasses []schedulingcfgv1.PriorityClassApplyConfiguration `json:"priorityClasses"`
	SchedulerConfig *configv1.KubeSchedulerConfiguration              `json:"schedulerConfig"`
	Namespaces      []v1.NamespaceApplyConfiguration                  `json:"namespaces"`
}

type SchedulerService interface {
	GetSchedulerConfig() (*configv1.KubeSchedulerConfiguration, error)
	RestartScheduler(cfg *configv1.KubeSchedulerConfiguration) error
}

func NewService(client clientset.Interface, schedulers SchedulerService) *Service {
	return &Service{
		client:           client,
		schedulerService: schedulers,
	}
}

type options struct {
	ignoreErr                    bool
	ignoreSchedulerConfiguration bool
}

type (
	ignoreErrOption                    bool
	ignoreSchedulerConfigurationOption bool
)

type Option interface {
	apply(*options)
}

func (i ignoreErrOption) apply(opts *options) {
	opts.ignoreErr = bool(i)
}

func (i ignoreSchedulerConfigurationOption) apply(opts *options) {
	opts.ignoreSchedulerConfiguration = bool(i)
}

// IgnoreErr is the option to literally ignore errors.
// If it is enabled, the method won't return any errors, but just log errors as error logs.
func (s *Service) IgnoreErr() Option {
	return ignoreErrOption(true)
}

// IgnoreSchedulerConfiguration is the option to ignore the scheduler configuration in the given ResourcesForLoad.
// Note: this option is only for Load method.
// If it is enabled, the scheduler will not be restarted in load method.
func (s *Service) IgnoreSchedulerConfiguration() Option {
	return ignoreSchedulerConfigurationOption(true)
}

// get gets all resources from each service.
func (s *Service) get(ctx context.Context, opts options) (*ResourcesForSnap, error) {
	errgrp := util.NewErrGroupWithSemaphore(ctx)
	resources := ResourcesForSnap{}

	if err := s.listPods(ctx, &resources, errgrp, opts); err != nil {
		return nil, xerrors.Errorf("call listPods: %w", err)
	}
	if err := s.listNodes(ctx, &resources, errgrp, opts); err != nil {
		return nil, xerrors.Errorf("call listNodes: %w", err)
	}
	if err := s.listPvs(ctx, &resources, errgrp, opts); err != nil {
		return nil, xerrors.Errorf("call listPvs: %w", err)
	}
	if err := s.listPvcs(ctx, &resources, errgrp, opts); err != nil {
		return nil, xerrors.Errorf("call listPvcs: %w", err)
	}
	if err := s.listStorageClasses(ctx, &resources, errgrp, opts); err != nil {
		return nil, xerrors.Errorf("call listStorageClasses: %w", err)
	}
	if err := s.listPcs(ctx, &resources, errgrp, opts); err != nil {
		return nil, xerrors.Errorf("call listPcs: %w", err)
	}
	if err := s.listNamespaces(ctx, &resources, errgrp, opts); err != nil {
		return nil, xerrors.Errorf("call listNamespaces: %w", err)
	}
	if err := s.getSchedulerConfig(&resources, errgrp, opts); err != nil {
		return nil, xerrors.Errorf("call getSchedulerConfig: %w", err)
	}

	if err := errgrp.Wait(); err != nil {
		return nil, xerrors.Errorf("get resources all: %w", err)
	}
	return &resources, nil
}

// Snap exports all resources as one data.
func (s *Service) Snap(ctx context.Context, opts ...Option) (*ResourcesForSnap, error) {
	options := options{}
	for _, o := range opts {
		o.apply(&options)
	}
	resources, err := s.get(ctx, options)
	if err != nil {
		return nil, xerrors.Errorf("failed to get(): %w", err)
	}
	return resources, nil
}

// Apply applies all resources to each service.
//
//nolint:cyclop // For readability.
func (s *Service) apply(ctx context.Context, resources *ResourcesForLoad, opts options) error {
	errgrp := util.NewErrGroupWithSemaphore(ctx)
	// `applyNamespaces` must be called before calling namespaced resources applying.
	if err := s.applyNamespaces(ctx, resources, errgrp, opts); err != nil {
		return xerrors.Errorf("call applyNamespaces: %w", err)
	}
	if err := errgrp.Wait(); err != nil {
		return xerrors.Errorf("apply resources: %w", err)
	}

	if err := s.applyPcs(ctx, resources, errgrp, opts); err != nil {
		return xerrors.Errorf("call applyPcs: %w", err)
	}
	if err := s.applyStorageClasses(ctx, resources, errgrp, opts); err != nil {
		return xerrors.Errorf("call applyStorageClasses: %w", err)
	}
	if err := s.applyPvcs(ctx, resources, errgrp, opts); err != nil {
		return xerrors.Errorf("call applyPvcs: %w", err)
	}
	if err := s.applyNodes(ctx, resources, errgrp, opts); err != nil {
		return xerrors.Errorf("call applyNodes: %w", err)
	}
	if err := s.applyPods(ctx, resources, errgrp, opts); err != nil {
		return xerrors.Errorf("call applyPods: %w", err)
	}
	if err := errgrp.Wait(); err != nil {
		return xerrors.Errorf("apply resources: %w", err)
	}

	// `applyPvs` should be called after `applyPvcs` finished,
	// because `applyPvs` look up PersistentVolumeClaim for `Spec.ClaimRef.UID` field.
	if err := s.applyPvs(ctx, resources, errgrp, opts); err != nil {
		return xerrors.Errorf("call applyPvs: %w", err)
	}
	if err := errgrp.Wait(); err != nil {
		return xerrors.Errorf("apply PVs: %w", err)
	}
	return nil
}

// Load imports all resources from posted data.
// (1) Restart scheduler based on the data.
// (2) Apply each resource.
//   - If UID is not nil, an error will occur. (This is because the api-server will try to find that from current resources by UID)
func (s *Service) Load(ctx context.Context, resources *ResourcesForLoad, opts ...Option) error {
	options := options{}
	for _, o := range opts {
		o.apply(&options)
	}
	if !options.ignoreSchedulerConfiguration {
		if err := s.schedulerService.RestartScheduler(resources.SchedulerConfig); err != nil {
			if !errors.Is(err, scheduler.ErrServiceDisabled) {
				return xerrors.Errorf("restart scheduler with loaded configuration: %w", err)
			}
			klog.Info("The scheduler configuration hasn't been loaded because of an external scheduler is enabled.")
		}
	}
	if err := s.apply(ctx, resources, options); err != nil {
		return xerrors.Errorf("failed to apply(): %w", err)
	}
	return nil
}

func (s *Service) listPods(ctx context.Context, r *ResourcesForSnap, eg *util.SemaphoredErrGroup, opts options) error {
	if err := eg.Go(func() error {
		pods, err := s.client.CoreV1().Pods(metav1.NamespaceAll).List(ctx, metav1.ListOptions{})
		if err != nil {
			if !opts.ignoreErr {
				return xerrors.Errorf("call list Pod: %w", err)
			}
			klog.Errorf("failed to call list Pod: %v", err)
			pods = &corev1.PodList{Items: []corev1.Pod{}}
		}
		r.Pods = pods.Items
		return nil
	}); err != nil {
		return xerrors.Errorf("start error group: %w", err)
	}
	return nil
}

func (s *Service) listNodes(ctx context.Context, r *ResourcesForSnap, eg *util.SemaphoredErrGroup, opts options) error {
	if err := eg.Go(func() error {
		nodes, err := s.client.CoreV1().Nodes().List(ctx, metav1.ListOptions{})
		if err != nil {
			if !opts.ignoreErr {
				return xerrors.Errorf("call list Node: %w", err)
			}
			klog.Errorf("failed to call list Node: %v", err)
			nodes = &corev1.NodeList{Items: []corev1.Node{}}
		}
		r.Nodes = nodes.Items
		return nil
	}); err != nil {
		return xerrors.Errorf("start error group: %w", err)
	}
	return nil
}

func (s *Service) listPvs(ctx context.Context, r *ResourcesForSnap, eg *util.SemaphoredErrGroup, opts options) error {
	if err := eg.Go(func() error {
		pvs, err := s.client.CoreV1().PersistentVolumes().List(ctx, metav1.ListOptions{})
		if err != nil {
			if !opts.ignoreErr {
				return xerrors.Errorf("call list PersistentVolume: %w", err)
			}
			klog.Errorf("failed to call list PersistentVolume: %v", err)
			pvs = &corev1.PersistentVolumeList{Items: []corev1.PersistentVolume{}}
		}
		r.Pvs = pvs.Items
		return nil
	}); err != nil {
		return xerrors.Errorf("start error group: %w", err)
	}
	return nil
}

func (s *Service) listPvcs(ctx context.Context, r *ResourcesForSnap, eg *util.SemaphoredErrGroup, opts options) error {
	if err := eg.Go(func() error {
		pvcs, err := s.client.CoreV1().PersistentVolumeClaims(metav1.NamespaceAll).List(ctx, metav1.ListOptions{})
		if err != nil {
			if !opts.ignoreErr {
				return xerrors.Errorf("call list PersistentVolumeClaim: %w", err)
			}
			klog.Errorf("failed to call list PersistentVolumeClaim: %v", err)
			pvcs = &corev1.PersistentVolumeClaimList{Items: []corev1.PersistentVolumeClaim{}}
		}
		r.Pvcs = pvcs.Items
		return nil
	}); err != nil {
		return xerrors.Errorf("start error group: %w", err)
	}
	return nil
}

func (s *Service) listStorageClasses(ctx context.Context, r *ResourcesForSnap, eg *util.SemaphoredErrGroup, opts options) error {
	if err := eg.Go(func() error {
		scs, err := s.client.StorageV1().StorageClasses().List(ctx, metav1.ListOptions{})
		if err != nil {
			if !opts.ignoreErr {
				return xerrors.Errorf("call list StorageClass: %w", err)
			}
			klog.Errorf("failed to call list StorageClass: %v", err)
			scs = &storagev1.StorageClassList{Items: []storagev1.StorageClass{}}
		}
		r.StorageClasses = scs.Items
		return nil
	}); err != nil {
		return xerrors.Errorf("start error group: %w", err)
	}
	return nil
}

func (s *Service) listPcs(ctx context.Context, r *ResourcesForSnap, eg *util.SemaphoredErrGroup, opts options) error {
	if err := eg.Go(func() error {
		pcs, err := s.client.SchedulingV1().PriorityClasses().List(ctx, metav1.ListOptions{})
		if err != nil {
			if !opts.ignoreErr {
				return xerrors.Errorf("call list PriorityClass: %w", err)
			}
			klog.Errorf("failed to call list PriorityClass: %v", err)
			pcs = &schedulingv1.PriorityClassList{Items: []schedulingv1.PriorityClass{}}
		}
		result := []schedulingv1.PriorityClass{}
		for _, i := range pcs.Items {
			if !isSystemPriorityClass(i.GetObjectMeta().GetName()) {
				result = append(result, i)
			}
		}
		r.PriorityClasses = result
		return nil
	}); err != nil {
		return xerrors.Errorf("start error group: %w", err)
	}
	return nil
}

func (s *Service) listNamespaces(ctx context.Context, r *ResourcesForSnap, eg *util.SemaphoredErrGroup, opts options) error {
	if err := eg.Go(func() error {
		nss, err := s.client.CoreV1().Namespaces().List(ctx, metav1.ListOptions{})
		if err != nil {
			if !opts.ignoreErr {
				return xerrors.Errorf("call list Namespace: %w", err)
			}
			klog.Errorf("failed to call list Namespace: %v", err)
			nss = &corev1.NamespaceList{Items: []corev1.Namespace{}}
		}
		result := []corev1.Namespace{}
		for _, i := range nss.Items {
			if !isIgnoreNamespace(i.GetObjectMeta().GetName()) {
				result = append(result, i)
			}
		}
		r.Namespaces = result
		return nil
	}); err != nil {
		return xerrors.Errorf("start error group: %w", err)
	}
	return nil
}

func (s *Service) getSchedulerConfig(r *ResourcesForSnap, eg *util.SemaphoredErrGroup, opts options) error {
	if err := eg.Go(func() error {
		ss, err := s.schedulerService.GetSchedulerConfig()
		if err != nil && !errors.Is(err, scheduler.ErrServiceDisabled) {
			if !opts.ignoreErr {
				return xerrors.Errorf("get scheduler config: %w", err)
			}
			klog.Errorf("failed to get scheduler config: %v", err)
			return nil
		}
		r.SchedulerConfig = ss
		return nil
	}); err != nil {
		return xerrors.Errorf("start error group: %w", err)
	}
	return nil
}

func (s *Service) applyPcs(ctx context.Context, r *ResourcesForLoad, eg *util.SemaphoredErrGroup, opts options) error {
	for i := range r.PriorityClasses {
		pc := r.PriorityClasses[i]
		if isSystemPriorityClass(*pc.Name) {
			continue
		}
		if err := eg.Go(func() error {
			pc.ObjectMetaApplyConfiguration.UID = nil
			pc.WithAPIVersion("scheduling.k8s.io/v1").WithKind("PriorityClass")
			_, err := s.client.SchedulingV1().PriorityClasses().Apply(ctx, &pc, metav1.ApplyOptions{Force: true, FieldManager: "simulator"})
			if err != nil {
				if !opts.ignoreErr {
					return xerrors.Errorf("apply PriorityClass: %w", err)
				}
				klog.Errorf("failed to apply priorityClasses: %v", err)
			}
			return nil
		}); err != nil {
			return xerrors.Errorf("start error group: %w", err)
		}
	}
	return nil
}

func (s *Service) applyStorageClasses(ctx context.Context, r *ResourcesForLoad, eg *util.SemaphoredErrGroup, opts options) error {
	for i := range r.StorageClasses {
		sc := r.StorageClasses[i]
		if err := eg.Go(func() error {
			sc.ObjectMetaApplyConfiguration.UID = nil
			sc.WithAPIVersion("storage.k8s.io/v1").WithKind("StorageClass")
			_, err := s.client.StorageV1().StorageClasses().Apply(ctx, &sc, metav1.ApplyOptions{Force: true, FieldManager: "simulator"})
			if err != nil {
				if !opts.ignoreErr {
					return xerrors.Errorf("apply StorageClass: %w", err)
				}
				klog.Errorf("failed to apply StorageClass: %v", err)
			}
			return nil
		}); err != nil {
			return xerrors.Errorf("start error group: %w", err)
		}
	}
	return nil
}

func (s *Service) applyPvcs(ctx context.Context, r *ResourcesForLoad, eg *util.SemaphoredErrGroup, opts options) error {
	for i := range r.Pvcs {
		pvc := r.Pvcs[i]
		if err := eg.Go(func() error {
			pvc.ObjectMetaApplyConfiguration.UID = nil
			pvc.WithAPIVersion("v1").WithKind("PersistentVolumeClaim")
			_, err := s.client.CoreV1().PersistentVolumeClaims(*pvc.Namespace).Apply(ctx, &pvc, metav1.ApplyOptions{Force: true, FieldManager: "simulator"})
			if err != nil {
				if !opts.ignoreErr {
					return xerrors.Errorf("apply PersistentVolumeClaims: %w", err)
				}
				klog.Errorf("failed to apply PersistentVolumeClaims: %v", err)
			}
			return nil
		}); err != nil {
			return xerrors.Errorf("start error group: %w", err)
		}
	}
	return nil
}

func (s *Service) applyPvs(ctx context.Context, r *ResourcesForLoad, eg *util.SemaphoredErrGroup, opts options) error {
	for i := range r.Pvs {
		pv := r.Pvs[i]
		if err := eg.Go(func() error {
			pv.ObjectMetaApplyConfiguration.UID = nil
			pv.WithAPIVersion("v1").WithKind("PersistentVolume")
			if pv.Status != nil && pv.Status.Phase != nil {
				if *pv.Status.Phase == "Bound" {
					// PersistentVolumeClaims's UID has been changed to a new value.
					pvc, err := s.client.CoreV1().PersistentVolumeClaims(*pv.Spec.ClaimRef.Namespace).Get(ctx, *pv.Spec.ClaimRef.Name, metav1.GetOptions{})
					if err == nil {
						pv.Spec.ClaimRef.UID = &pvc.UID
					} else {
						klog.Errorf("failed to Get PersistentVolumeClaims from the specified name: %v", err)
						pv.Spec.ClaimRef.UID = nil
					}
				}
			}
			_, err := s.client.CoreV1().PersistentVolumes().Apply(ctx, &pv, metav1.ApplyOptions{Force: true, FieldManager: "simulator"})
			if err != nil {
				if !opts.ignoreErr {
					return xerrors.Errorf("apply PersistentVolume: %w", err)
				}
				klog.Errorf("failed to apply PersistentVolume: %v", err)
			}
			return nil
		}); err != nil {
			return xerrors.Errorf("start error group: %w", err)
		}
	}
	return nil
}

func (s *Service) applyNodes(ctx context.Context, r *ResourcesForLoad, eg *util.SemaphoredErrGroup, opts options) error {
	for i := range r.Nodes {
		node := r.Nodes[i]
		if err := eg.Go(func() error {
			node.ObjectMetaApplyConfiguration.UID = nil
			node.WithAPIVersion("v1").WithKind("Node")
			_, err := s.client.CoreV1().Nodes().Apply(ctx, &node, metav1.ApplyOptions{Force: true, FieldManager: "simulator"})
			if err != nil {
				if !opts.ignoreErr {
					return xerrors.Errorf("apply Node: %w", err)
				}
				klog.Errorf("failed to apply Node: %v", err)
			}
			return nil
		}); err != nil {
			return xerrors.Errorf("start error group: %w", err)
		}
	}
	return nil
}

func (s *Service) applyPods(ctx context.Context, r *ResourcesForLoad, eg *util.SemaphoredErrGroup, opts options) error {
	for i := range r.Pods {
		pod := r.Pods[i]
		if err := eg.Go(func() error {
			pod.ObjectMetaApplyConfiguration.UID = nil
			pod.WithAPIVersion("v1").WithKind("Pod")
			_, err := s.client.CoreV1().Pods(*pod.Namespace).Apply(ctx, &pod, metav1.ApplyOptions{Force: true, FieldManager: "simulator"})
			if err != nil {
				if !opts.ignoreErr {
					return xerrors.Errorf("apply Pod: %w", err)
				}
				klog.Errorf("failed to apply Pod: %v", err)
			}
			return nil
		}); err != nil {
			return xerrors.Errorf("start error group: %w", err)
		}
	}
	return nil
}

func (s *Service) applyNamespaces(ctx context.Context, r *ResourcesForLoad, eg *util.SemaphoredErrGroup, opts options) error {
	for i := range r.Namespaces {
		ns := r.Namespaces[i]
		if isIgnoreNamespace(*ns.Name) {
			continue
		}
		if err := eg.Go(func() error {
			ns.ObjectMetaApplyConfiguration.UID = nil
			ns.WithAPIVersion("v1").WithKind("Namespace")
			_, err := s.client.CoreV1().Namespaces().Apply(ctx, &ns, metav1.ApplyOptions{Force: true, FieldManager: "simulator"})
			if err != nil {
				if !opts.ignoreErr {
					return xerrors.Errorf("apply Namespace: %w", err)
				}
				klog.Errorf("failed to apply Namespace: %v", err)
			}
			return nil
		}); err != nil {
			return xerrors.Errorf("start error group: %w", err)
		}
	}
	return nil
}

// isSystemPriorityClass returns whether the given name of PriorityClass is prefixed with `system-` or not.
// The `system-` prefix is reserved by Kubernetes, and users cannot create a PriorityClass with such a name.
// See: https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass
//
// So, we need to exclude these PriorityClasses when saving/loading any PriorityClasses.
func isSystemPriorityClass(name string) bool {
	return strings.HasPrefix(name, "system-")
}

// isSystemNamespace returns whether the given name of Namespace is prefixed with `kube-` or not.
// The `kube-` prefix is reserved by Kubernetes, and users cannot create a Namespace with such a name.
// See: https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/#working-with-namespaces
//
// So, we need to exclude these Namespaces when saving/loading any Namespaces.
func isSystemNamespace(name string) bool {
	return strings.HasPrefix(name, "kube-")
}

// isIgnoreNamespace returns whether the given name of Namespace is ignored namespace or not.
// It's system reserved one and default namespace.
func isIgnoreNamespace(name string) bool {
	return isSystemNamespace(name) || name == "default"
}
</file>

<file path="kube-scheduler-simulator/simulator/snapshot/utils_test.go">
package snapshot

import (
	"testing"

	"github.com/google/go-cmp/cmp"
	corev1 "k8s.io/api/core/v1"
	schedulingv1 "k8s.io/api/scheduling/v1"
	storagev1 "k8s.io/api/storage/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	v1 "k8s.io/client-go/applyconfigurations/core/v1"
	schedulingcfgv1 "k8s.io/client-go/applyconfigurations/scheduling/v1"
	cfgstoragev1 "k8s.io/client-go/applyconfigurations/storage/v1"
)

func Test_convertPodListToApplyConfigurationList(t *testing.T) {
	t.Parallel()
	const defaultNamespaceName = "default"
	tests := []struct {
		name       string
		input      func() []corev1.Pod
		wantReturn func() []v1.PodApplyConfiguration
		wantErr    bool
	}{
		{
			name: "convert Pod list to PodApplyConfiguration list",
			input: func() []corev1.Pod {
				return []corev1.Pod{
					{
						ObjectMeta: metav1.ObjectMeta{
							Name:      "pod1",
							Namespace: defaultNamespaceName,
						},
					},
					{
						ObjectMeta: metav1.ObjectMeta{
							Name:      "pod2",
							Namespace: defaultNamespaceName,
						},
					},
				}
			},
			wantReturn: func() []v1.PodApplyConfiguration {
				return []v1.PodApplyConfiguration{
					*new(v1.PodApplyConfiguration).WithName("pod1").WithNamespace(defaultNamespaceName),
					*new(v1.PodApplyConfiguration).WithName("pod2").WithNamespace(defaultNamespaceName),
				}
			},
			wantErr: false,
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()

			input := tt.input()
			out, err := convertPodListToApplyConfigurationList(input)
			want := tt.wantReturn()
			for i, o := range out {
				diffResponse := cmp.Diff(o.Name, want[i].Name)
				if diffResponse != "" || (err != nil) != tt.wantErr {
					t.Fatalf("convertToApplyConfiguration() %v test, \nerror = %v,\n%s", tt.name, err, diffResponse)
				}
			}
		})
	}
}

func Test_convertNodeListToApplyConfigurationList(t *testing.T) {
	t.Parallel()
	tests := []struct {
		name       string
		input      func() []corev1.Node
		wantReturn func() []v1.NodeApplyConfiguration
		wantErr    bool
	}{
		{
			name: "convert Node list to NodeApplyConfiguration list",
			input: func() []corev1.Node {
				return []corev1.Node{
					{
						ObjectMeta: metav1.ObjectMeta{
							Name: "node1",
						},
					},
					{
						ObjectMeta: metav1.ObjectMeta{
							Name: "node2",
						},
					},
				}
			},
			wantReturn: func() []v1.NodeApplyConfiguration {
				return []v1.NodeApplyConfiguration{
					*new(v1.NodeApplyConfiguration).WithName("node1"),
					*new(v1.NodeApplyConfiguration).WithName("node2"),
				}
			},
			wantErr: false,
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()

			input := tt.input()
			out, err := convertNodeListToApplyConfigurationList(input)
			want := tt.wantReturn()
			for i, o := range out {
				diffResponse := cmp.Diff(o.Name, want[i].Name)
				if diffResponse != "" || (err != nil) != tt.wantErr {
					t.Fatalf("convertNodeListToApplyConfigurationList() %v test, \nerror = %v,\n%s", tt.name, err, diffResponse)
				}
			}
		})
	}
}

func Test_convertPvListToApplyConfigurationList(t *testing.T) {
	t.Parallel()
	tests := []struct {
		name       string
		input      func() []corev1.PersistentVolume
		wantReturn func() []v1.PersistentVolumeApplyConfiguration
		wantErr    bool
	}{
		{
			name: "convert PersistentVolume list to PersistentVolumeApplyConfiguration list",
			input: func() []corev1.PersistentVolume {
				return []corev1.PersistentVolume{
					{
						ObjectMeta: metav1.ObjectMeta{
							Name: "pv1",
						},
					},
					{
						ObjectMeta: metav1.ObjectMeta{
							Name: "pv2",
						},
					},
				}
			},
			wantReturn: func() []v1.PersistentVolumeApplyConfiguration {
				return []v1.PersistentVolumeApplyConfiguration{
					*new(v1.PersistentVolumeApplyConfiguration).WithName("pv1"),
					*new(v1.PersistentVolumeApplyConfiguration).WithName("pv2"),
				}
			},
			wantErr: false,
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()

			input := tt.input()
			out, err := convertPvListToApplyConfigurationList(input)
			want := tt.wantReturn()
			for i, o := range out {
				diffResponse := cmp.Diff(o.Name, want[i].Name)
				if diffResponse != "" || (err != nil) != tt.wantErr {
					t.Fatalf("convertPvListToApplyConfigurationList() %v test, \nerror = %v,\n%s", tt.name, err, diffResponse)
				}
			}
		})
	}
}

func Test_convertPvcListToApplyConfigurationList(t *testing.T) {
	t.Parallel()
	const defaultNamespaceName = "default"
	tests := []struct {
		name       string
		input      func() []corev1.PersistentVolumeClaim
		wantReturn func() []v1.PersistentVolumeClaimApplyConfiguration
		wantErr    bool
	}{
		{
			name: "convert PersistentVolumeClaim list to PersistentVolumeClaimApplyConfiguration list",
			input: func() []corev1.PersistentVolumeClaim {
				return []corev1.PersistentVolumeClaim{
					{
						ObjectMeta: metav1.ObjectMeta{
							Name:      "pvc1",
							Namespace: defaultNamespaceName,
						},
					},
					{
						ObjectMeta: metav1.ObjectMeta{
							Name:      "pvc2",
							Namespace: defaultNamespaceName,
						},
					},
				}
			},
			wantReturn: func() []v1.PersistentVolumeClaimApplyConfiguration {
				return []v1.PersistentVolumeClaimApplyConfiguration{
					*new(v1.PersistentVolumeClaimApplyConfiguration).WithName("pvc1").WithNamespace(defaultNamespaceName),
					*new(v1.PersistentVolumeClaimApplyConfiguration).WithName("pvc2").WithNamespace(defaultNamespaceName),
				}
			},
			wantErr: false,
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()

			input := tt.input()
			out, err := convertPvcListToApplyConfigurationList(input)
			want := tt.wantReturn()
			for i, o := range out {
				diffResponse := cmp.Diff(o.Name, want[i].Name)
				if diffResponse != "" || (err != nil) != tt.wantErr {
					t.Fatalf("convertPvcListToApplyConfigurationList() %v test, \nerror = %v,\n%s", tt.name, err, diffResponse)
				}
			}
		})
	}
}

func Test_convertStorageClassesListToApplyConfigurationList(t *testing.T) {
	t.Parallel()
	tests := []struct {
		name       string
		input      func() []storagev1.StorageClass
		wantReturn func() []cfgstoragev1.StorageClassApplyConfiguration
		wantErr    bool
	}{
		{
			name: "convert StorageClass list to StorageClassApplyConfiguration list",
			input: func() []storagev1.StorageClass {
				return []storagev1.StorageClass{
					{
						ObjectMeta: metav1.ObjectMeta{
							Name: "sc1",
						},
					},
					{
						ObjectMeta: metav1.ObjectMeta{
							Name: "sc2",
						},
					},
				}
			},
			wantReturn: func() []cfgstoragev1.StorageClassApplyConfiguration {
				return []cfgstoragev1.StorageClassApplyConfiguration{
					*new(cfgstoragev1.StorageClassApplyConfiguration).WithName("sc1"),
					*new(cfgstoragev1.StorageClassApplyConfiguration).WithName("sc2"),
				}
			},
			wantErr: false,
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()

			input := tt.input()
			out, err := convertStorageClassesListToApplyConfigurationList(input)
			want := tt.wantReturn()
			for i, o := range out {
				diffResponse := cmp.Diff(o.Name, want[i].Name)
				if diffResponse != "" || (err != nil) != tt.wantErr {
					t.Fatalf("convertStorageClassesListToApplyConfigurationList() %v test, \nerror = %v,\n%s", tt.name, err, diffResponse)
				}
			}
		})
	}
}

func Test_convertPriorityClassesListToApplyConfigurationList(t *testing.T) {
	t.Parallel()
	tests := []struct {
		name       string
		input      func() []schedulingv1.PriorityClass
		wantReturn func() []schedulingcfgv1.PriorityClassApplyConfiguration
		wantErr    bool
	}{
		{
			name: "convert PriorityClass list to PriorityClassApplyConfiguration list",
			input: func() []schedulingv1.PriorityClass {
				return []schedulingv1.PriorityClass{
					{
						ObjectMeta: metav1.ObjectMeta{
							Name: "pc1",
						},
					},
					{
						ObjectMeta: metav1.ObjectMeta{
							Name: "pc2",
						},
					},
				}
			},
			wantReturn: func() []schedulingcfgv1.PriorityClassApplyConfiguration {
				return []schedulingcfgv1.PriorityClassApplyConfiguration{
					*new(schedulingcfgv1.PriorityClassApplyConfiguration).WithName("pc1"),
					*new(schedulingcfgv1.PriorityClassApplyConfiguration).WithName("pc2"),
				}
			},
			wantErr: false,
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()

			input := tt.input()
			out, err := convertPriorityClassesListToApplyConfigurationList(input)
			want := tt.wantReturn()
			for i, o := range out {
				diffResponse := cmp.Diff(o.Name, want[i].Name)
				if diffResponse != "" || (err != nil) != tt.wantErr {
					t.Fatalf("convertPriorityClassesListToApplyConfigurationList() %v test, \nerror = %v,\n%s", tt.name, err, diffResponse)
				}
			}
		})
	}
}

func Test_convertNamespaceListToApplyConfigurationList(t *testing.T) {
	t.Parallel()
	tests := []struct {
		name       string
		input      func() []corev1.Namespace
		wantReturn func() []v1.NamespaceApplyConfiguration
		wantErr    bool
	}{
		{
			name: "convert Namespace list to NamespaceApplyConfiguration list",
			input: func() []corev1.Namespace {
				return []corev1.Namespace{
					{
						ObjectMeta: metav1.ObjectMeta{
							Name: "ns1",
						},
					},
					{
						ObjectMeta: metav1.ObjectMeta{
							Name: "ns2",
						},
					},
				}
			},
			wantReturn: func() []v1.NamespaceApplyConfiguration {
				return []v1.NamespaceApplyConfiguration{
					*new(v1.NamespaceApplyConfiguration).WithName("ns1"),
					*new(v1.NamespaceApplyConfiguration).WithName("ns2"),
				}
			},
			wantErr: false,
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()

			input := tt.input()
			out, err := convertNamespaceListToApplyConfigurationList(input)
			want := tt.wantReturn()
			for i, o := range out {
				diffResponse := cmp.Diff(o.Name, want[i].Name)
				if diffResponse != "" || (err != nil) != tt.wantErr {
					t.Fatalf("convertNamespaceListToApplyConfigurationList() %v test, \nerror = %v,\n%s", tt.name, err, diffResponse)
				}
			}
		})
	}
}
</file>

<file path="kube-scheduler-simulator/simulator/snapshot/utils.go">
package snapshot

import (
	"encoding/json"

	"golang.org/x/xerrors"
	corev1 "k8s.io/api/core/v1"
	schedulingv1 "k8s.io/api/scheduling/v1"
	storagev1 "k8s.io/api/storage/v1"
	v1 "k8s.io/client-go/applyconfigurations/core/v1"
	schedulingcfgv1 "k8s.io/client-go/applyconfigurations/scheduling/v1"
	cfgstoragev1 "k8s.io/client-go/applyconfigurations/storage/v1"
)

// ConvertResourcesForSnapToResourcesForLoad returns ResourcesForLoad based on ResourcesForSnap.
func ConvertResourcesForSnapToResourcesForLoad(expRes *ResourcesForSnap) (*ResourcesForLoad, error) {
	pods, err := convertPodListToApplyConfigurationList(expRes.Pods)
	if err != nil {
		return nil, xerrors.Errorf("call convertPodListToApplyConfigurationList: %w", err)
	}
	nodes, err := convertNodeListToApplyConfigurationList(expRes.Nodes)
	if err != nil {
		return nil, xerrors.Errorf("call convertNodeListToApplyConfigurationList: %w", err)
	}
	pvs, err := convertPvListToApplyConfigurationList(expRes.Pvs)
	if err != nil {
		return nil, xerrors.Errorf("call convertPvListToApplyConfigurationList: %w", err)
	}
	pvcs, err := convertPvcListToApplyConfigurationList(expRes.Pvcs)
	if err != nil {
		return nil, xerrors.Errorf("call convertPvcListToApplyConfigurationList: %w", err)
	}
	scs, err := convertStorageClassesListToApplyConfigurationList(expRes.StorageClasses)
	if err != nil {
		return nil, xerrors.Errorf("call convertStorageClassesListToApplyConfigurationList: %w", err)
	}
	pcs, err := convertPriorityClassesListToApplyConfigurationList(expRes.PriorityClasses)
	if err != nil {
		return nil, xerrors.Errorf("call convertPriorityClassesListToApplyConfigurationList: %w", err)
	}
	nss, err := convertNamespaceListToApplyConfigurationList(expRes.Namespaces)
	if err != nil {
		return nil, xerrors.Errorf("call convertNamespaceListToApplyConfigurationList: %w", err)
	}
	return &ResourcesForLoad{
		Pods:            pods,
		Nodes:           nodes,
		Pvs:             pvs,
		Pvcs:            pvcs,
		StorageClasses:  scs,
		PriorityClasses: pcs,
		SchedulerConfig: expRes.SchedulerConfig,
		Namespaces:      nss,
	}, nil
}

func convertPodListToApplyConfigurationList(pods []corev1.Pod) ([]v1.PodApplyConfiguration, error) {
	rto := make([]v1.PodApplyConfiguration, len(pods))
	for i, p := range pods {
		if err := convertToApplyConfiguration(p, &rto[i]); err != nil {
			return nil, xerrors.Errorf("convert Pod to apply configuration: %w", err)
		}
	}
	return rto, nil
}

func convertNodeListToApplyConfigurationList(nodes []corev1.Node) ([]v1.NodeApplyConfiguration, error) {
	rto := make([]v1.NodeApplyConfiguration, len(nodes))
	for i, n := range nodes {
		if err := convertToApplyConfiguration(n, &rto[i]); err != nil {
			return nil, xerrors.Errorf("convert Node to apply configuration: %w", err)
		}
	}
	return rto, nil
}

func convertPvListToApplyConfigurationList(pvs []corev1.PersistentVolume) ([]v1.PersistentVolumeApplyConfiguration, error) {
	rto := make([]v1.PersistentVolumeApplyConfiguration, len(pvs))
	for i, p := range pvs {
		if err := convertToApplyConfiguration(p, &rto[i]); err != nil {
			return nil, xerrors.Errorf("convert PersistentVolume to apply configuration: %w", err)
		}
	}
	return rto, nil
}

func convertPvcListToApplyConfigurationList(pvcs []corev1.PersistentVolumeClaim) ([]v1.PersistentVolumeClaimApplyConfiguration, error) {
	rto := make([]v1.PersistentVolumeClaimApplyConfiguration, len(pvcs))
	for i, p := range pvcs {
		if err := convertToApplyConfiguration(p, &rto[i]); err != nil {
			return nil, xerrors.Errorf("convert PersistentVolumeClaim to apply configuration: %w", err)
		}
	}
	return rto, nil
}

func convertStorageClassesListToApplyConfigurationList(scs []storagev1.StorageClass) ([]cfgstoragev1.StorageClassApplyConfiguration, error) {
	rto := make([]cfgstoragev1.StorageClassApplyConfiguration, len(scs))
	for i, s := range scs {
		if err := convertToApplyConfiguration(s, &rto[i]); err != nil {
			return nil, xerrors.Errorf("convert StorageClass to apply configuration: %w", err)
		}
	}
	return rto, nil
}

func convertPriorityClassesListToApplyConfigurationList(pcs []schedulingv1.PriorityClass) ([]schedulingcfgv1.PriorityClassApplyConfiguration, error) {
	rto := make([]schedulingcfgv1.PriorityClassApplyConfiguration, len(pcs))
	for i, p := range pcs {
		if err := convertToApplyConfiguration(p, &rto[i]); err != nil {
			return nil, xerrors.Errorf("convert PriorityClasses to apply configuration: %w", err)
		}
	}
	return rto, nil
}

func convertNamespaceListToApplyConfigurationList(nss []corev1.Namespace) ([]v1.NamespaceApplyConfiguration, error) {
	rto := make([]v1.NamespaceApplyConfiguration, len(nss))
	for i, ns := range nss {
		if err := convertToApplyConfiguration(ns, &rto[i]); err != nil {
			return nil, xerrors.Errorf("convert Namespace to apply configuration: %w", err)
		}
	}
	return rto, nil
}

// convertToApplyConfiguration is convert some object to XXXXApplyConfiguration.
// out should be the pointer of XXXXApplyConfiguration, otherwise, you can not get the result of conversion.
//
//nolint:funlen,cyclop // For readability.
func convertToApplyConfiguration(in interface{}, out interface{}) error {
	_in, err := json.Marshal(in)
	if err != nil {
		return xerrors.Errorf("call Marshal to convert object: %w", err)
	}
	switch in.(type) {
	case corev1.Pod:
		typedout, ok := out.(*v1.PodApplyConfiguration)
		if !ok {
			return xerrors.New("unexpected type was given as out")
		}
		if err := json.Unmarshal(_in, &typedout); err != nil {
			return xerrors.Errorf("call Unmarshal to convert Pod: %w", err)
		}
		return nil
	case corev1.Node:
		typedout, ok := out.(*v1.NodeApplyConfiguration)
		if !ok {
			return xerrors.New("unexpected type was given as out")
		}
		if err := json.Unmarshal(_in, &typedout); err != nil {
			return xerrors.Errorf("call Unmarshal to convert Node: %w", err)
		}
		return nil
	case corev1.PersistentVolume:
		typedout, ok := out.(*v1.PersistentVolumeApplyConfiguration)
		if !ok {
			return xerrors.New("unexpected type was given as out")
		}
		if err := json.Unmarshal(_in, &typedout); err != nil {
			return xerrors.Errorf("call Unmarshal to convert PersistentVolume: %w", err)
		}
		return nil
	case corev1.PersistentVolumeClaim:
		typedout, ok := out.(*v1.PersistentVolumeClaimApplyConfiguration)
		if !ok {
			return xerrors.New("unexpected type was given as out")
		}
		if err := json.Unmarshal(_in, &typedout); err != nil {
			return xerrors.Errorf("call Unmarshal to convert PersistentVolumeClaim: %w", err)
		}
		return nil
	case storagev1.StorageClass:
		typedout, ok := out.(*cfgstoragev1.StorageClassApplyConfiguration)
		if !ok {
			return xerrors.New("unexpected type was given as out")
		}
		if err := json.Unmarshal(_in, &typedout); err != nil {
			return xerrors.Errorf("call Unmarshal to convert StorageClass: %w", err)
		}
		return nil
	case schedulingv1.PriorityClass:
		typedout, ok := out.(*schedulingcfgv1.PriorityClassApplyConfiguration)
		if !ok {
			return xerrors.New("unexpected type was given as out")
		}
		if err := json.Unmarshal(_in, &typedout); err != nil {
			return xerrors.Errorf("call Unmarshal to convert PriorityClass: %w", err)
		}
		return nil
	case corev1.Namespace:
		typedout, ok := out.(*v1.NamespaceApplyConfiguration)
		if !ok {
			return xerrors.New("Uhexpected type was given as out")
		}
		if err := json.Unmarshal(_in, &typedout); err != nil {
			return xerrors.Errorf("call Unmarshal to convert Namespace: %w", err)
		}
		return nil
	default:
		return xerrors.Errorf("unknown type")
	}
}
</file>

<file path="kube-scheduler-simulator/simulator/syncer/syncer_test.go">
package syncer

import (
	"context"
	"fmt"
	"testing"
	"time"

	"github.com/google/go-cmp/cmp"
	"github.com/google/go-cmp/cmp/cmpopts"
	v1 "k8s.io/api/core/v1"
	apierrors "k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/util/sets"
	"k8s.io/apimachinery/pkg/util/wait"
	dynamicFake "k8s.io/client-go/dynamic/fake"
	"k8s.io/client-go/restmapper"
	scheduling "k8s.io/kubernetes/pkg/apis/scheduling/v1"
	storage "k8s.io/kubernetes/pkg/apis/storage/v1"

	"sigs.k8s.io/kube-scheduler-simulator/simulator/resourceapplier"
)

//nolint:gocognit // it is because of huge test cases.
func TestSyncerWithPod(t *testing.T) {
	t.Parallel()

	tests := []struct {
		name                    string
		initialPodsInSrcCluster []*v1.Pod
		podsCreatedInSrcCluster []*v1.Pod
		podsUpdatedInSrcCluster []*v1.Pod
		podsDeletedInSrcCluster []*v1.Pod
		afterPodsInDestCluster  []*v1.Pod
	}{
		{
			name: "unscheduled pod is created in src cluster",
			initialPodsInSrcCluster: []*v1.Pod{
				{
					TypeMeta: metav1.TypeMeta{
						Kind:       "Pod",
						APIVersion: "v1",
					},
					ObjectMeta: metav1.ObjectMeta{
						Name:      "pod-1",
						Namespace: "default",
					},
					Spec: v1.PodSpec{
						Containers: []v1.Container{
							{
								Name: "container-1",
							},
						},
					},
				},
			},
			podsCreatedInSrcCluster: []*v1.Pod{
				{
					TypeMeta: metav1.TypeMeta{
						Kind:       "Pod",
						APIVersion: "v1",
					},
					ObjectMeta: metav1.ObjectMeta{
						Name:      "pod-2",
						Namespace: "default",
					},
					Spec: v1.PodSpec{
						Containers: []v1.Container{
							{
								Name: "container-2",
							},
						},
					},
				},
				{
					TypeMeta: metav1.TypeMeta{
						Kind:       "Pod",
						APIVersion: "v1",
					},
					ObjectMeta: metav1.ObjectMeta{
						Name:      "pod-3",
						Namespace: "default-3",
					},
					Spec: v1.PodSpec{
						Containers: []v1.Container{
							{
								Name: "container-3",
							},
						},
					},
				},
			},
			afterPodsInDestCluster: []*v1.Pod{
				{
					TypeMeta: metav1.TypeMeta{
						Kind:       "Pod",
						APIVersion: "v1",
					},
					ObjectMeta: metav1.ObjectMeta{
						Name:      "pod-1",
						Namespace: "default",
					},
					Spec: v1.PodSpec{
						Containers: []v1.Container{
							{
								Name: "container-1",
							},
						},
					},
				},
				{
					TypeMeta: metav1.TypeMeta{
						Kind:       "Pod",
						APIVersion: "v1",
					},
					ObjectMeta: metav1.ObjectMeta{
						Name:      "pod-2",
						Namespace: "default",
					},
					Spec: v1.PodSpec{
						Containers: []v1.Container{
							{
								Name: "container-2",
							},
						},
					},
				},
				{
					TypeMeta: metav1.TypeMeta{
						Kind:       "Pod",
						APIVersion: "v1",
					},
					ObjectMeta: metav1.ObjectMeta{
						Name:      "pod-3",
						Namespace: "default-3",
					},
					Spec: v1.PodSpec{
						Containers: []v1.Container{
							{
								Name: "container-3",
							},
						},
					},
				},
			},
		},
		{
			name: "pod is created and deleted in src cluster",
			podsCreatedInSrcCluster: []*v1.Pod{
				{
					TypeMeta: metav1.TypeMeta{
						Kind:       "Pod",
						APIVersion: "v1",
					},
					ObjectMeta: metav1.ObjectMeta{
						Name:      "pod-1",
						Namespace: "default",
					},
					Spec: v1.PodSpec{
						Containers: []v1.Container{
							{
								Name: "container-1",
							},
						},
					},
				},
				{
					TypeMeta: metav1.TypeMeta{
						Kind:       "Pod",
						APIVersion: "v1",
					},
					ObjectMeta: metav1.ObjectMeta{
						Name:      "pod-2",
						Namespace: "default-2",
					},
					Spec: v1.PodSpec{
						Containers: []v1.Container{
							{
								Name: "container-2",
							},
						},
					},
				},
			},
			podsDeletedInSrcCluster: []*v1.Pod{
				{
					TypeMeta: metav1.TypeMeta{
						Kind:       "Pod",
						APIVersion: "v1",
					},
					ObjectMeta: metav1.ObjectMeta{
						Name:      "pod-1",
						Namespace: "default",
					},
					Spec: v1.PodSpec{
						Containers: []v1.Container{
							{
								Name: "container-1",
							},
						},
					},
				},
			},
			afterPodsInDestCluster: []*v1.Pod{
				{
					TypeMeta: metav1.TypeMeta{
						Kind:       "Pod",
						APIVersion: "v1",
					},
					ObjectMeta: metav1.ObjectMeta{
						Name:      "pod-2",
						Namespace: "default-2",
					},
					Spec: v1.PodSpec{
						Containers: []v1.Container{
							{
								Name: "container-2",
							},
						},
					},
				},
			},
		},
		{
			name: "unscheduled pod is updated in src cluster",
			podsCreatedInSrcCluster: []*v1.Pod{
				{
					TypeMeta: metav1.TypeMeta{
						Kind:       "Pod",
						APIVersion: "v1",
					},
					ObjectMeta: metav1.ObjectMeta{
						Name:      "pod-1",
						Namespace: "default",
					},
					Spec: v1.PodSpec{
						Containers: []v1.Container{
							{
								Name: "container-1",
							},
						},
					},
				},
			},
			podsUpdatedInSrcCluster: []*v1.Pod{
				{
					TypeMeta: metav1.TypeMeta{
						Kind:       "Pod",
						APIVersion: "v1",
					},
					ObjectMeta: metav1.ObjectMeta{
						Name:      "pod-1",
						Namespace: "default",
						Labels: map[string]string{
							"foo": "bar",
						},
					},
					Spec: v1.PodSpec{
						Containers: []v1.Container{
							{
								Name: "container-1",
							},
						},
					},
				},
			},
			afterPodsInDestCluster: []*v1.Pod{
				{
					TypeMeta: metav1.TypeMeta{
						Kind:       "Pod",
						APIVersion: "v1",
					},
					ObjectMeta: metav1.ObjectMeta{
						Name:      "pod-1",
						Namespace: "default",
						Labels: map[string]string{
							"foo": "bar",
						},
					},
					Spec: v1.PodSpec{
						Containers: []v1.Container{
							{
								Name: "container-1",
							},
						},
					},
				},
			},
		},
		{
			name: "scheduled pod is NOT updated in src cluster",
			podsCreatedInSrcCluster: []*v1.Pod{
				{
					TypeMeta: metav1.TypeMeta{
						Kind:       "Pod",
						APIVersion: "v1",
					},
					ObjectMeta: metav1.ObjectMeta{
						Name:      "pod-1",
						Namespace: "default",
					},
					Spec: v1.PodSpec{
						Containers: []v1.Container{
							{
								Name: "container-1",
							},
						},
					},
				},
			},
			podsUpdatedInSrcCluster: []*v1.Pod{
				{
					TypeMeta: metav1.TypeMeta{
						Kind:       "Pod",
						APIVersion: "v1",
					},
					ObjectMeta: metav1.ObjectMeta{
						Name:      "pod-1",
						Namespace: "default",
					},
					Spec: v1.PodSpec{
						Containers: []v1.Container{
							{
								Name: "container-1",
							},
						},

						NodeName: "node-1", // Got NodeName, so this Pod is scheduled.
					},
				},
			},
			afterPodsInDestCluster: []*v1.Pod{
				{
					TypeMeta: metav1.TypeMeta{
						Kind:       "Pod",
						APIVersion: "v1",
					},
					ObjectMeta: metav1.ObjectMeta{
						Name:      "pod-1",
						Namespace: "default",
					},
					Spec: v1.PodSpec{
						Containers: []v1.Container{
							{
								Name: "container-1",
							},
						},
					},
				},
			},
		},
	}

	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()

			s := runtime.NewScheme()
			v1.AddToScheme(s)
			scheduling.AddToScheme(s)
			storage.AddToScheme(s)
			src := dynamicFake.NewSimpleDynamicClient(s)
			dest := dynamicFake.NewSimpleDynamicClient(s)
			resources := []*restmapper.APIGroupResources{
				{
					Group: metav1.APIGroup{
						Versions: []metav1.GroupVersionForDiscovery{
							{Version: "v1"},
						},
					},
					VersionedResources: map[string][]metav1.APIResource{
						"v1": {
							{Name: "pods", Namespaced: true, Kind: "Pod"},
						},
					},
				},
				{
					Group: metav1.APIGroup{
						Versions: []metav1.GroupVersionForDiscovery{
							{Version: "v1"},
						},
					},
					VersionedResources: map[string][]metav1.APIResource{
						"v1": {
							{Name: "nodes", Namespaced: true, Kind: "Node"},
						},
					},
				},
			}
			mapper := restmapper.NewDiscoveryRESTMapper(resources)
			resourceApplier := resourceapplier.New(dest, mapper, resourceapplier.Options{})
			service := New(src, resourceApplier)

			ctx, cancel := context.WithCancel(context.Background())

			createdPods := sets.New[podKey]()
			for _, pod := range tt.initialPodsInSrcCluster {
				p, err := runtime.DefaultUnstructuredConverter.ToUnstructured(pod)
				if err != nil {
					t.Fatalf("failed to convert pod to unstructured: %v", err)
				}
				unstructedPod := &unstructured.Unstructured{Object: p}
				_, err = src.Resource(v1.Resource("pods").WithVersion("v1")).Namespace(pod.Namespace).Create(ctx, unstructedPod, metav1.CreateOptions{})
				if err != nil {
					t.Fatalf("failed to create pod: %v", err)
				}
				createdPods.Insert(podKey{pod.Name, pod.Namespace})
			}

			go service.Run(ctx)
			defer cancel()

			for _, pod := range tt.podsCreatedInSrcCluster {
				p, err := runtime.DefaultUnstructuredConverter.ToUnstructured(pod)
				if err != nil {
					t.Fatalf("failed to convert pod to unstructured: %v", err)
				}
				unstructedPod := &unstructured.Unstructured{Object: p}
				_, err = src.Resource(v1.Resource("pods").WithVersion("v1")).Namespace(pod.Namespace).Create(ctx, unstructedPod, metav1.CreateOptions{})
				if err != nil {
					t.Fatalf("failed to create pod: %v", err)
				}
				createdPods.Insert(podKey{pod.Name, pod.Namespace})
			}

			for _, pod := range tt.podsUpdatedInSrcCluster {
				p, err := runtime.DefaultUnstructuredConverter.ToUnstructured(pod)
				if err != nil {
					t.Fatalf("failed to convert pod to unstructured: %v", err)
				}
				unstructedPod := &unstructured.Unstructured{Object: p}
				_, err = src.Resource(v1.Resource("pods").WithVersion("v1")).Namespace(pod.Namespace).Update(ctx, unstructedPod, metav1.UpdateOptions{})
				if err != nil {
					t.Fatalf("failed to update pod: %v", err)
				}
			}

			for _, pod := range tt.podsDeletedInSrcCluster {
				err := src.Resource(v1.Resource("pods").WithVersion("v1")).Namespace(pod.Namespace).Delete(ctx, pod.Name, metav1.DeleteOptions{})
				if err != nil {
					t.Fatalf("failed to delete pod: %v", err)
				}
			}

			errMessage := ""
			err := wait.PollUntilContextTimeout(ctx, 100*time.Millisecond, 5*time.Second, false, func(context.Context) (done bool, err error) {
				checkedPods := sets.New[podKey]()
				for _, pod := range tt.afterPodsInDestCluster {
					// get Pod from dest cluster
					p, err := dest.Resource(v1.Resource("pods").WithVersion("v1")).Namespace(pod.Namespace).Get(ctx, pod.Name, metav1.GetOptions{})
					if err != nil {
						errMessage = fmt.Sprintf("failed to get pod: %v", err)
						return false, nil
					}

					// convert Pod to v1.Pod
					var got v1.Pod
					err = runtime.DefaultUnstructuredConverter.FromUnstructured(p.Object, &got)
					if err != nil {
						errMessage = fmt.Sprintf("failed to convert pod to v1.Pod: %v", err)
						return false, nil
					}

					if diff := cmp.Diff(pod, &got, cmpopts.IgnoreTypes(metav1.Time{})); diff != "" {
						errMessage = fmt.Sprintf("diff: %s", diff)
						return false, nil
					}
					checkedPods.Insert(podKey{pod.Name, pod.Namespace})
				}

				for _, pod := range createdPods.Difference(checkedPods).UnsortedList() {
					// get Pod from dest cluster
					_, err := dest.Resource(v1.Resource("pods").WithVersion("v1")).Namespace(pod.namespace).Get(ctx, pod.name, metav1.GetOptions{})
					if err != nil && !apierrors.IsNotFound(err) {
						errMessage = fmt.Sprintf("failed to get pod: %v", err)
						return false, nil
					}
					if err == nil {
						errMessage = fmt.Sprintf("pod %s/%s should be deleted", pod.namespace, pod.name)
						return false, nil
					}
				}

				return true, nil
			})
			if err != nil {
				t.Fatal(errMessage)
			}
		})
	}
}

type podKey struct{ name, namespace string }
</file>

<file path="kube-scheduler-simulator/simulator/syncer/syncer.go">
package syncer

import (
	"context"

	"golang.org/x/xerrors"
	"k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
	"k8s.io/apimachinery/pkg/runtime/schema"
	"k8s.io/client-go/dynamic"
	"k8s.io/client-go/dynamic/dynamicinformer"
	"k8s.io/client-go/tools/cache"
	"k8s.io/klog/v2"

	"sigs.k8s.io/kube-scheduler-simulator/simulator/resourceapplier"
)

// DefaultGVRs is a list of GroupVersionResource that we sync by default (configurable with Options),
// which is a suitable resource set for the vanilla scheduler.
//
// Note that this order matters - When first importing resources, we want to sync namespaces first, then priorityclasses, storageclasses...
var DefaultGVRs = []schema.GroupVersionResource{
	{Group: "", Version: "v1", Resource: "namespaces"},
	{Group: "scheduling.k8s.io", Version: "v1", Resource: "priorityclasses"},
	{Group: "storage.k8s.io", Version: "v1", Resource: "storageclasses"},
	{Group: "", Version: "v1", Resource: "persistentvolumeclaims"},
	{Group: "", Version: "v1", Resource: "nodes"},
	{Group: "", Version: "v1", Resource: "persistentvolumes"},
	{Group: "", Version: "v1", Resource: "pods"},
}

type Service struct {
	gvrs                   []schema.GroupVersionResource
	srcDynamicClient       dynamic.Interface
	resourceApplierService *resourceapplier.Service
}

func New(srcDynamicClient dynamic.Interface, resourceApplierService *resourceapplier.Service) *Service {
	s := &Service{
		gvrs:                   DefaultGVRs,
		srcDynamicClient:       srcDynamicClient,
		resourceApplierService: resourceApplierService,
	}

	if resourceApplierService.GVRsToSync != nil {
		s.gvrs = resourceApplierService.GVRsToSync
	}

	return s
}

func (s *Service) Run(ctx context.Context) error {
	klog.Info("Starting the cluster resource importer")

	infFact := dynamicinformer.NewFilteredDynamicSharedInformerFactory(s.srcDynamicClient, 0, metav1.NamespaceAll, nil)
	for _, gvr := range s.gvrs {
		inf := infFact.ForResource(gvr).Informer()
		_, err := inf.AddEventHandler(cache.ResourceEventHandlerFuncs{
			AddFunc:    s.addFunc,
			UpdateFunc: s.updateFunc,
			DeleteFunc: s.deleteFunc,
		})
		if err != nil {
			return xerrors.Errorf("failed to add event handler: %w", err)
		}
		go inf.Run(ctx.Done())
		infFact.WaitForCacheSync(ctx.Done())
	}

	klog.Info("Cluster resource syncer started")

	return nil
}

func (s *Service) addFunc(obj interface{}) {
	ctx := context.Background()
	unstructObj, ok := obj.(*unstructured.Unstructured)
	if !ok {
		klog.Error("Failed to convert runtime.Object to *unstructured.Unstructured")
		return
	}

	err := s.resourceApplierService.Create(ctx, unstructObj)
	if err != nil {
		klog.ErrorS(err, "Failed to create resource on destination cluster")
	}
}

func (s *Service) updateFunc(_, newObj interface{}) {
	ctx := context.Background()
	unstructObj, ok := newObj.(*unstructured.Unstructured)
	if !ok {
		klog.Error("Failed to convert runtime.Object to *unstructured.Unstructured")
		return
	}

	err := s.resourceApplierService.Update(ctx, unstructObj)
	if err != nil {
		if errors.IsNotFound(err) {
			// We just ignore the not found error because the scheduler may preempt the Pods, or users may remove the resources for debugging.
			klog.Info("Skipped to update resource on destination: ", err)
		} else {
			klog.ErrorS(err, "Failed to update resource on destination cluster")
		}
	}
}

func (s *Service) deleteFunc(obj interface{}) {
	ctx := context.Background()
	unstructObj, ok := obj.(*unstructured.Unstructured)
	if !ok {
		klog.Error("Failed to convert runtime.Object to *unstructured.Unstructured")
		return
	}

	err := s.resourceApplierService.Delete(ctx, unstructObj)
	if err != nil {
		if errors.IsNotFound(err) {
			// We just ignore the not found error because the scheduler may preempt the Pods, or users may remove the resources for debugging.
			klog.Info("Skipped to delete resource on destination: ", err)
		} else {
			klog.ErrorS(err, "Failed to delete resource on destination cluster")
		}
	}
}
</file>

<file path="kube-scheduler-simulator/simulator/util/decoder_test.go">
package util

import (
	"reflect"
	"testing"
)

func TestPrivateFieldsDecoder(t *testing.T) {
	t.Parallel()
	type args struct {
		stru             interface{}
		privateFieldName string
	}
	type NestedEmbedded struct {
		nestedHiddenField int
	}
	type Embedded struct {
		hiddenField string
		nested      NestedEmbedded
	}
	type MyStruct struct {
		privateInt    int
		privateString string
		privateSlice  []int
		privateMap    map[string]int
		embedded      Embedded
	}
	tests := []struct {
		name     string
		args     args
		assertFn func(a reflect.Value) bool
	}{
		{
			name: "Accessing private int field",
			args: args{
				stru:             &MyStruct{privateInt: 42},
				privateFieldName: "privateInt",
			},
			assertFn: func(a reflect.Value) bool {
				return a.Int() == 42
			},
		},
		{
			name: "Accessing private string field",
			args: args{
				stru:             &MyStruct{privateString: "Hello, world!"},
				privateFieldName: "privateString",
			},
			assertFn: func(a reflect.Value) bool {
				return a.String() == "Hello, world!"
			},
		},
		{
			name: "Accessing private slice field",
			args: args{
				stru:             &MyStruct{privateSlice: []int{1, 2, 3}},
				privateFieldName: "privateSlice",
			},
			assertFn: func(a reflect.Value) bool {
				return a.Len() == 3 && a.Index(0).Int() == 1 && a.Index(1).Int() == 2 && a.Index(2).Int() == 3
			},
		},
		{
			name: "Accessing private map field",
			args: args{
				stru:             &MyStruct{privateMap: map[string]int{"one": 1, "two": 2}},
				privateFieldName: "privateMap",
			},
			assertFn: func(a reflect.Value) bool {
				expectedMap := map[string]int{"one": 1, "two": 2}
				mapKeys := a.MapKeys()

				if len(mapKeys) != len(expectedMap) {
					return false
				}

				for _, key := range mapKeys {
					expectedValue, ok := expectedMap[key.String()]
					if !ok || a.MapIndex(key).Int() != int64(expectedValue) {
						return false
					}
				}

				return true
			},
		},
		{
			name: "Accessing embedded private field",
			args: args{
				stru:             &MyStruct{embedded: Embedded{hiddenField: "Hidden message"}},
				privateFieldName: "embedded.hiddenField",
			},
			assertFn: func(a reflect.Value) bool {
				return a.String() == "Hidden message"
			},
		},
		{
			name: "Accessing nested private struct field",
			args: args{
				stru: &MyStruct{
					embedded: Embedded{
						nested: NestedEmbedded{
							nestedHiddenField: 99,
						},
					},
				},
				privateFieldName: "embedded.nested.nestedHiddenField",
			},
			assertFn: func(a reflect.Value) bool {
				return a.Int() == 99
			},
		},
	}
	for _, tt := range tests {
		tt := tt
		t.Run(tt.name, func(t *testing.T) {
			t.Parallel()
			got := PrivateFieldsDecoder(tt.args.stru, tt.args.privateFieldName)
			if !tt.assertFn(got) {
				t.Errorf("PrivateFieldsDecoder() = %v", got)
			}
		})
	}
}
</file>

<file path="kube-scheduler-simulator/simulator/util/decoder.go">
package util

import (
	"reflect"
	"strings"
)

// PrivateFieldsDecoder is a function that provides access to the values of private fields
// within a specified struct. The stru parameter should be a pointer to a struct containing
// the private field(s). The privateFieldName parameter should be the name of the private
// field you wish to access. If you need to access a subfield, specify the field names
// separated by dots (e.g. "privateField2.privateField").
//
// The function returns a reflect.Value object containing the value of the specified private field.
// See TestPrivateFieldsDecoder in decoder_test.go to know how to access your wanted field via reflect.Value.
func PrivateFieldsDecoder(stru interface{}, privateFieldName string) reflect.Value {
	value := reflect.ValueOf(stru).Elem()
	fieldNames := strings.Split(privateFieldName, ".")

	for _, fieldName := range fieldNames {
		value = value.FieldByName(fieldName)
	}

	return value
}
</file>

<file path="kube-scheduler-simulator/simulator/util/retry.go">
package util

import (
	"time"

	"k8s.io/apimachinery/pkg/util/wait"
)

const (
	// Parameters for retrying with exponential backoff.
	retryBackoffInitialDuration = 100 * time.Millisecond
	retryBackoffFactor          = 3
	retryBackoffJitter          = 0
	retryBackoffSteps           = 6
)

// RetryWithExponentialBackOff is the utility for retrying the given function with exponential backoff.
func RetryWithExponentialBackOff(fn wait.ConditionFunc) error {
	backoff := wait.Backoff{
		Duration: retryBackoffInitialDuration,
		Factor:   retryBackoffFactor,
		Jitter:   retryBackoffJitter,
		Steps:    retryBackoffSteps,
	}
	return wait.ExponentialBackoff(backoff, fn)
}
</file>

<file path="kube-scheduler-simulator/simulator/util/semaphored_errgroup.go">
package util

import (
	"context"
	"runtime"

	"golang.org/x/sync/errgroup"
	"golang.org/x/sync/semaphore"
	"golang.org/x/xerrors"
)

type SemaphoredErrGroup struct {
	g *errgroup.Group
	s *semaphore.Weighted
}

func NewErrGroupWithSemaphore(ctx context.Context) *SemaphoredErrGroup {
	g, _ := errgroup.WithContext(ctx)
	sem := semaphore.NewWeighted(int64(runtime.GOMAXPROCS(0)))
	return &SemaphoredErrGroup{
		g: g,
		s: sem,
	}
}

func (e *SemaphoredErrGroup) Go(fn func() error) error {
	ctx := context.Background()
	if err := e.s.Acquire(ctx, 1); err != nil {
		return xerrors.Errorf("acquire semaphore: %w", err)
	}

	e.g.Go(func() error {
		defer e.s.Release(1)
		return fn()
	})
	return nil
}

func (e *SemaphoredErrGroup) Wait() error {
	return e.g.Wait()
}
</file>

<file path="kube-scheduler-simulator/simulator/.dockerignore">
web/*
submodules/kubernetes/*
compose.yml
compose.local.yml
</file>

<file path="kube-scheduler-simulator/simulator/.golangci.yml">
run:
  skip-dirs:
    - docs
linters-settings:
  # if you fix this gci config, you should modify the gci command on `make format` as well.
  gci:
    sections:
      - Standard
      - Default
      - Prefix(sigs.k8s.io/kube-scheduler-simulator/simulator)

  gocritic:
    disabled-checks:
      - appendAssign

  revive:
    rules:
      # default settings
      # from: https://github.com/mgechev/revive/blob/master/defaults.toml
      - name: blank-imports
      - name: context-as-argument
      - name: context-keys-type
      - name: dot-imports
      - name: error-return
      - name: error-strings
      - name: error-naming
      - name: exported
      - name: if-return
      - name: increment-decrement
      - name: var-naming
      - name: var-declaration
      - name: package-comments
      - name: range
      - name: receiver-naming
      - name: time-naming
      - name: unexported-return
      - name: indent-error-flow
      - name: errorf
      - name: empty-block
      - name: superfluous-else
      - name: unused-parameter
      - name: unreachable-code
      - name: redefines-builtin-id
      # additional settings
      - name: duplicated-imports

linters:
  disable-all: true
  enable:
    - unused
    - govet
    - staticcheck
    - typecheck
    - errcheck
    - gosimple
    - asciicheck
    - bodyclose
    - cyclop
    - dogsled
    - durationcheck
    - errorlint
    - forcetypeassert
    - funlen
    - gci
    - gochecknoinits
    - gocognit
    - gocritic
    - gocyclo
    - godot
    - err113
    - gofmt
    - gofumpt
    - gosec
    - makezero
    - nakedret
    - nestif
    - nilerr
    - nolintlint
    - paralleltest
    - predeclared
    - revive
    - rowserrcheck
    - sqlclosecheck
    - thelper
    - unconvert
    - unparam
    - wastedassign
    - prealloc

issues:
  exclude-rules:
    - path: _test\.go
      linters:
        - funlen
        - goerr113
        - errcheck
        - cyclop
</file>

<file path="kube-scheduler-simulator/simulator/config.yaml">
# Configuration file for scheduler-simulator

# This is an example config for scheduler-simulator.

apiVersion: kube-scheduler-simulator-config/v1alpha1
kind: SimulatorConfiguration

# This is the port number on which kube-scheduler-simulator
# server is started.
port: 1212

# This is the URL for etcd. The simulator runs kube-apiserver
# internally, and the kube-apiserver uses this etcd.
etcdURL: "http://127.0.0.1:2379"

# This URL represents the URL once web UI is started.
# The simulator and internal kube-apiserver set the allowed
# origin for CorsAllowedOriginList
corsAllowedOriginList:
  - "http://localhost:3000"

# This is for the beta feature "One-shot importing cluster's resources"
# and "Continuous syncing cluster's resources".
# This variable is used to find Kubeconfig required to access your
# cluster for importing resources to scheduler simulator.
kubeConfig: "/kubeconfig.yaml"

# This is the url of kube-apiserver.
# This variable is used to connect to the user cluster's kube-apiserver.
kubeApiServerUrl: ""

# The path to a KubeSchedulerConfiguration file.
# If passed, the simulator will start the scheduler
# with that configuration. Or, if you use web UI,
# you can change the configuration from the web UI as well.
kubeSchedulerConfigPath: ""

# This variable indicates whether the simulator will
# import resources from a user cluster specified by kubeConfig.
# Note that it only imports the resources once when the simulator is started.
# You cannot make both externalImportEnabled and resourceSyncEnabled true because those features would be conflicted.
# This is still a beta feature.
externalImportEnabled: false

# This variable indicates whether the simulator will
# keep syncing resources from an user cluster's or not.
# You cannot make both externalImportEnabled and resourceSyncEnabled true because those features would be conflicted.
# Note, this is still a beta feature.
resourceSyncEnabled: false
</file>

<file path="kube-scheduler-simulator/simulator/kubeconfig.yaml">
apiVersion: v1
clusters:
- cluster:
    server: http://fake-source-cluster:3132
  name: kwok
contexts:
- context:
    cluster: kwok
  name: kwok
current-context: kwok
kind: Config
preferences: {}
users: null
</file>

<file path="kube-scheduler-simulator/simulator/Makefile">
.PHONY: generate
generate:
	go generate ./...
	make format

.PHONY: lint
lint:
	golangci-lint run --timeout 90m ./...

.PHONY: format
format:
	find . -name "*.go" | xargs gci write --section Standard --section Default --section "Prefix(sigs.k8s.io/kube-scheduler-simulator/simulator)"
	golangci-lint run --fix ./...

.PHONY: test
test: 
	go test ./...

.PHONY: mod-download
mod-download: ## Downloads the Go module
	go mod download -x

.PHONY: build
build:  
	go build -o ./bin/simulator ./cmd/simulator/simulator.go
	go build -o ./bin/scheduler ./cmd/scheduler/scheduler.go
</file>

<file path="kube-scheduler-simulator/tools/tools.go">
//go:build tools
// +build tools

package tools

import (
	_ "github.com/daixiang0/gci"
	_ "go.uber.org/mock/gomock"
)
</file>

<file path="kube-scheduler-simulator/web/api/v1/export.ts">
import { V1Namespace, V1Pod } from "@kubernetes/client-node";
import { V1Node } from "@kubernetes/client-node";
import { V1PersistentVolume } from "@kubernetes/client-node";
import { V1PersistentVolumeClaim } from "@kubernetes/client-node";
import { V1StorageClass } from "@kubernetes/client-node";
import { V1PriorityClass } from "@kubernetes/client-node";
import { SchedulerConfiguration } from "./types";
import { AxiosInstance } from "axios";

export default function exportAPI(instance: AxiosInstance) {
  return {
    exportScheduler: async () => {
      const res = await instance.get<ResourcesForImport>(`/export`, {});
      return res.data;
    },

    importScheduler: async (data: ResourcesForImport) => {
      try {
        const res = await instance.post<ResourcesForImport>(`/import`, data);
        return res.data;
      } catch (e: any) {
        throw new Error(e);
      }
    },
  };
}

export declare class ResourcesForImport {
  "pods": V1Pod[];
  "nodes": V1Node[];
  "pvs": V1PersistentVolume[];
  "pvcs": V1PersistentVolumeClaim[];
  "storageClasses": V1StorageClass[];
  "priorityClasses": V1PriorityClass[];
  "schedulerConfig": SchedulerConfiguration;
  "namespaces": V1Namespace[];
}

export type ExportAPI = ReturnType<typeof exportAPI>;
</file>

<file path="kube-scheduler-simulator/web/api/v1/namespace.ts">
import { V1Namespace, V1NamespaceList } from "@kubernetes/client-node";
import { AxiosInstance} from "axios";

export default function namespaceAPI(k8sInstance: AxiosInstance) {
  return {
    // createNamespace accepts only Namespace that has .metadata.GenerateName.
    // If you want to create a Pod that has .metadata.Name, use applyPod instead.
    createNamespace: async (req: V1Namespace) => {
      try {
        if (!req.metadata?.generateName) {
          throw new Error("metadata.generate")
        }
        req.kind = "Namespace";
        req.apiVersion = "v1";
        if (req.metadata.managedFields) {
          delete req.metadata.managedFields;
        }
        const res = await k8sInstance.post<V1Namespace>(
          "/namespaces?fieldManager=simulator&force=true",
          req,
          { headers: { "Content-Type": "application/yaml" }}
        );
        return res.data;
      } catch (e: any) {
        throw new Error(`failed to create namespace: ${e}`);
      }
    },
    applyNamespace: async (req: V1Namespace) => {
      try {
        if (!req.metadata?.name) {
          throw new Error("metadata.name is not provided.");
        }
        req.kind = "Namespace";
        req.apiVersion = "v1";
        if (req.metadata.managedFields) {
          delete req.metadata.managedFields;
        }
        const res = await k8sInstance.patch<V1Namespace>(
          `/namespaces/${req.metadata.name}?fieldManager=simulator&force=true`,
          req,
          { headers: { "Content-Type": "application/apply-patch+yaml" }}
        );
        return res.data;
      } catch (e: any) {
        throw new Error(`failed to apply namespace`);
      }
    },
    listNamespace: async () => {
      try {
        const res = await k8sInstance.get<V1NamespaceList>("/namespaces", {});
        return res.data;
      } catch (e: any) {
        throw new Error(`failed to listt namespaces: ${e}`);
      }
    },
    getNamespace: async (name: string) => {
      try {
        const res = await k8sInstance.get<V1Namespace>(`/namespaces/${name}`, {});
        return res.data;
      } catch (e: any) {
        throw new Error(`failed to get namespace: ${e}`);
      }
    },
    deleteNamespace: async (name: string) => {
      try {
        const res = await k8sInstance.delete<V1Namespace>(`/namespaces/${name}`, {});
        return res.data;
      } catch (e: any) {
        throw new Error(`failed to delete namespace: ${e}`);
      }
    },
    // finalizeNamespace finalizes the specified namespace.
    // This expected to be called when after the deleteNamespace method is called and the namespace's Status remains "Terminating".
    finalizeNamespace: async (req: V1Namespace) => {
      try {
        const res = await k8sInstance.put(`/namespaces/${req.metadata?.name}/finalize`,
        req,
        { headers: { "Content-Type": "application/json" }}
      );
        return res.data;
      } catch (e: any) {
        throw new Error(`failed to finalize namespace: ${e}`);
      }
    }
  };
}
export type NamespaceAPI = ReturnType<typeof namespaceAPI>;
</file>

<file path="kube-scheduler-simulator/web/api/v1/node.ts">
import { V1Node, V1NodeList } from "@kubernetes/client-node";
import { AxiosInstance } from "axios";

export default function nodeAPI(k8sInstance: AxiosInstance) {
  return {
    // createNode accepts only Node that has .metadata.GeneratedName.
    // If you want to create a Node that has .metadata.Name, use applyNode instead.
    createNode: async (req: V1Node) => {
      try {
        if (!req.metadata?.generateName) {
          throw new Error("metadata.generateName is not provided");
        }
        req.kind = "Node";
        req.apiVersion = "v1";
        if (req.metadata.managedFields) {
          delete req.metadata.managedFields;
        }
        const res = await k8sInstance.post<V1Node>(
          "/nodes?fieldManager=simulator&force=true",
          req,
          { headers: { "Content-Type": "application/yaml" } }
        );
        return res.data;
      } catch (e: any) {
        throw new Error(`failed to create node: ${e}`);
      }
    },
    applyNode: async (req: V1Node) => {
      try {
        if (!req.metadata?.name) {
          throw new Error("metadata.name is not provided");
        }
        req.kind = "Node";
        req.apiVersion = "v1";
        if (req.metadata.managedFields) {
          delete req.metadata.managedFields;
        }
        const res = await k8sInstance.patch<V1Node>(
          `/nodes/${req.metadata.name}?fieldManager=simulator&force=true`,
          req,
          { headers: { "Content-Type": "application/apply-patch+yaml" } }
        );
        return res.data;
      } catch (e: any) {
        throw new Error(`failed to apply node: ${e}`);
      }
    },

    listNode: async () => {
      try {
        const res = await k8sInstance.get<V1NodeList>("/nodes", {});
        return res.data;
      } catch (e: any) {
        throw new Error(`failed to list nodes: ${e}`);
      }
    },

    getNode: async (name: string) => {
      try {
        const res = await k8sInstance.get<V1Node>(`/nodes/${name}`, {});
        return res.data;
      } catch (e: any) {
        throw new Error(`failed to get node: ${e}`);
      }
    },

    deleteNode: async (name: string) => {
      try {
        const res = await k8sInstance.delete(`/nodes/${name}`, {});
        return res.data;
      } catch (e: any) {
        throw new Error(`failed to delete node: ${e}`);
      }
    },
  };
}
export type NodeAPI = ReturnType<typeof nodeAPI>;
</file>

<file path="kube-scheduler-simulator/web/api/v1/pod.ts">
import { V1Pod, V1PodList } from "@kubernetes/client-node";
import { AxiosInstance } from "axios";

export default function podAPI(k8sInstance: AxiosInstance) {
  const defaultNamespace = "default"
  return {
    // createPod accepts only Pod that has .metadata.GeneratedName.
    // If you want to create a Pod that has .metadata.Name, use applyPod instead.
    createPod: async (req: V1Pod) => {
      try {
        if (!req.metadata?.generateName) {
          throw new Error("metadata.generateName is not provided");
        }
        if (!req.metadata.namespace) {
          throw new Error("metadata.namespace is not provided");
        }
        req.kind = "Pod";
        req.apiVersion = "v1"; 
        if (req.metadata.managedFields) {
          delete req.metadata.managedFields;
        }
        const res = await k8sInstance.post<V1Pod>(
          `namespaces/${req.metadata.namespace}/pods?fieldManager=simulator&force=true`,
          req,
          { headers: { "Content-Type": "application/yaml" } }
        );
        return res.data;
      } catch (e: any) {
        throw new Error(`failed to create pod: ${e}`);
      }
    },
    applyPod: async (req: V1Pod) => {
      try {
        if (!req.metadata?.name) {
          throw new Error("metadata.name is not provided");
        }
        if (!req.metadata.namespace) {
          throw new Error("metadata.namespace is not provided");
        }
        req.kind = "Pod";
        req.apiVersion = "v1";
        if (req.metadata.managedFields) {
          delete req.metadata.managedFields;
        }
        const res = await k8sInstance.patch<V1Pod>(
            `namespaces/${req.metadata.namespace}/pods/${req.metadata.name}?fieldManager=simulator&force=true`,
          req,
          { headers: { "Content-Type": "application/apply-patch+yaml" } }
        );
        return res.data;
      } catch (e: any) {
        throw new Error(`failed to apply pod: ${e}`);
      }
    },
    listPod: async () => {
      try {
        // This URL path could list all pods on each namespace.
        const res = await k8sInstance.get<V1PodList>(
          "pods",
          {}
        );
        return res.data;
      } catch (e: any) {
        throw new Error(`failed to list pods: ${e}`);
      }
    },
    getPod: async (namespace: string, name: string) => {
      try {
        const res = await k8sInstance.get<V1Pod>(
          `namespaces/${namespace}/pods/${name}`,
          {}
        );
        return res.data;
      } catch (e: any) {
        throw new Error(`failed to get pod: ${e}`);
      }
    },
    deletePod: async (namespace: string, name: string) => {
      try {
        const res = await k8sInstance.delete(
          `namespaces/${namespace}/pods/${name}?gracePeriodSeconds=0`,
          {}
        );
        return res.data;
      } catch (e: any) {
        throw new Error(`failed to delete pod: ${e}`);
      }
    },
  };
}

export type PodAPI = ReturnType<typeof podAPI>;
</file>

<file path="kube-scheduler-simulator/web/api/v1/priorityclass.ts">
import { V1PriorityClass, V1PriorityClassList } from "@kubernetes/client-node";
import { AxiosInstance } from "axios";

export default function priorityClassAPI(k8sSchedulingInstance: AxiosInstance) {
  return {
    // createPriorityClass accepts only PriorityClass that has .metadata.GeneratedName.
    // If you want to create a PriorityClass that has .metadata.Name, use applyPriorityClass instead.
    createPriorityClass: async (req: V1PriorityClass) => {
      try {
        if (!req.metadata?.generateName) {
          throw new Error("metadata.generateName is not provided");
        }
        req.kind = "PriorityClass";
        req.apiVersion = "scheduling.k8s.io/v1";
        if (req.metadata.managedFields) {
          delete req.metadata.managedFields;
        }
        const res = await k8sSchedulingInstance.post<V1PriorityClass>(
          "/priorityclasses?fieldManager=simulator&force=true",
          req,
          { headers: { "Content-Type": "application/yaml" } }
        );
        return res.data;
      } catch (e: any) {
        throw new Error(`failed to create priority class: ${e}`);
      }
    },
    applyPriorityClass: async (req: V1PriorityClass) => {
      try {
        if (!req.metadata?.name) {
          throw new Error("metadata.name is not provided");
        }
        req.kind = "PriorityClass";
        req.apiVersion = "scheduling.k8s.io/v1";
        if (req.metadata.managedFields) {
          delete req.metadata.managedFields;
        }
        const res = await k8sSchedulingInstance.patch<V1PriorityClass>(
          `/priorityclasses/${req.metadata.name}?fieldManager=simulator&force=true`,
          req,
          { headers: { "Content-Type": "application/apply-patch+yaml" } }
        );
        return res.data;
      } catch (e: any) {
        throw new Error(`failed to apply priority class: ${e}`);
      }
    },

    listPriorityClass: async () => {
      try {
        const res = await k8sSchedulingInstance.get<V1PriorityClassList>(
          "/priorityclasses",
          {}
        );
        return res.data;
      } catch (e: any) {
        throw new Error(`failed to list priority classes: ${e}`);
      }
    },

    getPriorityClass: async (name: string) => {
      try {
        const res = await k8sSchedulingInstance.get<V1PriorityClass>(
          `/priorityclasses/${name}`,
          {}
        );
        return res.data;
      } catch (e: any) {
        throw new Error(`failed to get priority class: ${e}`);
      }
    },

    deletePriorityClass: async (name: string) => {
      try {
        const res = await k8sSchedulingInstance.delete(
          `/priorityclasses/${name}`,
          {}
        );
        return res.data;
      } catch (e: any) {
        throw new Error(`failed to delete priority class: ${e}`);
      }
    },
  };
}

export type PriorityClassAPI = ReturnType<typeof priorityClassAPI>;
</file>

<file path="kube-scheduler-simulator/web/api/v1/pv.ts">
import {
  V1PersistentVolume,
  V1PersistentVolumeList,
} from "@kubernetes/client-node";
import { AxiosInstance } from "axios";

export default function pvAPI(k8sInstance: AxiosInstance) {
  return {
    // createPersistentVolume accepts only PersistentVolume that has .metadata.GeneratedName.
    // If you want to create a PersistentVolume that has .metadata.Name, use applyPersistentVolume instead.
    createPersistentVolume: async (req: V1PersistentVolume) => {
      try {
        if (!req.metadata?.generateName) {
          throw new Error("metadata.genrateName is not provided");
        }
        req.kind = "PersistentVolume";
        req.apiVersion = "v1";
        if (req.metadata.managedFields) {
          delete req.metadata.managedFields;
        }
        const res = await k8sInstance.post<V1PersistentVolume>(
          "/persistentvolumes?fieldManager=simulator&force=true",
          req,
          { headers: { "Content-Type": "application/yaml" } }
        );
        return res.data;
      } catch (e: any) {
        throw new Error(`failed to create persistent volume: ${e}`);
      }
    },
    applyPersistentVolume: async (req: V1PersistentVolume) => {
      try {
        if (!req.metadata?.name) {
          throw new Error("metadata.name is not provided");
        }
        req.kind = "PersistentVolume";
        req.apiVersion = "v1";
        if (req.metadata.managedFields) {
          delete req.metadata.managedFields;
        }
        const res = await k8sInstance.patch<V1PersistentVolume>(
          `/persistentvolumes/${req.metadata.name}?fieldManager=simulator&force=true`,
          req,
          { headers: { "Content-Type": "application/apply-patch+yaml" } }
        );
        return res.data;
      } catch (e: any) {
        throw new Error(`failed to apply persistent volume: ${e}`);
      }
    },

    listPersistentVolume: async () => {
      try {
        const res = await k8sInstance.get<V1PersistentVolumeList>(
          "/persistentvolumes",
          {}
        );
        return res.data;
      } catch (e: any) {
        throw new Error(`failed to list persistent volumes: ${e}`);
      }
    },

    getPersistentVolume: async (name: string) => {
      try {
        const res = await k8sInstance.get<V1PersistentVolume>(
          `/persistentvolumes/${name}`,
          {}
        );
        return res.data;
      } catch (e: any) {
        throw new Error(`failed to get persistent volume: ${e}`);
      }
    },

    deletePersistentVolume: async (name: string) => {
      try {
        const res = await k8sInstance.delete(`/persistentvolumes/${name}`, {});
        return res.data;
      } catch (e: any) {
        throw new Error(`failed to delete persistent volume: ${e}`);
      }
    },
  };
}

export type PVAPI = ReturnType<typeof pvAPI>;
</file>

<file path="kube-scheduler-simulator/web/api/v1/pvc.ts">
import {
  V1PersistentVolumeClaim,
  V1PersistentVolumeClaimList,
} from "@kubernetes/client-node";
import { AxiosInstance } from "axios";

export default function pvcAPI(k8sInstance: AxiosInstance) {
  return {
    // createPersistentVolumeClaim accepts only PersistentVolumeClaim that has .metadata.GeneratedName.
    // If you want to create a PersistentVolumeClaim that has .metadata.Name, use applyPersistentVolumeClaim instead.
    createPersistentVolumeClaim: async (req: V1PersistentVolumeClaim) => {
      try {
        if (!req.metadata?.generateName) {
          throw new Error("metadata.generateName is not provided");
        }
        if (!req.metadata?.namespace) {
          throw new Error("metadata.namespace is not provided");
        }
        req.kind = "PersistentVolumeClaim";
        req.apiVersion = "v1";
        if (req.metadata.managedFields) {
          delete req.metadata.managedFields;
        }
        const res = await k8sInstance.post<V1PersistentVolumeClaim>(
            `namespaces/${req.metadata.namespace}/persistentvolumeclaims?fieldManager=simulator&force=true`,
          req,
          { headers: { "Content-Type": "application/yaml" } }
        );
        return res.data;
      } catch (e: any) {
        throw new Error(`failed to create persistent volume claim: ${e}`);
      }
    },
    applyPersistentVolumeClaim: async (req: V1PersistentVolumeClaim) => {
      try {
        if (!req.metadata?.name) {
          throw new Error("metadata.name is not provided");
        }
        if (!req.metadata?.namespace) {
          throw new Error("metadata.namespace is not provided");
        }
        req.kind = "PersistentVolumeClaim";
        req.apiVersion = "v1";
        if (req.metadata.managedFields) {
          delete req.metadata.managedFields;
        }
        const res = await k8sInstance.patch<V1PersistentVolumeClaim>(
            `namespaces/${req.metadata.namespace}/persistentvolumeclaims/${req.metadata.name}?fieldManager=simulator&force=true`,
          req,
          { headers: { "Content-Type": "application/apply-patch+yaml" } }
        );
        return res.data;
      } catch (e: any) {
        throw new Error(`failed to apply persistent volume claim: ${e}`);
      }
    },

    listPersistentVolumeClaim: async () => {
      try {
        // This URL path could list all pods on each namespace.
        const res = await k8sInstance.get<V1PersistentVolumeClaimList>(
          "persistentvolumeclaims",
          {}
        );
        return res.data;
      } catch (e: any) {
        throw new Error(`failed to list persistent volume claims: ${e}`);
      }
    },

    getPersistentVolumeClaim: async (namespace: string, name: string) => {
      try {
        const res = await k8sInstance.get<V1PersistentVolumeClaim>(
          `namespaces/${namespace}/persistentvolumeclaims/${name}`,
          {}
        );
        return res.data;
      } catch (e: any) {
        throw new Error(`failed to get persistent volume claim: ${e}`);
      }
    },

    deletePersistentVolumeClaim: async (namespace: string, name: string) => {
      try {
        const res = await k8sInstance.delete(
          `namespaces/${namespace}/persistentvolumeclaims/${name}`,
          {}
        );
        return res.data;
      } catch (e: any) {
        throw new Error(`failed to delete persistent volume claim: ${e}`);
      }
    },
  };
}

export type PVCAPI = ReturnType<typeof pvcAPI>;
</file>

<file path="kube-scheduler-simulator/web/api/v1/reset.ts">
import { AxiosInstance } from "axios";

export default function resetAPI(instance: AxiosInstance) {
  return {
    reset: async () => {
      const res = await instance.put(`/reset`, {});
      return res.data;
    },
  };
}

export type ResetAPI = ReturnType<typeof resetAPI>;
</file>

<file path="kube-scheduler-simulator/web/api/v1/schedulerconfiguration.ts">
import { AxiosInstance } from "axios";
import { SchedulerConfiguration } from "./types";

export default function schedulerconfigurationAPI(instance: AxiosInstance) {
  return {
    applySchedulerConfiguration: async (req: SchedulerConfiguration) => {
      try {
        const res = await instance.post<SchedulerConfiguration>(
          `/schedulerconfiguration`,
          req
        );
        return res.data;
      } catch (e: any) {
        throw new Error(`failed to apply scheduler configration: ${e}`);
      }
    },

    getSchedulerConfiguration: async () => {
      const res = await instance.get<SchedulerConfiguration>(
        `/schedulerconfiguration`
      );
      return res.data;
    },
  };
}

export type SchedulerconfigurationAPI = ReturnType<
  typeof schedulerconfigurationAPI
>;
</file>

<file path="kube-scheduler-simulator/web/api/v1/storageclass.ts">
import { V1StorageClass, V1StorageClassList } from "@kubernetes/client-node";
import { AxiosInstance } from "axios";

export default function storageClassAPI(k8sStorageInstance: AxiosInstance) {
  return {
    // createStorageClass accepts only StorageClass that has .metadata.GeneratedName.
    // If you want to create a StorageClass that has .metadata.Name, use applyStorageClass instead.
    createStorageClass: async (req: V1StorageClass) => {
      try {
        if (!req.metadata?.generateName) {
          throw new Error("metadata.generateName is not provided");
        }
        req.kind = "StorageClass";
        req.apiVersion = "storage.k8s.io/v1";
        if (req.metadata.managedFields) {
          delete req.metadata.managedFields;
        }
        const res = await k8sStorageInstance.post<V1StorageClass>(
          "/storageclasses?fieldManager=simulator&force=true",
          req,
          { headers: { "Content-Type": "application/yaml" } }
        );
        return res.data;
      } catch (e: any) {
        throw new Error(`failed to create storage class: ${e}`);
      }
    },
    applyStorageClass: async (req: V1StorageClass) => {
      try {
        if (!req.metadata?.name) {
          throw new Error("metadata.name is not provided");
        }
        req.kind = "StorageClass";
        req.apiVersion = "storage.k8s.io/v1";
        if (req.metadata.managedFields) {
          delete req.metadata.managedFields;
        }
        const res = await k8sStorageInstance.patch<V1StorageClass>(
          `/storageclasses/${req.metadata.name}?fieldManager=simulator&force=true`,
          req,
          { headers: { "Content-Type": "application/apply-patch+yaml" } }
        );
        return res.data;
      } catch (e: any) {
        throw new Error(`failed to apply storage class: ${e}`);
      }
    },

    listStorageClass: async () => {
      try {
        const res = await k8sStorageInstance.get<V1StorageClassList>(
          "/storageclasses",
          {}
        );
        return res.data;
      } catch (e: any) {
        throw new Error(`failed to list storage classes: ${e}`);
      }
    },

    getStorageClass: async (name: string) => {
      try {
        const res = await k8sStorageInstance.get<V1StorageClass>(
          `/storageclasses/${name}`,
          {}
        );
        return res.data;
      } catch (e: any) {
        throw new Error(`failed to get storage class: ${e}`);
      }
    },

    deleteStorageClass: async (name: string) => {
      try {
        const res = await k8sStorageInstance.delete(
          `/storageclasses/${name}`,
          {}
        );
        return res.data;
      } catch (e: any) {
        throw new Error(`failed to delete storage class: ${e}`);
      }
    },
  };
}

export type StorageClassAPI = ReturnType<typeof storageClassAPI>;
</file>

<file path="kube-scheduler-simulator/web/api/v1/types.ts">
export interface SchedulerConfiguration {
  kind: string;
  apiVersion: string;
  Profiles: KubeSchedulerProfile[];
}

export interface KubeSchedulerProfile {
  SchedulerName: string;
  Plugins: Plugins;
}

export interface Plugins {
  QueueSort: PluginSet;
  PreFilter: PluginSet;
  Filter: PluginSet;
  PostFilter: PluginSet;
  PreScore: PluginSet;
  Score: PluginSet;
  Reserve: PluginSet;
  Permit: PluginSet;
  PreBind: PluginSet;
  Bind: PluginSet;
  PostBind: PluginSet;
}

export interface PluginSet {
  Enabled: Plugin[];
  Disabled: Plugin[];
}

export interface Plugin {
  Name: string;
  Weight: number;
}
</file>

<file path="kube-scheduler-simulator/web/api/v1/watcher.ts">
import { AxiosInstance } from "axios";
import { LastResourceVersions } from "@/types/api/v1";

export default function watcherAPI(instance: AxiosInstance) {
  return {
    // watchResources is a server push API.
    watchResources: async (lrvs: LastResourceVersions) => {
      try {
        const queries = `podsLastResourceVersion=${lrvs.pods}&nodesLastResourceVersion=${lrvs.nodes}&pvsLastResourceVersion=${lrvs.pvs}&pvcsLastResourceVersion=${lrvs.pvcs}&scsLastResourceVersion=${lrvs.storageClasses}&pcsLastResourceVersion=${lrvs.priorityClasses}&namespaceLastResourceVersion=${lrvs.namespaces}`;
        // return stream of Node events.
        return await fetch(
          `${instance.defaults.baseURL}/listwatchresources?${queries}`
        );
      } catch (e: any) {
        throw new Error(`failed to start to watch resources: ${e}`);
      }
    },
  };
}

export type WatcherAPI = ReturnType<typeof watcherAPI>;
</file>

<file path="kube-scheduler-simulator/web/api/APIProvider.vue">
<template>
  <div>
    <slot />
  </div>
</template>

<script lang="ts">
import { defineComponent, provide, useContext } from "@nuxtjs/composition-api";
import podAPI from "./v1/pod";
import nodeAPI from "./v1/node";
import priorityClassAPI from "./v1/priorityclass";
import exportAPI from "./v1/export";
import pvAPI from "./v1/pv";
import pvcAPI from "./v1/pvc";
import resetAPI from "./v1/reset";
import schedulerconfigurationAPI from "./v1/schedulerconfiguration";
import storageClassAPI from "./v1/storageclass";
import namespaceAPI from "./v1/namespace";
import watcherAPI from "./v1/watcher";
import { PodAPIKey } from "./APIProviderKeys";
import { NodeAPIKey } from "./APIProviderKeys";
import { PriorityClassAPIKey } from "./APIProviderKeys";
import { ExportAPIKey } from "./APIProviderKeys";
import { PVAPIKey } from "./APIProviderKeys";
import { PVCAPIKey } from "./APIProviderKeys";
import { ResetAPIKey } from "./APIProviderKeys";
import { SchedulerconfigurationAPIKey } from "./APIProviderKeys";
import { StorageClassAPIKey } from "./APIProviderKeys";
import { NamespaceAPIKey } from "./APIProviderKeys";
import { WatcherAPIKey } from "./APIProviderKeys";

export default defineComponent({
  setup() {
    const { app } = useContext();
    provide(PodAPIKey, podAPI(app.$k8sInstance));
    provide(NodeAPIKey, nodeAPI(app.$k8sInstance));
    provide(PriorityClassAPIKey, priorityClassAPI(app.$k8sSchedulingInstance));
    provide(ExportAPIKey, exportAPI(app.$instance));
    provide(PVAPIKey, pvAPI(app.$k8sInstance));
    provide(PVCAPIKey, pvcAPI(app.$k8sInstance));
    provide(ResetAPIKey, resetAPI(app.$instance));
    provide(
      SchedulerconfigurationAPIKey,
      schedulerconfigurationAPI(app.$instance)
    );
    provide(StorageClassAPIKey, storageClassAPI(app.$k8sStorageInstance));
    provide(NamespaceAPIKey, namespaceAPI(app.$k8sInstance))
    provide(WatcherAPIKey, watcherAPI(app.$instance));
    return {};
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/api/APIProviderKeys.ts">
import { InjectionKey } from "@nuxtjs/composition-api";
import { PodAPI } from "./v1/pod";
import { NodeAPI } from "./v1/node";
import { ExportAPI } from "./v1/export";
import { PriorityClassAPI } from "./v1/priorityclass";
import { PVAPI } from "./v1/pv";
import { PVCAPI } from "./v1/pvc";
import { ResetAPI } from "./v1/reset";
import { SchedulerconfigurationAPI } from "./v1/schedulerconfiguration";
import { StorageClassAPI } from "./v1/storageclass";
import { WatcherAPI } from "./v1/watcher";
import { NamespaceAPI } from "./v1/namespace";

export const PodAPIKey: InjectionKey<PodAPI> = Symbol("PodAPI");
export const NodeAPIKey: InjectionKey<NodeAPI> = Symbol("NodeAPI");
export const ExportAPIKey: InjectionKey<ExportAPI> = Symbol("ExportAPI");
export const PriorityClassAPIKey: InjectionKey<PriorityClassAPI> =
  Symbol("PriorityClassAPI");
export const PVAPIKey: InjectionKey<PVAPI> = Symbol("PVAPI");
export const PVCAPIKey: InjectionKey<PVCAPI> = Symbol("PVCAPI");
export const ResetAPIKey: InjectionKey<ResetAPI> = Symbol("ResetAPI");
export const SchedulerconfigurationAPIKey: InjectionKey<SchedulerconfigurationAPI> =
  Symbol("SchedulerconfigurationAPI");
export const StorageClassAPIKey: InjectionKey<StorageClassAPI> =
  Symbol("StorageClassAPI");
export const WatcherAPIKey: InjectionKey<WatcherAPI> = Symbol("WatcherAPI");
export const NamespaceAPIKey: InjectionKey<NamespaceAPI> = Symbol("NamespaceAPI");
</file>

<file path="kube-scheduler-simulator/web/assets/README.md">
# ASSETS

**This directory is not required, you can delete it if you don't want to use it.**

This directory contains your un-compiled assets such as LESS, SASS, or JavaScript.

More information about the usage of this directory in [the documentation](https://nuxtjs.org/guide/assets#webpacked).
</file>

<file path="kube-scheduler-simulator/web/assets/variables.scss">
// Ref: https://github.com/nuxt-community/vuetify-module#customvariables
//
// The variables you want to modify
// $font-size-root: 20px;
</file>

<file path="kube-scheduler-simulator/web/components/ResourceBar/BarHeader.vue">
<template v-slot:prepend>
  <v-list-item two-line>
    <v-list-item-content>
      <v-list-item-title> {{ title }} </v-list-item-title>
      <v-row>
        <v-col>
          <v-switch
            v-if="enableEditmodeSwitch"
            class="ma-5 mb-0"
            inset
            label="edit"
            @change="editmodeOnChange"
          />
        </v-col>
        <v-spacer v-for="n in 3" :key="n" />
        <v-col>
          <v-btn class="ma-5 mb-0" @click="applyOnClick"> Apply </v-btn>
        </v-col>
        <v-col>
          <ResourceDeleteButton
            v-if="enableDeleteBtn"
            :delete-on-click="deleteOnClick"
          />
        </v-col>
      </v-row>
    </v-list-item-content>
  </v-list-item>
</template>

<script lang="ts">
import { ref, defineComponent } from "@nuxtjs/composition-api";
import ResourceDeleteButton from "./DeleteButton.vue";

export default defineComponent({
  components: {
    ResourceDeleteButton,
  },
  props: {
    title: {
      type: String,
      default: "",
    },
    deleteOnClick: {
      type: Function,
      default: null,
    },
    applyOnClick: {
      type: Function,
      default: null,
    },
    editmodeOnChange: {
      type: Function,
      default: null,
    },
    enableDeleteBtn: {
      type: Boolean,
      default: false,
    },
    enableEditmodeSwitch: {
      type: Boolean,
      default: false,
    },
  },
  setup() {
    const dialog = ref(false);
    return {
      dialog,
    };
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/components/ResourceBar/DefinitionTree.vue">
<template>
  <v-sheet>
    <v-card-title>Resource Definition</v-card-title>
    <v-treeview dense :items="items"></v-treeview>
  </v-sheet>
</template>

<script lang="ts">
import { defineComponent } from "@nuxtjs/composition-api";

export default defineComponent({
  props: {
    items: {
      type: Array,
      required: true,
    },
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/components/ResourceBar/DeleteButton.vue">
<template>
  <v-dialog v-model="dialog" width="500">
    <template #activator="{ on, attrs }">
      <v-btn class="ma-5 mb-0" color="error" v-bind="attrs" v-on="on">
        Delete
      </v-btn>
    </template>

    <v-card>
      <v-card-title class="2">
        Are you sure to delete the resource?
      </v-card-title>
      <v-divider></v-divider>
      <v-divider></v-divider>

      <v-card-actions>
        <v-spacer></v-spacer>
        <v-btn color="green darken-1" text @click="deleteOnClick"> Yes </v-btn>
        <v-btn color="green darken-1" text @click="dialog = false"> No </v-btn>
      </v-card-actions>
    </v-card>
  </v-dialog>
</template>
<script lang="ts">
import { ref, defineComponent } from "@nuxtjs/composition-api";
export default defineComponent({
  props: {
    deleteOnClick: {
      type: Function,
      required: true,
    },
  },
  setup() {
    const dialog = ref(false);

    return {
      dialog,
    };
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/components/ResourceBar/ResourceBar.vue">
<template>
  <v-navigation-drawer
    v-model="drawer"
    fixed
    right
    temporary
    bottom
    width="70%"
  >
    <BarHeader
      title="Resource"
      :delete-on-click="deleteOnClick"
      :apply-on-click="applyOnClick"
      :editmode-on-change="
        () => {
          editmode = !editmode;
        }
      "
      :enable-delete-btn="selected && !selected.isNew && selected.isDeletable"
      :enable-editmode-switch="selected && !selected.isNew"
    />

    <v-divider></v-divider>

    <template v-if="editmode">
      <v-spacer v-for="n in 3" :key="n" />
      <v-divider></v-divider>

      <YamlEditor v-model="formData" />
    </template>

    <template v-if="!editmode">
      <SchedulingResults v-if="selectedResourceKind() == 'Pod'" :selected="selectedPod" />
      <ResourceDefinitionTree :items="treeData" />
      <!-- This is required to work around the vuetify's bug, refer more details in #10 -->
      <div style="height: 80%"></div>
    </template>
  </v-navigation-drawer>
</template>
<script lang="ts">
import {
  ref,
  computed,
  inject,
  watch,
  defineComponent,
} from "@nuxtjs/composition-api";
import yaml from "js-yaml";
import PodStoreKey from "../StoreKey/PodStoreKey";
import { objectToTreeViewData } from "../lib/util";
import NodeStoreKey from "../StoreKey/NodeStoreKey";
import PersistentVolumeStoreKey from "../StoreKey/PVStoreKey";
import PersistentVolumeClaimStoreKey from "../StoreKey/PVCStoreKey";
import StorageClassStoreKey from "../StoreKey/StorageClassStoreKey";
import PriorityClassStoreKey from "../StoreKey/PriorityClassStoreKey";
import SchedulerConfigurationStoreKey from "../StoreKey/SchedulerConfigurationStoreKey";
import NamespaceStoreKey from "../StoreKey/NamespaceStoreKey";
import YamlEditor from "./YamlEditor.vue";
import SchedulingResults from "./SchedulingResults.vue";
import ResourceDefinitionTree from "./DefinitionTree.vue";
import BarHeader from "./BarHeader.vue";
import {
  V1Node,
  V1PersistentVolumeClaim,
  V1PersistentVolume,
  V1Pod,
  V1StorageClass,
  V1PriorityClassList,
  V1Namespace,
} from "@kubernetes/client-node";
import SnackBarStoreKey from "../StoreKey/SnackBarStoreKey";
import { SchedulerConfiguration } from "~/api/v1/types";

type Resource =
  | V1Pod
  | V1Node
  | V1PersistentVolumeClaim
  | V1PersistentVolume
  | V1StorageClass
  | V1PriorityClassList
  | SchedulerConfiguration
  | V1Namespace;

interface Store {
  readonly selected: object | null;
  resetSelected(): void;
  apply(_: Resource): Promise<void>;
  delete(_: Resource): Promise<void>;
  fetchSelected(): Promise<void>;
}

interface SelectedItem {
  isNew: boolean;
  item: Resource;
  resourceKind: string;
  isDeletable: boolean;
}

export default defineComponent({
  components: {
    YamlEditor,
    BarHeader,
    ResourceDefinitionTree,
    SchedulingResults,
  },
  setup() {
    var store: Store | null = null;

    // inject stores
    const podstore = inject(PodStoreKey);
    if (!podstore) {
      throw new Error(`${PodStoreKey.description} is not provided`);
    }
    const nodestore = inject(NodeStoreKey);
    if (!nodestore) {
      throw new Error(`${NodeStoreKey.description} is not provided`);
    }
    const pvstore = inject(PersistentVolumeStoreKey);
    if (!pvstore) {
      throw new Error(`${PersistentVolumeStoreKey.description} is not provided`);
    }
    const pvcstore = inject(PersistentVolumeClaimStoreKey);
    if (!pvcstore) {
      throw new Error(`${PersistentVolumeClaimStoreKey.description} is not provided`);
    }
    const storageclassstore = inject(StorageClassStoreKey);
    if (!storageclassstore) {
      throw new Error(`${StorageClassStoreKey.description} is not provided`);
    }
    const priorityclassstore = inject(PriorityClassStoreKey);
    if (!priorityclassstore) {
      throw new Error(`${PriorityClassStoreKey.description} is not provided`);
    }
    const schedulerconfigurationstore = inject(SchedulerConfigurationStoreKey);
    if (!schedulerconfigurationstore) {
      throw new Error(`${SchedulerConfigurationStoreKey.description} is not provided`);
    }
    const namespacestore = inject(NamespaceStoreKey);
    if (!namespacestore) {
      throw new Error(`${NamespaceStoreKey.description} is not provided`);
    }

    const snackbarstore = inject(SnackBarStoreKey);
    if (!snackbarstore) {
      throw new Error(`${SnackBarStoreKey.description} is not provided`);
    }

    const treeData = ref(objectToTreeViewData(null));

    // for edit mode
    const formData = ref("");

    // boolean to switch some view
    const drawer = ref(false);
    const editmode = ref(false);

    // watch each selected resource
    const selected = ref(null as SelectedItem | null);
    const selectedPod = ref(null as V1Pod | null)
    const pod = computed(() => podstore.selected);
    watch(pod, () => {
      store = podstore;
      selected.value = pod.value;
      if (pod.value?.item) {
        selectedPod.value = pod.value.item
      }
    });

    const node = computed(() => nodestore.selected);
    watch(node, () => {
      store = nodestore;
      selected.value = node.value;
    });

    const pv = computed(() => pvstore.selected);
    watch(pv, () => {
      store = pvstore;
      selected.value = pv.value;
    });

    const pvc = computed(() => pvcstore.selected);
    watch(pvc, () => {
      store = pvcstore;
      selected.value = pvc.value;
    });

    const sc = computed(() => storageclassstore.selected);
    watch(sc, () => {
      store = storageclassstore;
      selected.value = sc.value;
    });

    const pc = computed(() => priorityclassstore.selected);
    watch(pc, () => {
      store = priorityclassstore;
      selected.value = pc.value;
    });

    const config = computed(() => schedulerconfigurationstore.selected);
    watch(config, () => {
      store = schedulerconfigurationstore;
      selected.value = config.value;
    });

    const namespace = computed(() => namespacestore.selected);
    watch(namespace, () => {
      store = namespacestore;
      selected.value = namespace.value
    })

    watch(selected, (newVal, oldVal) => {
      if (selected.value) {
        if (!oldVal) {
          fetchSelected().then((_) => {
            if (selected.value) {
              editmode.value = selected.value.isNew;

              formData.value = yaml.dump(selected.value.item);
              treeData.value = objectToTreeViewData(selected.value.item);
              drawer.value = true;
            }
          });
        }
      }
    });

    watch(drawer, (newValue, _) => {
      if (!newValue) {
        // reset editmode.
        editmode.value = false;
        if (store) {
          store.resetSelected();
        }
        store = null;
        selected.value = null;
      }
    });

    const fetchSelected = async () => {
      if (store) {
        await store.fetchSelected().catch((e) => setServerErrorMessage(e));
      }
    };

    const setServerErrorMessage = (error: string) => {
      snackbarstore.setServerErrorMessage(error);
    };

    const applyOnClick = () => {
      if (store) {
        const y = <Resource>yaml.load(formData.value);
        store.apply(y).catch((e) => setServerErrorMessage(e));
      }
      drawer.value = false;
    };

    const deleteOnClick = () => {
      if (selectedResourceKind() === "Node") {
        // when the Node is deleted, all Pods on the Node should be deleted as well.
        //@ts-ignore
        if (podstore.pods[selected.value?.item.metadata?.name]) {
          //@ts-ignore
          podstore.pods[selected.value?.item.metadata?.name].forEach((p) => {
            //@ts-ignore
            if (p.spec?.nodeName === selected.value?.item.metadata?.name) {
              podstore
                //@ts-ignore
                .delete(p)
                .catch((e) => setServerErrorMessage(e));
            }
          });
        }
      }
      if (selectedResourceKind() != "SchedulerConfiguration") {
        //@ts-ignore // Only SchedulerConfiguration don't have the metadata field.
        if (selected.value?.item.metadata?.name && store) {
          store
            .delete(
              //@ts-ignore
              selected.value.item
            )
            .catch((e) => setServerErrorMessage(e));
        }
      }
      drawer.value = false;
    };
    const selectedResourceKind = () :String | undefined => {
      return selected.value?.resourceKind
    }

    return {
      drawer,
      editmode,
      selected,
      formData,
      treeData,
      applyOnClick,
      deleteOnClick,
      selectedResourceKind,
      selectedPod,
    };
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/components/ResourceBar/SchedulingResults.vue">
<template>
  <v-expansion-panels accordion multiple>
    <v-expansion-panel v-if="filterTableData().length > 1">
      <v-expansion-panel-header> Filter </v-expansion-panel-header>
      <v-expansion-panel-content>
        <v-data-table
          dense
          :headers="filterTableHeader()"
          :items="filterTableData()"
          item-key="Node"
        >
        </v-data-table>
      </v-expansion-panel-content>
    </v-expansion-panel>
    <v-expansion-panel v-if="scoreTableData().length > 1">
      <v-expansion-panel-header> Score </v-expansion-panel-header>
      <v-expansion-panel-content>
        <v-data-table
          dense
          :headers="scoreTableHeader()"
          :items="scoreTableData()"
          item-key="Node"
        >
        </v-data-table>
      </v-expansion-panel-content>
    </v-expansion-panel>
    <v-expansion-panel v-if="finalscoreTableData().length > 1">
      <v-expansion-panel-header>
        Final Score (Normalized + Applied plugin weight)
      </v-expansion-panel-header>
      <v-expansion-panel-content>
        <v-data-table
          dense
          :headers="finalscoreTableHeader()"
          :items="finalscoreTableData()"
          item-key="Node"
        >
        </v-data-table>
      </v-expansion-panel-content>
    </v-expansion-panel>
  </v-expansion-panels>
</template>
<script lang="ts">
import { V1Pod } from "@kubernetes/client-node";
import { defineComponent } from "@nuxtjs/composition-api";
import { extractTableHeader, schedulingResultToTableData } from "../lib/util";

export default defineComponent({
  props: {
    selected: {
      type: Object,
    },
  },
  
  setup(props) {
    const filterResultAnnotationKey = "kube-scheduler-simulator.sigs.k8s.io/filter-result";
    const scoreResultAnnotationKey = "kube-scheduler-simulator.sigs.k8s.io/score-result";
    const finalScoreResultAnnotationKey =
      "kube-scheduler-simulator.sigs.k8s.io/finalscore-result";

    // scheduling results
    const filterTableHeader = ():Array<{text: string;value: string;}> => {
      const p = props.selected as V1Pod;
      if (p.metadata?.annotations) {
        if (filterResultAnnotationKey in p.metadata.annotations) {
          return extractTableHeader(JSON.parse(
            p.metadata.annotations[filterResultAnnotationKey]
          ));
        }
      }
      return [];
    }

    const filterTableData = ():Array<{ [name: string]: string | number }> => {
      const p = props.selected as V1Pod;
      if (p.metadata?.annotations) {
        if (filterResultAnnotationKey in p.metadata.annotations) {
          return schedulingResultToTableData(JSON.parse(p.metadata.annotations[filterResultAnnotationKey]));
        }
      }
      return [];
    };

    const scoreTableHeader = ():Array<{text: string;value: string;}> => {
      const p = props.selected as V1Pod;
      if (p.metadata?.annotations) {
        if (scoreResultAnnotationKey in p.metadata.annotations) {
          return extractTableHeader(JSON.parse(p.metadata.annotations[scoreResultAnnotationKey]));
        }
      }
      return [];
    };

    const scoreTableData = ():Array<{ [name: string]: string | number }> => {
      const p = props.selected as V1Pod;
      if (p.metadata?.annotations) {
        if (scoreResultAnnotationKey in p.metadata.annotations) {
          return schedulingResultToTableData(JSON.parse(p.metadata.annotations[scoreResultAnnotationKey]));
        }
      }
      return [];
    };

    const finalscoreTableHeader = ():Array<{text: string;value: string;}> => {
      const p = props.selected as V1Pod;
      if (p.metadata?.annotations) {
        if (finalScoreResultAnnotationKey in p.metadata.annotations) {
          return extractTableHeader(JSON.parse(p.metadata.annotations[finalScoreResultAnnotationKey]));
        }
      }
      return [];
    };
    const finalscoreTableData = ():Array<{ [name: string]: string | number }> => {
      const p = props.selected as V1Pod;
      if (p.metadata?.annotations) {
        if (finalScoreResultAnnotationKey in p.metadata.annotations) {
          return schedulingResultToTableData(JSON.parse(p.metadata.annotations[finalScoreResultAnnotationKey]));
        }
      }
      return [];
    };

    return {
      filterTableHeader,
      filterTableData,
      scoreTableHeader,
      scoreTableData,
      finalscoreTableHeader,
      finalscoreTableData,
    };
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/components/ResourceBar/YamlEditor.vue">
<template>
  <monaco-editor
    v-model="formData"
    class="editor mt-1"
    language="yaml"
    @change="onChange"
  ></monaco-editor>
</template>

<script lang="ts">
import { defineComponent, ref, watch } from "@nuxtjs/composition-api";
//@ts-ignore // it is ok to ignore.
import MonacoEditor from "vue-monaco";

export default defineComponent({
  components: {
    MonacoEditor,
  },
  props: {
    value: {
      type: String,
      required: true,
    },
  },
  emits: ["input"],
  setup(props, { emit }) {
    const formData = ref(props.value);

    watch(props, (newvalue, _) => {
      if (newvalue.value) {
        formData.value = newvalue.value;
      }
    });

    const onChange = () => {
      emit("input", formData.value);
    };

    return {
      formData,
      onChange,
    };
  },
});
</script>

<style>
.editor {
  width: auto;
  height: 100%;
}
</style>
</file>

<file path="kube-scheduler-simulator/web/components/ResourceViews/DataTables/DataTable.vue">
<template>
  <v-row>
    <v-col>
      <v-card class="ma-2" outlined>
        <v-card-title class="mb-1">
          <v-row
            ><v-col> {{ title }} <v-spacer></v-spacer> </v-col
            ><v-col>
              <v-text-field
                v-model="search"
                append-icon="mdi-magnify"
                label="Search"
                single-line
                hide-details
              ></v-text-field></v-col></v-row></v-card-title
        ><v-data-table
          :headers="headers"
          :items="items"
          :items-per-page="5"
          :search="search"
          multi-sort
          class="row-pointer"
          @click:row="onClick"
        ></v-data-table>
      </v-card>
    </v-col>
  </v-row>
</template>

<script lang="ts">
import { defineComponent } from "@nuxtjs/composition-api";

export default defineComponent({
  props: {
    headers: {
      type: Array,
      default: () => [],
    },
    title: {
      type: String,
      default: "",
    },
    items: {
      type: Array,
      default: () => [],
    },
    onClick: {
      type: Function,
      default: () => {},
    },
  },
  setup() {
    const search = "";

    return {
      search,
    };
  },
});
</script>

<style>
.row-pointer > .v-data-table__wrapper > table > tbody > tr:hover {
  cursor: pointer;
}
</style>
</file>

<file path="kube-scheduler-simulator/web/components/ResourceViews/DataTables/NamespaceDataTable.vue">
<template>
  <DataTable
    :title="`Namespaces`"
    :headers="headers"
    :items="namespaces"
    :on-click="onClick"
  />
</template>

<script lang="ts">
import { V1Namespace } from "@kubernetes/client-node";
import { computed, inject, defineComponent } from "@nuxtjs/composition-api";
import DataTable from "./DataTable.vue";
import NamespaceStoreKey from "../../StoreKey/NamespaceStoreKey";

export default defineComponent({
  components: {
    DataTable,
  },
  setup() {
    const store = inject(NamespaceStoreKey);
    if (!store) {
      throw new Error(`${NamespaceStoreKey.description} is not provided`)
    }
    const onClick = (ns: V1Namespace) => {
      store.select(ns, false)
    };

    const namespaces = computed(() => store.namespaces);
    const search = "";
    const headers = [
      {
        text: "Name",
        value: "metadata.name",
        sortable: true,
      },
      {
        text: "CreationTime",
        value: "metadata.creationTimestamp",
        sortable: true,
      },
      {
        text: "UpdateTime",
        value: "metadata.managedFields[0].time",
        sortable: true,
      },
    ];
    return {
      namespaces,
      onClick,
      search,
      headers,
    };
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/components/ResourceViews/DataTables/NodeDataTable.vue">
<template>
  <DataTable
    :title="`Nodes`"
    :headers="headers"
    :items="nodes"
    :on-click="onClick"
  />
</template>

<script lang="ts">
import { computed, inject, defineComponent } from "@nuxtjs/composition-api";
import DataTable from "./DataTable.vue";
import NodeStoreKey from "../../StoreKey/NodeStoreKey";
import { V1Node } from "@kubernetes/client-node";
import {} from "../../lib/util";

export default defineComponent({
  components: {
    DataTable,
  },
  setup() {
    const nstore = inject(NodeStoreKey);
    if (!nstore) {
      throw new Error(`${NodeStoreKey.description} is not provided`);
    }

    const nodes = computed(() => nstore.nodes);
    const onClick = (node: V1Node) => {
      nstore.select(node, false);
    };
    const search = "";
    const headers = [
      {
        text: "Name",
        value: "metadata.name",
        sortable: true,
      },
      { text: "CPU", value: "status.capacity.cpu", sortable: true },
      { text: "Memory", value: "status.capacity.memory", sortable: true },
      { text: "Pods", value: "status.capacity.pods", sortable: true },
      {
        text: "CreationTime",
        value: "metadata.creationTimestamp",
        sortable: true,
      },
      {
        text: "UpdateTime",
        value: "metadata.managedFields[0].time",
        sortable: true,
      },
    ];
    return {
      nodes,
      search,
      headers,
      onClick,
    };
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/components/ResourceViews/DataTables/PodDataTable.vue">
<template>
  <DataTable
    :title="`Pods`"
    :headers="headers"
    :items="pods"
    :on-click="onClick"
  />
</template>

<script lang="ts">
import { V1Pod } from "@kubernetes/client-node";
import { computed, inject, defineComponent } from "@nuxtjs/composition-api";
import DataTable from "./DataTable.vue";
import {} from "../../lib/util";
import PodStoreKey from "../../StoreKey/PodStoreKey";

export default defineComponent({
  components: {
    DataTable,
  },
  setup() {
    const store = inject(PodStoreKey);
    if (!store) {
      throw new Error(`${PodStoreKey.description} is not provided`);
    }

    const onClick = (pod: V1Pod) => {
      store.select(pod, false);
    };

    const pods = computed(() => {
      return Array<V1Pod>().concat(
        ...Object.values(store.pods).map((p) => {
          return p;
        })
      );
    });
    const search = "";
    const headers = [
      {
        text: "Name",
        value: "metadata.name",
        sortable: true,
      },
      { text: "Namespace", value: "metadata.namespace", sortable: true },
      { text: "Node", value: "spec.nodeName", sortable: true },
      {
        text: "Conditions",
        value: "status.conditions[0].type",
        sortable: true,
      },
      {
        text: "CreationTime",
        value: "metadata.creationTimestamp",
        sortable: true,
      },
      {
        text: "UpdateTime",
        value: "metadata.managedFields[0].time",
        sortable: true,
      },
    ];
    return {
      pods,
      search,
      onClick,
      headers,
    };
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/components/ResourceViews/DataTables/PriorityClassDataTable.vue">
<template>
  <DataTable
    :title="`PriorityClasses`"
    :headers="headers"
    :items="priorityclasses"
    :on-click="onClick"
  />
</template>

<script lang="ts">
import { V1PriorityClass } from "@kubernetes/client-node";
import { computed, inject, defineComponent } from "@nuxtjs/composition-api";
import DataTable from "./DataTable.vue";
import {} from "../../lib/util";
import PriorityClassStoreKey from "../../StoreKey/PriorityClassStoreKey";

export default defineComponent({
  components: {
    DataTable,
  },
  setup() {
    const store = inject(PriorityClassStoreKey);
    if (!store) {
      throw new Error(`${PriorityClassStoreKey.description} is not provided`);
    }

    const onClick = (priorityclass: V1PriorityClass) => {
      store.select(priorityclass, false);
    };

    const priorityclasses = computed(() => store.priorityclasses);
    const search = "";
    const headers = [
      {
        text: "Name",
        value: "metadata.name",
        sortable: true,
      },
      { text: "Value", value: "value", sortable: true },
      { text: "GlobalDefault", value: "globalDefault", sortable: true },
      { text: "PreemptionPolicy", value: "preemptionPolicy", sortable: true },
      {
        text: "CreationTime",
        value: "metadata.creationTimestamp",
        sortable: true,
      },
      {
        text: "UpdateTime",
        value: "metadata.managedFields[0].time",
        sortable: true,
      },
    ];
    return {
      priorityclasses,
      onClick,
      search,
      headers,
    };
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/components/ResourceViews/DataTables/PVCDataTable.vue">
<template>
  <DataTable
    :title="`PersistentVolumeClaims`"
    :headers="headers"
    :items="pvcs"
    :on-click="onClick"
  />
</template>

<script lang="ts">
import { V1PersistentVolumeClaim } from "@kubernetes/client-node";
import { computed, inject, defineComponent } from "@nuxtjs/composition-api";
import DataTable from "./DataTable.vue";
import {} from "../../lib/util";
import PersistentVolumeClaimStoreKey from "../../StoreKey/PVCStoreKey";
export default defineComponent({
  components: {
    DataTable,
  },
  setup() {
    const store = inject(PersistentVolumeClaimStoreKey);
    if (!store) {
      throw new Error(`${PersistentVolumeClaimStoreKey.description} is not provided`);
    }

    const onClick = (pvc: V1PersistentVolumeClaim) => {
      store.select(pvc, false);
    };

    const pvcs = computed(() => store.pvcs);
    const search = "";
    const headers = [
      {
        text: "Name",
        value: "metadata.name",
        sortable: true,
      },
      { text: "Namespace", value: "metadata.namespace", sortable: true },
      { text: "VolumeName", value: "spec.volumeName", sortable: true },
      { text: "VolumeMode", value: "spec.volumeMode", sortable: true },
      {
        text: "Capacity",
        value: "spec.resources.requests.storage",
        sortable: true,
      },
      {
        text: "CreationTime",
        value: "metadata.creationTimestamp",
        sortable: true,
      },
      {
        text: "UpdateTime",
        value: "metadata.managedFields[0].time",
        sortable: true,
      },
    ];
    return {
      pvcs,
      search,
      headers,
      onClick,
    };
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/components/ResourceViews/DataTables/PVDataTable.vue">
<template>
  <DataTable
    :title="`PersistentVolumes`"
    :headers="headers"
    :items="pvs"
    :on-click="onClick"
  />
</template>

<script lang="ts">
import { V1PersistentVolume } from "@kubernetes/client-node";
import { computed, inject, defineComponent } from "@nuxtjs/composition-api";
import DataTable from "./DataTable.vue";
import PersistentVolumeStoreKey from "../../StoreKey/PVStoreKey";
import {} from "../../lib/util";

export default defineComponent({
  components: {
    DataTable,
  },
  setup() {
    const store = inject(PersistentVolumeStoreKey);
    if (!store) {
      throw new Error(`${PersistentVolumeStoreKey.description} is not provided`);
    }

    const onClick = (pv: V1PersistentVolume) => {
      store.select(pv, false);
    };

    const pvs = computed(() => store.pvs);
    const search = "";
    const headers = [
      {
        text: "Name",
        value: "metadata.name",
        sortable: true,
      },
      { text: "Status", value: "status.phase", sortable: true },
      { text: "VolumeMode", value: "spec.volumeMode", sortable: true },
      {
        text: "Capacity",
        value: "spec.resources.requests.storage",
        sortable: true,
      },
      {
        text: "CreationTime",
        value: "metadata.creationTimestamp",
        sortable: true,
      },
      {
        text: "UpdateTime",
        value: "metadata.managedFields[0].time",
        sortable: true,
      },
    ];
    return {
      pvs,
      search,
      headers,
      onClick,
    };
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/components/ResourceViews/DataTables/ResourcesDataTable.vue">
<template>
  <div>
    <PodDataTable />
    <NodeDataTable />
    <PVDataTable />
    <PVCDataTable />
    <StorageClassDataTable />
    <PriorityClassDataTable />
    <NamespaceDataTable />
  </div>
</template>

<script lang="ts">
import { defineComponent } from "@nuxtjs/composition-api";
import PodDataTable from "./PodDataTable.vue";
import NodeDataTable from "./NodeDataTable.vue";
import PVDataTable from "./PVDataTable.vue";
import PVCDataTable from "./PVCDataTable.vue";
import StorageClassDataTable from "./StorageClassDataTable.vue";
import PriorityClassDataTable from "./PriorityClassDataTable.vue";
import NamespaceDataTable from "./NamespaceDataTable.vue";

export default defineComponent({
  components: {
    PodDataTable,
    NodeDataTable,
    PVDataTable,
    PVCDataTable,
    StorageClassDataTable,
    PriorityClassDataTable,
    NamespaceDataTable,
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/components/ResourceViews/DataTables/StorageClassDataTable.vue">
<template>
  <DataTable
    :title="`StorageClasses`"
    :headers="headers"
    :items="storageclasses"
    :on-click="onClick"
  />
</template>

<script lang="ts">
import { V1StorageClass } from "@kubernetes/client-node";
import { computed, inject, defineComponent } from "@nuxtjs/composition-api";
import DataTable from "./DataTable.vue";
import StorageClassStoreKey from "../../StoreKey/StorageClassStoreKey";
import {} from "../../lib/util";

export default defineComponent({
  components: {
    DataTable,
  },
  setup() {
    const store = inject(StorageClassStoreKey);
    if (!store) {
      throw new Error(`${StorageClassStoreKey.description} is not provided`);
    }

    const onClick = (storageclass: V1StorageClass) => {
      store.select(storageclass, false);
    };
    const storageclasses = computed(() => store.storageclasses);
    const search = "";
    const headers = [
      {
        text: "Name",
        value: "metadata.name",
        sortable: true,
      },
      { text: "Provisioner", value: "provisioner", sortable: true },
      { text: "Parameters", value: "parameters", sortable: true },
      { text: "Reclaim-Policy", value: "reclaimPolicy", sortable: true },
      {
        text: "VolumeBindingMode",
        value: "volumeBindingMode",
        sortable: true,
      },
      {
        text: "CreationTime",
        value: "metadata.creationTimestamp",
        sortable: true,
      },
      {
        text: "UpdateTime",
        value: "metadata.managedFields[0].time",
        sortable: true,
      },
    ];
    return {
      storageclasses,
      headers,
      search,
      onClick,
    };
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/components/ResourceViews/Lists/NamespaceList.vue">
<template>
  <v-row v-if="namespaces.length !== 0" no-gutters>
    <v-col>
      <v-card class="ma-2" outlined>
        <v-card-title class="mb-1"> Namespaces </v-card-title>
        <v-card-actions>
          <v-chip
            v-for="(p, i) in namespaces"
            :key="i"
            class="ma-2"
            color="primary"
            outlined
            large
            label
            @click.stop="onClick(p)"
          >
            {{ p.metadata.name }}
          </v-chip>
        </v-card-actions>
      </v-card>
    </v-col>
  </v-row>
</template>

<script lang="ts">
import { V1Namespace } from "@kubernetes/client-node";
import { computed, inject, defineComponent } from "@nuxtjs/composition-api";
import NamespaceStoreKey from "../../StoreKey/NamespaceStoreKey";
export default defineComponent({
  setup() {
    const store = inject(NamespaceStoreKey);
    if (!store) {
      throw new Error(`${NamespaceStoreKey.description} is not provided`);
    }

    const onClick = (ns: V1Namespace) => {
      store.select(ns, false)
    };
    const namespaces = computed(() => store.namespaces);
    return {
      namespaces,
      onClick,
    }
  }
})
</script>
</file>

<file path="kube-scheduler-simulator/web/components/ResourceViews/Lists/NodeList.vue">
<template>
  <v-card v-if="nodes.length !== 0" class="ma-2" outlined>
    <v-card-title class="mb-1"> Nodes </v-card-title>
    <v-container>
      <v-row no-gutters>
        <v-col v-for="(n, i) in nodes" :key="i" tile cols="auto">
          <v-card class="ma-2" outlined @click="onClick(n)">
            <v-card-title>
              <img
                src="/node.svg"
                height="40"
                alt="p.metadata.name"
                class="mr-2"
              />
              {{ n.metadata.name }}
            </v-card-title>
            <PodList :node-name="n.metadata.name" />
          </v-card>
        </v-col>
      </v-row>
    </v-container>
  </v-card>
</template>

<script lang="ts">
import { computed, inject, defineComponent } from "@nuxtjs/composition-api";
import NodeStoreKey from "../../StoreKey/NodeStoreKey";
import PodList from "./PodList.vue";
import { V1Node } from "@kubernetes/client-node";
import {} from "../../lib/util";

export default defineComponent({
  components: { PodList },
  setup() {
    const nstore = inject(NodeStoreKey);
    if (!nstore) {
      throw new Error(`${NodeStoreKey.description} is not provided`);
    }

    const nodes = computed(() => nstore.nodes);

    const onClick = (node: V1Node) => {
      nstore.select(node, false);
    };

    return {
      nodes,
      onClick,
    };
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/components/ResourceViews/Lists/PodList.vue">
<template>
  <v-card-actions>
    <v-chip
      v-for="(p, i) in pods"
      :key="i"
      class="ma-2"
      color="primary"
      outlined
      large
      label
      @click.stop="onClick(p)"
    >
      <img src="/pod.svg" height="40" alt="p.metadata.name" class="mr-2" />
      {{ p.metadata.name }}
    </v-chip>
  </v-card-actions>
</template>

<script lang="ts">
import { V1Pod } from "@kubernetes/client-node";
import { computed, inject, defineComponent } from "@nuxtjs/composition-api";
import {} from "../../lib/util";
import PodStoreKey from "../../StoreKey/PodStoreKey";
export default defineComponent({
  props: {
    nodeName: {
      type: String,
      required: true,
    },
  },
  setup(props) {
    const store = inject(PodStoreKey);
    if (!store) {
      throw new Error(`${PodStoreKey.description} is not provided`);
    }

    const onClick = (pod: V1Pod) => {
      store.select(pod, false);
    };

    const pods: any = computed(function () {
      return store.pods[props.nodeName];
    });
    return {
      pods,
      onClick,
    };
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/components/ResourceViews/Lists/PriorityClassList.vue">
<template>
  <v-row v-if="priorityclasses.length !== 0" no-gutters>
    <v-col>
      <v-card class="ma-2" outlined>
        <v-card-title class="mb-1"> PriorityClasses </v-card-title>
        <v-card-actions>
          <v-chip
            v-for="(p, i) in priorityclasses"
            :key="i"
            class="ma-2"
            color="primary"
            outlined
            large
            label
            @click.stop="onClick(p)"
          >
            {{ p.metadata.name }}
          </v-chip>
        </v-card-actions>
      </v-card>
    </v-col>
  </v-row>
</template>

<script lang="ts">
import { V1PriorityClass } from "@kubernetes/client-node";
import { computed, inject, defineComponent } from "@nuxtjs/composition-api";
import {} from "../../lib/util";
import PriorityClassStoreKey from "../../StoreKey/PriorityClassStoreKey";
export default defineComponent({
  setup() {
    const store = inject(PriorityClassStoreKey);
    if (!store) {
      throw new Error(`${PriorityClassStoreKey.description} is not provided`);
    }

    const onClick = (priorityclass: V1PriorityClass) => {
      store.select(priorityclass, false);
    };

    const priorityclasses = computed(() => store.priorityclasses);
    return {
      priorityclasses,
      onClick,
    };
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/components/ResourceViews/Lists/PVCList.vue">
<template>
  <v-row v-if="pvcs.length !== 0" no-gutters>
    <v-col>
      <v-card class="ma-2" outlined>
        <v-card-title class="mb-1"> PersistentVolumeClaims </v-card-title>
        <v-card-actions>
          <v-chip
            v-for="(p, i) in pvcs"
            :key="i"
            class="ma-2"
            color="primary"
            outlined
            large
            label
            @click.stop="onClick(p)"
          >
            <img
              src="/pvc.svg"
              height="40"
              alt="p.metadata.name"
              class="mr-2"
            />
            {{ p.metadata.name }}
          </v-chip>
        </v-card-actions>
      </v-card>
    </v-col>
  </v-row>
</template>

<script lang="ts">
import { V1PersistentVolumeClaim } from "@kubernetes/client-node";
import { computed, inject, defineComponent } from "@nuxtjs/composition-api";
import {} from "../../lib/util";
import PersistentVolumeClaimStoreKey from "../../StoreKey/PVCStoreKey";
export default defineComponent({
  setup() {
    const store = inject(PersistentVolumeClaimStoreKey);
    if (!store) {
      throw new Error(`${PersistentVolumeClaimStoreKey.description} is not provided`);
    }

    const onClick = (pvc: V1PersistentVolumeClaim) => {
      store.select(pvc, false);
    };

    const pvcs = computed(() => store.pvcs);
    return {
      pvcs,
      onClick,
    };
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/components/ResourceViews/Lists/PVList.vue">
<template>
  <v-row v-if="pvs.length !== 0" no-gutters>
    <v-col>
      <v-card class="ma-2" outlined>
        <v-card-title class="mb-1"> PersistentVolumes </v-card-title>
        <v-card-actions>
          <v-chip
            v-for="(p, i) in pvs"
            :key="i"
            class="ma-2"
            color="primary"
            outlined
            large
            label
            @click.stop="onClick(p)"
          >
            <img src="/pv.svg" height="40" alt="p.metadata.name" class="mr-2" />
            {{ p.metadata.name }}
          </v-chip>
        </v-card-actions>
      </v-card>
    </v-col>
  </v-row>
</template>

<script lang="ts">
import { V1PersistentVolume } from "@kubernetes/client-node";
import { computed, inject, defineComponent } from "@nuxtjs/composition-api";
import {} from "../../lib/util";
import PersistentVolumeStoreKey from "../../StoreKey/PVStoreKey";
export default defineComponent({
  setup() {
    const store = inject(PersistentVolumeStoreKey);
    if (!store) {
      throw new Error(`${PersistentVolumeStoreKey.description} is not provided`);
    }

    const onClick = (pv: V1PersistentVolume) => {
      store.select(pv, false);
    };

    const pvs = computed(() => store.pvs);
    return {
      pvs,
      onClick,
    };
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/components/ResourceViews/Lists/ResourcesList.vue">
<template>
  <div>
    <NodeList />
    <UnscheduledPodList />
    <PVList />
    <PVCList />
    <StorageClassList />
    <PriorityClassList />
    <NamespaceList />
  </div>
</template>

<script lang="ts">
import { defineComponent } from "@nuxtjs/composition-api";
import NodeList from "./NodeList.vue";
import UnscheduledPodList from "./UnscheduledPodList.vue";
import PVList from "./PVList.vue";
import PVCList from "./PVCList.vue";
import StorageClassList from "./StorageClassList.vue";
import PriorityClassList from "./PriorityClassList.vue";
import NamespaceList from "./NamespaceList.vue";

export default defineComponent({
  components: {
    NodeList,
    UnscheduledPodList,
    PVList,
    PVCList,
    StorageClassList,
    PriorityClassList,
    NamespaceList,
},
});
</script>
</file>

<file path="kube-scheduler-simulator/web/components/ResourceViews/Lists/StorageClassList.vue">
<template>
  <v-row v-if="storageclasses.length !== 0" no-gutters>
    <v-col>
      <v-card class="ma-2" outlined>
        <v-card-title class="mb-1"> StorageClasses </v-card-title>
        <v-card-actions>
          <v-chip
            v-for="(p, i) in storageclasses"
            :key="i"
            class="ma-2"
            color="primary"
            outlined
            large
            label
            @click.stop="onClick(p)"
          >
            <img src="/sc.svg" height="40" alt="p.metadata.name" class="mr-2" />
            {{ p.metadata.name }}
          </v-chip>
        </v-card-actions>
      </v-card>
    </v-col>
  </v-row>
</template>

<script lang="ts">
import { V1StorageClass } from "@kubernetes/client-node";
import { computed, inject, defineComponent } from "@nuxtjs/composition-api";
import {} from "../../lib/util";
import StorageClassStoreKey from "../../StoreKey/StorageClassStoreKey";
export default defineComponent({
  setup() {
    const store = inject(StorageClassStoreKey);
    if (!store) {
      throw new Error(`${StorageClassStoreKey.description} is not provided`);
    }
    const onClick = (storageclass: V1StorageClass) => {
      store.select(storageclass, false);
    };
    const storageclasses = computed(() => store.storageclasses);
    return {
      storageclasses,
      onClick,
    };
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/components/ResourceViews/Lists/UnscheduledPodList.vue">
<template>
  <v-card v-if="pods && pods.length !== 0" class="ma-2" outlined>
    <v-card-title class="mb-1"> Unscheduled Pods </v-card-title>
    <PodList node-name="unscheduled" />
  </v-card>
</template>

<script lang="ts">
import { computed, inject, defineComponent } from "@nuxtjs/composition-api";
import {} from "../../lib/util";
import PodStoreKey from "../../StoreKey/PodStoreKey";
import PodList from "./PodList.vue";

export default defineComponent({
  components: { PodList },
  setup() {
    const store = inject(PodStoreKey);
    if (!store) {
      throw new Error(`${PodStoreKey.description} is not provided`);
    }

    const pods: any = computed(function () {
      return store.pods["unscheduled"];
    });
    return {
      pods,
    };
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/components/ResourceViews/ResourcesViewPanel.vue">
<template>
  <div>
    <div v-if="IsAlphaTableViewEnabled()">
      <ResourcesDataTable />
    </div>
    <div v-else>
      <ResourcesList />
    </div>
  </div>
</template>

<script lang="ts">
import { defineComponent } from "@nuxtjs/composition-api";
import ResourcesList from "./Lists/ResourcesList.vue";
import ResourcesDataTable from "./DataTables/ResourcesDataTable.vue";

export default defineComponent({
  components: {
    ResourcesList,
    ResourcesDataTable,
  },
  setup() {
    const IsAlphaTableViewEnabled = () => {
      return process.env.ALPHA_TABLE_VIEWS == "1";
    };

    return {
      IsAlphaTableViewEnabled,
    };
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/components/StoreKey/NamespaceStoreKey.ts">
import { InjectionKey } from "@nuxtjs/composition-api";
import { NamespaceStore } from "../../store/namespace";

const NamespaceStoreKey: InjectionKey<NamespaceStore> = Symbol("NamespaceStore");
export default NamespaceStoreKey;
</file>

<file path="kube-scheduler-simulator/web/components/StoreKey/NodeStoreKey.ts">
import { InjectionKey } from "@nuxtjs/composition-api";
import { NodeStore } from "../../store/node";

const NodeStoreKey: InjectionKey<NodeStore> = Symbol("NodeStore");
export default NodeStoreKey;
</file>

<file path="kube-scheduler-simulator/web/components/StoreKey/PodStoreKey.ts">
import { InjectionKey } from "@nuxtjs/composition-api";
import { PodStore } from "../../store/pod";

const PodStoreKey: InjectionKey<PodStore> = Symbol("PodStore");
export default PodStoreKey;
</file>

<file path="kube-scheduler-simulator/web/components/StoreKey/PriorityClassStoreKey.ts">
import { InjectionKey } from "@nuxtjs/composition-api";
import { PriorityClassStore } from "~/store/priorityclass";

const PriorityClassStoreKey: InjectionKey<PriorityClassStore> =
  Symbol("PriorityClassStore");
export default PriorityClassStoreKey;
</file>

<file path="kube-scheduler-simulator/web/components/StoreKey/PVCStoreKey.ts">
import { InjectionKey } from "@nuxtjs/composition-api";
import { PersistentVolumeClaimStore } from "../../store/pvc";

const PersistentVolumeClaimStoreKey: InjectionKey<PersistentVolumeClaimStore> =
  Symbol("PersistentVolumeClaimStore");
export default PersistentVolumeClaimStoreKey;
</file>

<file path="kube-scheduler-simulator/web/components/StoreKey/PVStoreKey.ts">
import { InjectionKey } from "@nuxtjs/composition-api";
import { PersistentVolumeStore } from "../../store/pv";

const PersistentVolumeStoreKey: InjectionKey<PersistentVolumeStore> =
  Symbol("pvStore");
export default PersistentVolumeStoreKey;
</file>

<file path="kube-scheduler-simulator/web/components/StoreKey/SchedulerConfigurationStoreKey.ts">
import { InjectionKey } from "@nuxtjs/composition-api";
import { SchedulerConfigurationStore } from "~/store/schedulerconfiguration";

const SchedulerConfigurationStoreKey: InjectionKey<SchedulerConfigurationStore> =
  Symbol("SchedulerConfigurationStore");
export default SchedulerConfigurationStoreKey;
</file>

<file path="kube-scheduler-simulator/web/components/StoreKey/SnackBarStoreKey.ts">
import { InjectionKey } from "@nuxtjs/composition-api";
import { SnackBarStore } from "../../store/snackbar";

const SnackBarStoreKey: InjectionKey<SnackBarStore> = Symbol("snackBarStore");
export default SnackBarStoreKey;
</file>

<file path="kube-scheduler-simulator/web/components/StoreKey/StorageClassStoreKey.ts">
import { InjectionKey } from "@nuxtjs/composition-api";
import { StorageClassStore } from "../../store/storageclass";

const StorageClassStoreKey: InjectionKey<StorageClassStore> =
  Symbol("StorageClassStore");
export default StorageClassStoreKey;
</file>

<file path="kube-scheduler-simulator/web/components/StoreProvider/NamespaceStoreProvider.vue">
<template>
  <div>
    <slot />
  </div>
</template>

<script lang="ts">
import { provide, defineComponent } from "@nuxtjs/composition-api";
import namespaceStore from "../../store/namespace";
import NamespaceStoreKey from "../StoreKey/NamespaceStoreKey";

export default defineComponent({
  setup() {
    provide(NamespaceStoreKey, namespaceStore());
    return {};
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/components/StoreProvider/NodeStoreProvider.vue">
<template>
  <div>
    <slot />
  </div>
</template>

<script lang="ts">
import { provide, defineComponent } from "@nuxtjs/composition-api";
import nodeStore from "../../store/node";
import NodeStoreKey from "../StoreKey/NodeStoreKey";

export default defineComponent({
  setup() {
    provide(NodeStoreKey, nodeStore());
    return {};
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/components/StoreProvider/PodStoreProvider.vue">
<template>
  <div>
    <slot />
  </div>
</template>

<script lang="ts">
import { defineComponent, provide } from "@nuxtjs/composition-api";
import podStore from "../../store/pod";
import PodStoreKey from "../StoreKey/PodStoreKey";

export default defineComponent({
  setup() {
    provide(PodStoreKey, podStore());
    return {};
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/components/StoreProvider/PriorityClassStoreProvider.vue">
<template>
  <div>
    <slot />
  </div>
</template>

<script lang="ts">
import { defineComponent, provide } from "@nuxtjs/composition-api";
import priorityClassStore from "../../store/priorityclass";
import PriorityClassStoreKey from "../StoreKey/PriorityClassStoreKey";

export default defineComponent({
  setup() {
    provide(PriorityClassStoreKey, priorityClassStore());
    return {};
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/components/StoreProvider/PVCStoreProvider.vue">
<template>
  <div>
    <slot />
  </div>
</template>

<script lang="ts">
import { defineComponent, provide } from "@nuxtjs/composition-api";
import PersistentVolumeClaimStore from "../../store/pvc";
import PersistentVolumeClaimStoreKey from "../StoreKey/PVCStoreKey";

export default defineComponent({
  setup() {
    provide(PersistentVolumeClaimStoreKey, PersistentVolumeClaimStore());
    return {};
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/components/StoreProvider/PVStoreProvider.vue">
<template>
  <div>
    <slot />
  </div>
</template>

<script lang="ts">
import { defineComponent, provide } from "@nuxtjs/composition-api";
import PersistentVolumeStore from "../../store/pv";
import PersistentVolumeStoreKey from "../StoreKey/PVStoreKey";

export default defineComponent({
  setup() {
    provide(PersistentVolumeStoreKey, PersistentVolumeStore());
    return {};
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/components/StoreProvider/SchedulerConfigurationStoreProvider.vue">
<template>
  <div>
    <slot />
  </div>
</template>

<script lang="ts">
import { provide, defineComponent } from "@nuxtjs/composition-api";
import schedulerconfigurationStore from "../../store/schedulerconfiguration";
import SchedulerConfigurationStoreKey from "../StoreKey/SchedulerConfigurationStoreKey";

export default defineComponent({
  setup() {
    provide(SchedulerConfigurationStoreKey, schedulerconfigurationStore());
    return {};
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/components/StoreProvider/SnackbarStoreProvider.vue">
<template>
  <div>
    <slot />
  </div>
</template>

<script lang="ts">
import { defineComponent, provide } from "@nuxtjs/composition-api";
import snackbarStore from "../../store/snackbar";
import SnackBarStoreKey from "../StoreKey/SnackBarStoreKey";

export default defineComponent({
  setup() {
    provide(SnackBarStoreKey, snackbarStore());
    return {};
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/components/StoreProvider/StorageClassStoreProvider.vue">
<template>
  <div>
    <slot />
  </div>
</template>

<script lang="ts">
import { defineComponent, provide } from "@nuxtjs/composition-api";
import storageClassStore from "../../store/storageclass";
import StorageClassStoreKey from "../StoreKey/StorageClassStoreKey";

export default defineComponent({
  setup() {
    provide(StorageClassStoreKey, storageClassStore());
    return {};
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/components/TopBar/ExportButton.vue">
<template>
  <v-dialog v-model="data.dialog" width="500">
    <template #activator="{ on }">
      <v-btn color="ma-2" :retain-focus-on-click="false" v-on="on">
        Export
      </v-btn>
    </template>

    <v-card>
      <v-card-title class="text-h5 grey lighten-2"> Export </v-card-title>

      <v-card-text>
        Export the current created resources and scheduler configuration.
      </v-card-text>

      <v-divider></v-divider>

      <v-card-actions>
        <v-spacer></v-spacer>
        <v-btn color="green darken-1" text @click="data.dialog = false">
          Cancel
        </v-btn>
        <v-btn color="green darken-1" text @click="ExportScheduler()">
          Export
        </v-btn>
      </v-card-actions>
    </v-card>
  </v-dialog>
</template>

<script lang="ts">
import { saveAs } from "file-saver";
import { defineComponent, inject, reactive } from "@nuxtjs/composition-api";
import SnackBarStoreKey from "../StoreKey/SnackBarStoreKey";
import yaml from "js-yaml";
import { ExportAPIKey } from "~/api/APIProviderKeys";

export default defineComponent({
  setup() {
    const data = reactive({
      dialog: false,
    });

    const exportAPI = inject(ExportAPIKey);
    if (!exportAPI) {
      throw new Error(`${ExportAPIKey.description} is not provided`);
    }

    const snackbarstore = inject(SnackBarStoreKey);
    if (!snackbarstore) {
      throw new Error(`${SnackBarStoreKey.description} is not provided`);
    }
    const setServerErrorMessage = (error: string) => {
      snackbarstore.setServerErrorMessage(error);
    };

    const ExportScheduler = async () => {
      try {
        const c = await exportAPI.exportScheduler();
        if (c) {
          const blob = new Blob([yaml.dump(c)], {
            type: "application/yaml",
          });
          saveAs(blob, "export.yml");
          data.dialog = false;
        }
      } catch (e: any) {
        setServerErrorMessage(e);
      }
    };
    return {
      data,
      ExportScheduler,
    };
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/components/TopBar/ImportButton.vue">
<template>
  <v-dialog v-model="data.dialog" width="500">
    <template #activator="{ on }">
      <v-btn color="ma-2" :retain-focus-on-click="false" v-on="on">
        Import
      </v-btn>
    </template>

    <v-card>
      <v-card-title class="text-h5 grey lighten-2"> Import </v-card-title>

      <v-card-text>
        Import resources and scheduler configuration.<br />
        Note that all current created resources will be deleted and then
        resources are imported.
        <form>
          <input
            v-if="data.dialog"
            type="file"
            accept=".yml"
            @change="readfile"
          />
        </form>
      </v-card-text>

      <v-divider></v-divider>

      <v-card-actions>
        <v-spacer></v-spacer>
        <v-btn color="green darken-1" text @click="data.dialog = false">
          Cancel
        </v-btn>
        <v-btn
          color="green darken-1"
          text
          :disabled="data.isImportButtonDisabled"
          @click="ImportScheduler()"
        >
          Import
        </v-btn>
      </v-card-actions>
    </v-card>
  </v-dialog>
</template>

<script lang="ts">
import {
  defineComponent,
  inject,
  reactive,
  watch,
} from "@nuxtjs/composition-api";
import { ResourcesForImport } from "~/api/v1/export";
import yaml from "js-yaml";
import SnackBarStoreKey from "../StoreKey/SnackBarStoreKey";
import { ExportAPIKey } from "~/api/APIProviderKeys";

interface SelectedItem {
  dialog: boolean;
  filedata: ResourcesForImport;
  isImportButtonDisabled: boolean;
}

export default defineComponent({
  setup() {
    const data = reactive({
      dialog: false,
      isImportButtonDisabled: true,
    } as SelectedItem);

    const exportAPI = inject(ExportAPIKey);
    if (!exportAPI) {
      throw new Error(`${ExportAPIKey.description} is not provided`);
    }

    const snackbarstore = inject(SnackBarStoreKey);
    if (!snackbarstore) {
      throw new Error(`${SnackBarStoreKey.description} is not provided`);
    }
    const setServerErrorMessage = (error: string) => {
      snackbarstore.setServerErrorMessage(error);
    };

    watch(data, (newValue, _) => {
      if (!newValue.dialog) {
        // reset filedata
        data.filedata = {} as ResourcesForImport;
        data.isImportButtonDisabled = true;
      }
    });

    const ImportScheduler = async () => {
      exportAPI
        .importScheduler(data.filedata as ResourcesForImport)
        .catch((e) => setServerErrorMessage(e))
        .finally(() => {
          data.dialog = false;
        });
    };
    function readfile(e: { target: { files: FileList | null } }) {
      if (e.target.files === null) return;
      const file = e.target.files[0];
      const reader = new FileReader();

      reader.onload = function () {
        try {
          const filedata = <ResourcesForImport>yaml.load(
            reader.result as string
          );
          data.filedata = filedata;
          data.isImportButtonDisabled = false;
        } catch (e) {
          setServerErrorMessage("Failed to load the selected file.");
        }
      };
      reader.onabort = function () {
        console.log("file read aborted");
      };
      reader.readAsText(file);
    }

    return {
      data,
      ImportScheduler,
      readfile,
    };
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/components/TopBar/ResetButton.vue">
<template>
  <v-dialog v-model="data.dialog" width="500">
    <template #activator="{ on }">
      <v-btn class="ma-2" color="error" v-on="on"> Reset </v-btn>
    </template>

    <v-card>
      <v-card-title class="2">
        Are you sure to reset all resources and scheduler configuration?
      </v-card-title>
      <v-divider></v-divider>
      <v-divider></v-divider>

      <v-card-actions>
        <v-spacer></v-spacer>
        <v-btn color="green darken-1" text @click="resetFn"> Reset </v-btn>
        <v-btn color="green darken-1" text @click="data.dialog = false">
          Cancel
        </v-btn>
      </v-card-actions>
    </v-card>
  </v-dialog>
</template>

<script lang="ts">
import { inject, defineComponent, reactive } from "@nuxtjs/composition-api";
import { ResetAPIKey } from "~/api/APIProviderKeys";
import SnackBarStoreKey from "../StoreKey/SnackBarStoreKey";

export default defineComponent({
  setup() {
    const resetAPI = inject(ResetAPIKey);
    if (!resetAPI) {
      throw new Error(`${ResetAPIKey.description} is not provided`);
    }

    const snackbarstore = inject(SnackBarStoreKey);
    if (!snackbarstore) {
      throw new Error(`${SnackBarStoreKey.description} is not provided`);
    }

    const setServerErrorMessage = (error: string) => {
      snackbarstore.setServerErrorMessage(error);
    };

    const setInfoMessage = (message: string) => {
      snackbarstore.setServerInfoMessage(message);
    };

    const resetFn = async () => {
      try {
        await resetAPI.reset();
        setInfoMessage("Successfully reset all resources");
      } catch (e: any) {
        setServerErrorMessage(e.message);
      } finally {
        data.dialog = false;
      }
    };

    const data = reactive({
      dialog: false,
    });

    return {
      resetFn,
      data,
    };
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/components/TopBar/SchedulerConfigurationEditButton.vue">
<template>
  <v-btn class="ma-2" @click="onClick()" v-if="!enabled()">
    <v-icon> mdi-cog </v-icon>
  </v-btn>
</template>

<script lang="ts">
import { inject, defineComponent, onMounted } from "@nuxtjs/composition-api";
import SchedulerConfigurationStoreKey from "../StoreKey/SchedulerConfigurationStoreKey";

export default defineComponent({
  setup() {
    const schedulerconfigurationstore = inject(SchedulerConfigurationStoreKey);
    if (!schedulerconfigurationstore) {
      throw new Error(`${SchedulerConfigurationStoreKey.description} is not provided`);
    }

    const initializeSchedulerConfigurationStore = () => {
      schedulerconfigurationstore.initialize();
    };
    onMounted(initializeSchedulerConfigurationStore);

    const enabled = () => {
      return schedulerconfigurationstore.disabled;
    };

    const onClick = () => {
      schedulerconfigurationstore.fetchSelected();
    };
    return {
      onClick,
      enabled,
    };
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/components/TopBar/TopBar.vue">
<template>
  <v-sheet class="transparent">
    <SchedulerConfigurationEditButton />
    <ExportButton />
    <ImportButton />
    <ResetButton />
  </v-sheet>
</template>

<script lang="ts">
import { defineComponent } from "@nuxtjs/composition-api";
import SchedulerConfigurationEditButton from "./SchedulerConfigurationEditButton.vue";
import ExportButton from "./ExportButton.vue";
import ImportButton from "./ImportButton.vue";
import ResetButton from "./ResetButton.vue";

export default defineComponent({
  components: {
    ImportButton,
    ExportButton,
    SchedulerConfigurationEditButton,
    ResetButton,
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/components/README.md">
# COMPONENTS

**This directory is not required, you can delete it if you don't want to use it.**

The components directory contains your Vue.js Components.

_Nuxt.js doesn't supercharge these components._
</file>

<file path="kube-scheduler-simulator/web/components/ResourceAddButton.vue">
<template>
  <v-sheet class="transparent">
    <v-btn
      v-for="(rn, i) in resourceNames"
      :key="i"
      color="primary ma-2"
      dark
      @click="create(rn)"
    >
      New {{ rn }}
    </v-btn>
  </v-sheet>
</template>

<script lang="ts">
import { ref, inject, defineComponent } from "@nuxtjs/composition-api";
import {
  podTemplate,
  nodeTemplate,
  pvTemplate,
  pvcTemplate,
  storageclassTemplate,
  priorityclassTemplate,
  namespaceTemplate,
} from "./lib/template";
import {} from "./lib/util";
import PodStoreKey from "./StoreKey/PodStoreKey";
import NodeStoreKey from "./StoreKey/NodeStoreKey";
import PersistentVolumeStoreKey from "./StoreKey/PVStoreKey";
import PersistentVolumeClaimStoreKey from "./StoreKey/PVCStoreKey";
import StorageClassStoreKey from "./StoreKey/StorageClassStoreKey";
import PriorityClassStoreKey from "./StoreKey/PriorityClassStoreKey";
import NamespaceStoreKey from "./StoreKey/NamespaceStoreKey";
import {
  V1Node,
  V1PersistentVolumeClaim,
  V1PersistentVolume,
  V1Pod,
  V1StorageClass,
  V1PriorityClass,
  V1Namespace,
} from "@kubernetes/client-node";

type Resource =
  | V1Pod
  | V1Node
  | V1PersistentVolumeClaim
  | V1PersistentVolume
  | V1StorageClass
  | V1PriorityClass
  | V1Namespace;

interface Store {
  readonly selected: object | null;
  readonly count: number;
  select(_resource: Resource | null, _isNew: boolean): void;
}

export default defineComponent({
  setup() {
    var store: Store | null = null;

    const podstore = inject(PodStoreKey);
    if (!podstore) {
      throw new Error(`${PodStoreKey.description} is not provided`);
    }

    const nodestore = inject(NodeStoreKey);
    if (!nodestore) {
      throw new Error(`${NodeStoreKey.description} is not provided`);
    }

    const pvstore = inject(PersistentVolumeStoreKey);
    if (!pvstore) {
      throw new Error(`${PersistentVolumeStoreKey.description} is not provided`);
    }

    const pvcstore = inject(PersistentVolumeClaimStoreKey);
    if (!pvcstore) {
      throw new Error(`${PersistentVolumeClaimStoreKey.description} is not provided`);
    }

    const storageclassstore = inject(StorageClassStoreKey);
    if (!storageclassstore) {
      throw new Error(`${StorageClassStoreKey.description} is not provided`);
    }

    const priorityclassstore = inject(PriorityClassStoreKey);
    if (!priorityclassstore) {
      throw new Error(`${PriorityClassStoreKey.description} is not provided`);
    }
    const namespacestore = inject(NamespaceStoreKey);
    if (!namespacestore) {
      throw new Error(`${namespacestore} is not provided`);
    }

    const dialog = ref(false);
    const resourceNames = [
      "StorageClass",
      "PersistentVolumeClaim",
      "PersistentVolume",
      "Node",
      "Pod",
      "PriorityClass",
      "Namespace",
    ];

    const create = (rn: string) => {
      var targetTemplate: Resource | null = null;
      switch (rn) {
        case "Pod":
          store = podstore;
          targetTemplate = podTemplate();
          break;
        case "Node":
          store = nodestore;
          targetTemplate = nodeTemplate();
          break;
        case "PersistentVolume":
          store = pvstore;
          targetTemplate = pvTemplate();
          break;
        case "PersistentVolumeClaim":
          store = pvcstore;
          targetTemplate = pvcTemplate();
          break;
        case "StorageClass":
          store = storageclassstore;
          targetTemplate = storageclassTemplate();
          break;
        case "PriorityClass":
          store = priorityclassstore;
          targetTemplate = priorityclassTemplate();
          break;
        case "Namespace":
          store = namespacestore;
          targetTemplate = namespaceTemplate();
          break;
      }

      if (store) {
        store.select(targetTemplate, true);
      }
      dialog.value = false;
    };

    return {
      create,
      dialog,
      resourceNames,
    };
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/components/ResourceWatcher.vue">
<template>
  <div>
    <slot />
  </div>
</template>

<script lang="ts">
import { defineComponent, inject, onMounted } from "@nuxtjs/composition-api";
import PodStoreKey from "./StoreKey/PodStoreKey";
import NodeStoreKey from "./StoreKey/NodeStoreKey";
import PersistentVolumeClaimStoreKey from "./StoreKey/PVCStoreKey";
import PersistentVolumeStoreKey from "./StoreKey/PVStoreKey";
import PriorityClassStoreKey from "./StoreKey/PriorityClassStoreKey";
import StorageClassStoreKey from "./StoreKey/StorageClassStoreKey";
import NamespaceStoreKey from "./StoreKey/NamespaceStoreKey";
import SnackBarStoreKey from "./StoreKey/SnackBarStoreKey";
import { WatcherAPIKey } from "~/api/APIProviderKeys";
import { WatchEventType } from "@/types/resources";
import { LastResourceVersions } from "@/types/api/v1";
import {
  V1Namespace,
  V1Node,
  V1PersistentVolume,
  V1PersistentVolumeClaim,
  V1Pod,
  V1PriorityClass,
  V1StorageClass,
} from "@kubernetes/client-node";

export default defineComponent({
  setup() {
    const watcherAPI = inject(WatcherAPIKey);
    if (!watcherAPI) {
      throw new Error(`${WatcherAPIKey.description} is not provided`);
    }
    const pstore = inject(PodStoreKey);
    if (!pstore) {
      throw new Error(`${PodStoreKey.description} is not provided`);
    }
    const nstore = inject(NodeStoreKey);
    if (!nstore) {
      throw new Error(`${NodeStoreKey.description} is not provided`);
    }
    const pvcstore = inject(PersistentVolumeClaimStoreKey);
    if (!pvcstore) {
      throw new Error(
        `${PersistentVolumeClaimStoreKey.description} is not provided`
      );
    }
    const pvstore = inject(PersistentVolumeStoreKey);
    if (!pvstore) {
      throw new Error(
        `${PersistentVolumeStoreKey.description} is not provided`
      );
    }
    const priorityclassstore = inject(PriorityClassStoreKey);
    if (!priorityclassstore) {
      throw new Error(`${PriorityClassStoreKey.description} is not provided`);
    }
    const storageclassstore = inject(StorageClassStoreKey);
    if (!storageclassstore) {
      throw new Error(`${StorageClassStoreKey.description} is not provided`);
    }
    const namespacestore = inject(NamespaceStoreKey);
    if (!namespacestore) {
      throw new Error(`${NamespaceStoreKey.description} is not provided`);
    }
    const snackbarstore = inject(SnackBarStoreKey);
    if (!snackbarstore) {
      throw new Error(`${SnackBarStoreKey.description} is not provided`);
    }

    // Initializes each resource and starts watching.
    onMounted(async () => {
      await pstore.initList();
      await nstore.initList();
      await pvcstore.initList();
      await pvstore.initList();
      await priorityclassstore.initList();
      await storageclassstore.initList();
      await namespacestore.initList();
      await watchAndUpdates();
    });

    const createlastResourceVersions = (): LastResourceVersions => {
      return {
        pods: pstore.lastResourceVersion,
        nodes: nstore.lastResourceVersion,
        pvs: pvstore.lastResourceVersion,
        pvcs: pvcstore.lastResourceVersion,
        storageClasses: storageclassstore.lastResourceVersion,
        priorityClasses: priorityclassstore.lastResourceVersion,
        namespaces: namespacestore.lastResourceVersion,
      } as LastResourceVersions;
    };

    // Call watch API and allocates the event to each resource's handler.
    const watchAndUpdates = () => {
      watcherAPI
        .watchResources(createlastResourceVersions() as LastResourceVersions)
        .then((response) => {
          if (!response.body) {
            return;
          }
          const stream = response.body.getReader();
          const utf8Decoder = new TextDecoder("utf-8");
          let buffer = "";

          return stream.read().then(function processText({ done, value }): any {
            if (done) {
              snackbarstore.setServerErrorMessage(
                "The watch stream is terminated. Please reload this page."
              );
              return;
            }
            buffer += utf8Decoder.decode(value);
            buffer = onNewLine(buffer, async (chunk: string) => {
              if (chunk.trim().length === 0) {
                return;
              }
              try {
                const event = JSON.parse(chunk) as WatchEvent;
                switch (event.Kind) {
                  case resourceKind.PODS: {
                    pstore.watchEventHandler(
                      event.EventType,
                      event.Obj as V1Pod
                    );
                    pstore.setLastResourceVersion(event.Obj as V1Pod);
                    break;
                  }
                  case resourceKind.NODES: {
                    nstore.watchEventHandler(
                      event.EventType,
                      event.Obj as V1Node
                    );
                    nstore.setLastResourceVersion(event.Obj as V1Node);
                    break;
                  }
                  case resourceKind.PVS: {
                    pvstore.watchEventHandler(
                      event.EventType,
                      event.Obj as V1PersistentVolume
                    );
                    pvstore.setLastResourceVersion(
                      event.Obj as V1PersistentVolume
                    );
                    break;
                  }
                  case resourceKind.PVCS: {
                    pvcstore.watchEventHandler(
                      event.EventType,
                      event.Obj as V1PersistentVolumeClaim
                    );
                    pvcstore.setLastResourceVersion(
                      event.Obj as V1PersistentVolumeClaim
                    );
                    break;
                  }
                  case resourceKind.SCS: {
                    storageclassstore.watchEventHandler(
                      event.EventType,
                      event.Obj as V1StorageClass
                    );
                    storageclassstore.setLastResourceVersion(
                      event.Obj as V1StorageClass
                    );
                    break;
                  }
                  case resourceKind.PCS: {
                    priorityclassstore.watchEventHandler(
                      event.EventType,
                      event.Obj as V1PriorityClass
                    );
                    priorityclassstore.setLastResourceVersion(
                      event.Obj as V1PriorityClass
                    );
                    break;
                  }
                  case resourceKind.NAMESPACES: {
                    namespacestore.watchEventHandler(
                      event.EventType,
                      event.Obj as V1Namespace
                    );
                    namespacestore.setLastResourceVersion(
                      event.Obj as V1Namespace
                    );
                    break;
                  }
                }
              } catch (error) {
                snackbarstore.setServerErrorMessage(
                  `Error while parsing: ${error}`
                );
              }
            });
            return stream.read().then(processText);
          });
        })
        .catch(() => {
          console.log(
            "Error during watching. Trying to reconnect to the server in 5 seconds..."
          );
          // Call the watch API again if some error occurs.
          setTimeout(() => watchAndUpdates(), 5000);
        });
    };
    return {};
  },
});

type WatchEvent = {
  Kind: resourceKind;
  EventType: WatchEventType;
  Obj: Object;
};

enum resourceKind {
  PODS = "pods",
  NODES = "nodes",
  PVS = "persistentvolumes",
  PVCS = "persistentvolumeclaims",
  SCS = "storageclasses",
  PCS = "priorityclasses",
  NAMESPACES = "namespaces",
}

function onNewLine(buffer: string, fn: Function): string {
  const newLineIndex = buffer.indexOf("\n");
  if (newLineIndex === -1) {
    return buffer;
  }
  const chunk = buffer.slice(0, buffer.indexOf("\n"));
  const newBuffer = buffer.slice(buffer.indexOf("\n") + 1);
  fn(chunk);
  return onNewLine(newBuffer, fn);
}
</script>
</file>

<file path="kube-scheduler-simulator/web/components/Snackbar.vue">
<template>
  <v-snackbar
    :value="isOpen"
    bottom
    :timeout="3000"
    :color="color()"
    @input="setIsOpen"
  >
    {{ message }}
    <v-btn color="white" text @click="closeSnackbar">Close</v-btn>
  </v-snackbar>
</template>
<script lang="ts">
import { inject, defineComponent, computed } from "@nuxtjs/composition-api";
import SnackBarStoreKey from "./StoreKey/SnackBarStoreKey";

export default defineComponent({
  setup() {
    const store = inject(SnackBarStoreKey);
    if (!store) {
      throw new Error(`${SnackBarStoreKey.description} is not provided`);
    }

    const color = () => {
      return store.messageType === "info" ? "primary" : "error";
    };
    const setIsOpen = (b: boolean) => {
      store.setIsOpen(b);
    };
    const closeSnackbar = () => store.close();

    const isOpen = computed(() => store.isOpen);
    const message = computed(() => store.message);

    return {
      color,
      setIsOpen,
      closeSnackbar,
      message,
      isOpen,
    };
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/docs/environment-variables.md">
### Environment Variables
These describe the environment variables that are used to configure the simulator's frontend.

Please refer [compose.yml](./../compose.yml) as an example use.

`KUBE_API_SERVER_URL`: This is the kube-apiserver URL. Its default
value is `http://localhost:3131`.

`BASE_URL`: This is the URL for the kube-scheduler-simulator
server. Its default value is `http://localhost:1212`.

`ALPHA_TABLE_VIEWS`: This variable enables the alpha feature `table
view`. Its value is either 0(default) or 1 (0 means disabled, 1
meaning enabled). We can see the resource status in the table.
</file>

<file path="kube-scheduler-simulator/web/layouts/default.vue">
<template>
  <v-app :style="{ background: $vuetify.theme.themes.light.background }">
    <v-app-bar color="primary" app>
      <img
        src="/kubernetes-logo_with_border.svg"
        height="40"
        alt="p.metadata.name"
        class="mx-2"
      />
      <v-toolbar-title
        class="white--text"
        :style="{ cursor: 'pointer' }"
        v-text="title"
      />
    </v-app-bar>
    <v-main>
      <v-container>
        <nuxt />
      </v-container>
    </v-main>
    <v-footer app>
      <span>&copy; {{ new Date().getFullYear() }}</span>
    </v-footer>
  </v-app>
</template>

<script>
import { defineComponent } from "@nuxtjs/composition-api";

export default defineComponent({
  setup() {},
  data() {
    return {
      title: "Kubernetes scheduler simulator",
    };
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/layouts/error.vue">
<template>
  <v-app dark>
    <h1 v-if="error.statusCode === 404">
      {{ pageNotFound }}
    </h1>
    <h1 v-else>
      {{ otherError }}
    </h1>
    <NuxtLink to="/"> Home page </NuxtLink>
  </v-app>
</template>

<script>
export default {
  layout: "empty",
  props: {
    error: {
      type: Object,
      default: null,
    },
  },
  data() {
    return {
      pageNotFound: "404 Not Found",
      otherError: "An error occurred",
    };
  },
  head() {
    const title =
      this.error.statusCode === 404 ? this.pageNotFound : this.otherError;
    return {
      title,
    };
  },
};
</script>

<style scoped>
h1 {
  font-size: 20px;
}
</style>
</file>

<file path="kube-scheduler-simulator/web/layouts/README.md">
# LAYOUTS

**This directory is not required, you can delete it if you don't want to use it.**

This directory contains your Application Layouts.

More information about the usage of this directory in [the documentation](https://nuxtjs.org/guide/views#layouts).
</file>

<file path="kube-scheduler-simulator/web/middleware/README.md">
# MIDDLEWARE

**This directory is not required, you can delete it if you don't want to use it.**

This directory contains your application middleware.
Middleware let you define custom functions that can be run before rendering either a page or a group of pages.

More information about the usage of this directory in [the documentation](https://nuxtjs.org/guide/routing#middleware).
</file>

<file path="kube-scheduler-simulator/web/pages/index.vue">
<template>
  <APIProvider>
    <PodStoreProvider>
      <NodeStoreProvider>
        <PVStoreProvider>
          <PVCStoreProvider>
            <SchedulerConfigurationStoreProvider>
              <StorageClassStoreProvider>
                <PriorityClassStoreProvider>
                  <NamespaceStoreProvider>
                    <SnackbarStoreProvider>
                      <ResourceWatcher>
                        <ResourceBar />
                        <TopBar />
                        <ResourceAddButton />
                        <ResourceViewPanel />
                        <Snackbar />
                      </ResourceWatcher>
                    </SnackbarStoreProvider>
                  </NamespaceStoreProvider>
                </PriorityClassStoreProvider>
              </StorageClassStoreProvider>
            </SchedulerConfigurationStoreProvider>
          </PVCStoreProvider>
        </PVStoreProvider>
      </NodeStoreProvider>
    </PodStoreProvider>
  </APIProvider>
</template>

<script lang="ts">
import { defineComponent } from "@nuxtjs/composition-api";
import APIProvider from "~/api/APIProvider.vue";
import TopBar from "~/components/TopBar/TopBar.vue";
import NodeStoreProvider from "~/components/StoreProvider/NodeStoreProvider.vue";
import PVStoreProvider from "~/components/StoreProvider/PVStoreProvider.vue";
import PVCStoreProvider from "~/components/StoreProvider/PVCStoreProvider.vue";
import SchedulerConfigurationStoreProvider from "~/components/StoreProvider/SchedulerConfigurationStoreProvider.vue";
import StorageClassStoreProvider from "~/components/StoreProvider/StorageClassStoreProvider.vue";
import PriorityClassStoreProvider from "~/components/StoreProvider/PriorityClassStoreProvider.vue";
import NamespaceStoreProvider from "~/components/StoreProvider/NamespaceStoreProvider.vue";
import ResourceViewPanel from "~/components/ResourceViews/ResourcesViewPanel.vue";
import PodStoreProvider from "~/components/StoreProvider/PodStoreProvider.vue";
import SnackbarStoreProvider from "~/components/StoreProvider/SnackbarStoreProvider.vue";
import ResourceAddButton from "~/components/ResourceAddButton.vue";
import ResourceBar from "~/components/ResourceBar/ResourceBar.vue";
import Snackbar from "~/components/Snackbar.vue";
import ResourceWatcher from "~/components/ResourceWatcher.vue";

export default defineComponent({
  components: {
    APIProvider,
    Snackbar,
    SnackbarStoreProvider,
    NodeStoreProvider,
    ResourceViewPanel,
    PodStoreProvider,
    ResourceAddButton,
    ResourceBar,
    StorageClassStoreProvider,
    PVStoreProvider,
    PVCStoreProvider,
    NamespaceStoreProvider,
    TopBar,
    SchedulerConfigurationStoreProvider,
    PriorityClassStoreProvider,
    ResourceWatcher,
  },
});
</script>
</file>

<file path="kube-scheduler-simulator/web/pages/README.md">
# PAGES

This directory contains your Application Views and Routes.
The framework reads all the `*.vue` files inside this directory and creates the router of your application.

More information about the usage of this directory in [the documentation](https://nuxtjs.org/guide/routing).
</file>

<file path="kube-scheduler-simulator/web/plugins/ApiRuntimeConfigPlugin.ts">
import { Plugin } from "@nuxt/types";

const ApiRuntimeConfigPlugin: Plugin = (context, inject): void => {
  const baseURL = context.$config.baseURL + "/api/v1";
  const instance = context.$axios.create({
    baseURL: baseURL,
    withCredentials: true,
  });

  const k8sBaseURL = context.$config.kubeApiServerURL + "/api/v1/";
  const k8sInstance = context.$axios.create({
    baseURL: k8sBaseURL,
    withCredentials: true,
  });
  const k8sSchedulingBaseURL =
    context.$config.kubeApiServerURL + "/apis/scheduling.k8s.io/v1/";
  const k8sSchedulingInstance = context.$axios.create({
    baseURL: k8sSchedulingBaseURL,
    withCredentials: true,
  });

  const k8sStorageBaseURL =
    context.$config.kubeApiServerURL + "/apis/storage.k8s.io/v1/";
  const k8sStorageInstance = context.$axios.create({
    baseURL: k8sStorageBaseURL,
    withCredentials: true,
  });

  inject("instance", instance);
  inject("k8sInstance", k8sInstance);
  inject("k8sSchedulingInstance", k8sSchedulingInstance);
  inject("k8sStorageInstance", k8sStorageInstance);
};

export default ApiRuntimeConfigPlugin;
</file>

<file path="kube-scheduler-simulator/web/static/kubernetes-logo_with_border.svg">
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!-- Created with Inkscape (http://www.inkscape.org/) -->

<svg
   xmlns:dc="http://purl.org/dc/elements/1.1/"
   xmlns:cc="http://creativecommons.org/ns#"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:svg="http://www.w3.org/2000/svg"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   width="761.57269"
   height="740.32532"
   id="svg2"
   version="1.1"
   inkscape:version="0.48.4 r9939"
   sodipodi:docname="logo_with_border.svg"
   inkscape:export-filename="/home/thockin/Downloads/kubernetes.png"
   inkscape:export-xdpi="90"
   inkscape:export-ydpi="90">
  <defs
     id="defs4" />
  <sodipodi:namedview
     id="base"
     pagecolor="#ffffff"
     bordercolor="#666666"
     borderopacity="1.0"
     inkscape:pageopacity="0.0"
     inkscape:pageshadow="2"
     inkscape:zoom="0.75130096"
     inkscape:cx="-124.96683"
     inkscape:cy="329.0861"
     inkscape:document-units="px"
     inkscape:current-layer="layer1"
     showgrid="false"
     inkscape:window-width="1519"
     inkscape:window-height="822"
     inkscape:window-x="73"
     inkscape:window-y="25"
     inkscape:window-maximized="0"
     inkscape:snap-global="false"
     fit-margin-top="10"
     fit-margin-left="10"
     fit-margin-right="10"
     fit-margin-bottom="10" />
  <metadata
     id="metadata7">
    <rdf:RDF>
      <cc:Work
         rdf:about="">
        <dc:format>image/svg+xml</dc:format>
        <dc:type
           rdf:resource="http://purl.org/dc/dcmitype/StillImage" />
        <dc:title />
      </cc:Work>
    </rdf:RDF>
  </metadata>
  <g
     inkscape:label="Layer 1"
     inkscape:groupmode="layer"
     id="layer1"
     transform="translate(8.8111836,-148.45869)">
    <path
       style="fill:#326ce5;fill-opacity:1;stroke:none;stroke-width:0;stroke-miterlimit:4;stroke-opacity:1;stroke-dasharray:none"
       d="m 369.53875,177.69828 a 46.724621,46.342246 0 0 0 -17.90625,4.53125 l -244.34376,116.75 a 46.724621,46.342246 0 0 0 -25.28125,31.4375 l -60.28125,262.25 a 46.724621,46.342246 0 0 0 6.34375,35.53125 46.724621,46.342246 0 0 0 2.65625,3.6875 l 169.12501,210.28125 a 46.724621,46.342246 0 0 0 36.53125,17.4375 l 271.21875,-0.0625 A 46.724621,46.342246 0 0 0 544.1325,842.13578 L 713.195,631.82328 a 46.724621,46.342246 0 0 0 9.03125,-39.21875 l -60.375,-262.25 A 46.724621,46.342246 0 0 0 636.57,298.91703 L 392.195,182.22953 a 46.724621,46.342246 0 0 0 -22.65625,-4.53125 z"
       id="path3055"
       inkscape:connector-curvature="0"
       inkscape:export-filename="new.png"
       inkscape:export-xdpi="250.55"
       inkscape:export-ydpi="250.55" />
    <path
       style="font-size:medium;font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;text-indent:0;text-align:start;text-decoration:none;line-height:normal;letter-spacing:normal;word-spacing:normal;text-transform:none;direction:ltr;block-progression:tb;writing-mode:lr-tb;text-anchor:start;baseline-shift:baseline;color:#000000;fill:#ffffff;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0;marker:none;visibility:visible;display:inline;overflow:visible;enable-background:accumulate;font-family:Sans;-inkscape-font-specification:Sans;stroke-miterlimit:4;stroke-dasharray:none"
       d="M 378.21875 10.25 A 49.325911 48.922245 0 0 0 359.34375 15.0625 L 101.375 138.28125 A 49.325911 48.922245 0 0 0 74.6875 171.46875 L 11.0625 448.3125 A 49.325911 48.922245 0 0 0 17.75 485.84375 A 49.325911 48.922245 0 0 0 20.5625 489.71875 L 199.09375 711.71875 A 49.325911 48.922245 0 0 0 237.65625 730.125 L 523.96875 730.0625 A 49.325911 48.922245 0 0 0 562.53125 711.6875 L 741 489.65625 A 49.325911 48.922245 0 0 0 750.53125 448.28125 L 686.8125 171.40625 A 49.325911 48.922245 0 0 0 660.125 138.25 L 402.15625 15.03125 A 49.325911 48.922245 0 0 0 378.21875 10.25 z M 378.34375 29.25 A 46.724621 46.342246 0 0 1 401 33.78125 L 645.375 150.46875 A 46.724621 46.342246 0 0 1 670.65625 181.90625 L 731.03125 444.15625 A 46.724621 46.342246 0 0 1 722 483.375 L 552.9375 693.6875 A 46.724621 46.342246 0 0 1 516.40625 711.09375 L 245.1875 711.15625 A 46.724621 46.342246 0 0 1 208.65625 693.71875 L 39.53125 483.4375 A 46.724621 46.342246 0 0 1 36.875 479.75 A 46.724621 46.342246 0 0 1 30.53125 444.21875 L 90.8125 181.96875 A 46.724621 46.342246 0 0 1 116.09375 150.53125 L 360.4375 33.78125 A 46.724621 46.342246 0 0 1 378.34375 29.25 z "
       id="path3054-2-9"
       transform="translate(-8.8111836,148.45869)" />
    <path
       inkscape:connector-curvature="0"
       id="path3059"
       d="m 371.9599,266.94541 c -8.07696,8.2e-4 -14.62596,7.27591 -14.625,16.24999 10e-6,0.13773 0.0282,0.26934 0.0312,0.40625 -0.0119,1.21936 -0.0708,2.68836 -0.0312,3.75 0.19262,5.176 1.3209,9.13749 2,13.90625 1.23028,10.20666 2.26117,18.66736 1.625,26.53125 -0.61869,2.9654 -2.80288,5.67741 -4.75,7.5625 l -0.34375,6.1875 c -8.77682,0.72717 -17.61235,2.05874 -26.4375,4.0625 -37.97461,8.62218 -70.67008,28.18307 -95.5625,54.59375 -1.61522,-1.10193 -4.44103,-3.12914 -5.2813,-3.75 -2.61117,0.35262 -5.25021,1.15829 -8.6875,-0.84375 -6.54491,-4.40563 -12.50587,-10.48693 -19.71875,-17.8125 -3.30498,-3.50419 -5.69832,-6.84101 -9.625,-10.21875 -0.89172,-0.76707 -2.25258,-1.80455 -3.25,-2.59375 -3.06988,-2.44757 -6.6907,-3.72402 -10.1875,-3.84375 -4.49589,-0.15394 -8.82394,1.60385 -11.65625,5.15625 -5.03521,6.31538 -3.42312,15.96805 3.59375,21.5625 0.0712,0.0567 0.14702,0.10078 0.21875,0.15625 0.96422,0.78162 2.14496,1.78313 3.03125,2.4375 4.16687,3.07655 7.9732,4.65145 12.125,7.09375 8.747,5.40181 15.99837,9.88086 21.75,15.28125 2.24602,2.39417 2.63858,6.61292 2.9375,8.4375 l 4.6875,4.1875 c -25.09342,37.76368 -36.70686,84.40946 -29.8437,131.9375 l -6.125,1.78125 c -1.6143,2.08461 -3.89541,5.36474 -6.2813,6.34375 -7.52513,2.37021 -15.99424,3.24059 -26.21875,4.3125 -4.80031,0.39915 -8.94218,0.16095 -14.03125,1.125 -1.12008,0.21218 -2.68072,0.61877 -3.90625,0.90625 -0.0426,0.009 -0.0824,0.0216 -0.125,0.0312 -0.0668,0.0155 -0.15456,0.0479 -0.21875,0.0625 -8.62014,2.08279 -14.15774,10.006 -12.375,17.8125 1.78316,7.80833 10.20314,12.55677 18.875,10.6875 0.0626,-0.0143 0.1535,-0.0167 0.21875,-0.0312 0.0979,-0.0224 0.18409,-0.0699 0.28125,-0.0937 1.20885,-0.26536 2.72377,-0.5606 3.78125,-0.84375 5.00334,-1.33963 8.62694,-3.30796 13.125,-5.03125 9.67694,-3.47077 17.69173,-6.37022 25.5,-7.5 3.26118,-0.25542 6.69711,2.01216 8.40625,2.96875 l 6.375,-1.09375 c 14.67018,45.48282 45.41416,82.24502 84.34375,105.3125 l -2.65625,6.375 c 0.95742,2.47542 2.01341,5.8247 1.30022,8.26932 -2.83868,7.3612 -7.70097,15.13097 -13.23772,23.79318 -2.68085,4.00192 -5.42453,7.10761 -7.84375,11.6875 -0.5789,1.09589 -1.31618,2.77932 -1.875,3.9375 -3.75884,8.04236 -1.00164,17.3052 6.21875,20.78125 7.26575,3.49788 16.28447,-0.19134 20.1875,-8.25 0.006,-0.0114 0.0257,-0.0198 0.0312,-0.0312 0.004,-0.009 -0.004,-0.0225 0,-0.0312 0.55593,-1.14255 1.34353,-2.64437 1.8125,-3.71875 2.07213,-4.74702 2.76161,-8.81506 4.21875,-13.40625 3.86962,-9.72014 5.99567,-19.91903 11.32258,-26.27411 1.45868,-1.74023 3.83681,-2.4095 6.30242,-3.06964 l 3.3125,-6 c 33.93824,13.0268 71.92666,16.52246 109.875,7.90625 8.65697,-1.96557 17.01444,-4.50945 25.09375,-7.5625 0.93098,1.65133 2.66113,4.8257 3.125,5.625 2.50559,0.81518 5.24044,1.23614 7.46875,4.53125 3.98539,6.80898 6.7109,14.86416 10.03125,24.59375 1.45738,4.59111 2.17762,8.65933 4.25,13.40625 0.47234,1.08195 1.256,2.60486 1.8125,3.75 3.89482,8.08484 12.94212,11.78667 20.21875,8.28125 7.2195,-3.4779 9.97974,-12.7399 6.21875,-20.78125 -0.55889,-1.15814 -1.3273,-2.84164 -1.90625,-3.9375 -2.41946,-4.57976 -5.1627,-7.65448 -7.84375,-11.65625 -5.53721,-8.66192 -10.12968,-15.8577 -12.96875,-23.21875 -1.18711,-3.79657 0.20028,-6.15774 1.125,-8.625 -0.55378,-0.63477 -1.73881,-4.22009 -2.4375,-5.90625 40.4574,-23.88816 70.29856,-62.02129 84.3125,-106.0625 1.8924,0.29742 5.18154,0.87936 6.25,1.09375 2.19954,-1.4507 4.22194,-3.34352 8.1875,-3.03125 7.80832,1.12937 15.82288,4.02973 25.5,7.5 4.49815,1.72306 8.1216,3.72313 13.125,5.0625 1.05749,0.28309 2.57238,0.5472 3.78125,0.8125 0.0972,0.0238 0.1833,0.0714 0.28125,0.0937 0.0653,0.0146 0.15615,0.0169 0.21875,0.0312 8.67236,1.86695 17.09384,-2.87871 18.875,-10.6875 1.78074,-7.80696 -3.7543,-15.73201 -12.375,-17.8125 -1.25393,-0.28513 -3.03225,-0.76938 -4.25,-1 -5.08912,-0.96378 -9.23092,-0.7261 -14.03125,-1.125 -10.22456,-1.07138 -18.6935,-1.94269 -26.21875,-4.3125 -3.06826,-1.19028 -5.25103,-4.84124 -6.31255,-6.34375 l -5.90625,-1.71875 c 3.06226,-22.15442 2.23655,-45.21134 -3.0625,-68.28125 -5.34839,-23.28471 -14.80037,-44.58084 -27.40625,-63.34375 1.51505,-1.37729 4.37619,-3.91091 5.1875,-4.65625 0.23716,-2.62417 0.0334,-5.37553 2.75,-8.28125 5.75134,-5.40069 13.00329,-9.87898 21.75,-15.28125 4.15167,-2.44252 7.98954,-4.01698 12.15625,-7.09375 0.94225,-0.69576 2.2289,-1.79759 3.21875,-2.59375 7.01538,-5.59633 8.63058,-15.24842 3.59375,-21.5625 -5.03683,-6.31408 -14.79712,-6.90883 -21.8125,-1.3125 -0.99856,0.79085 -2.35353,1.82252 -3.25,2.59375 -3.9265,3.37796 -6.35145,6.71439 -9.65625,10.21875 -7.21249,7.32595 -13.17407,13.43777 -19.71875,17.84375 -2.83601,1.65106 -6.98996,1.07978 -8.87505,0.96875 l -5.5625,3.96875 c -31.7188,-33.26057 -74.90466,-54.52546 -121.40605,-58.6563 -0.13006,-1.94872 -0.30045,-5.47117 -0.34375,-6.53125 -1.90371,-1.82165 -4.20342,-3.37686 -4.78125,-7.3125 -0.63617,-7.86389 0.42597,-16.32459 1.65625,-26.53125 0.6791,-4.76876 1.80738,-8.73025 2,-13.90625 0.0438,-1.17663 -0.0265,-2.88401 -0.0312,-4.15625 -9.6e-4,-8.97408 -6.54804,-16.25081 -14.625,-16.24999 z m -18.3125,113.43749 -4.34375,76.71875 -0.3125,0.15625 c -0.29134,6.86335 -5.93996,12.34375 -12.875,12.34375 -2.84081,0 -5.46294,-0.91229 -7.59375,-2.46875 l -0.125,0.0625 -62.90625,-44.59375 c 19.33365,-19.01115 44.06291,-33.06039 72.5625,-39.53125 5.20599,-1.18203 10.40966,-2.0591 15.59375,-2.6875 z m 36.65625,0 c 33.27347,4.09232 64.04501,19.15882 87.625,42.25 l -62.5,44.3125 -0.21875,-0.0937 c -5.54745,4.05169 -13.36343,3.04639 -17.6875,-2.375 -1.77132,-2.22096 -2.70072,-4.83239 -2.8125,-7.46875 l -0.0625,-0.0312 z m -147.625,70.875 57.4375,51.375 -0.0625,0.3125 c 5.18437,4.50697 5.94888,12.32794 1.625,17.75 -1.7712,2.22105 -4.14208,3.71074 -6.6875,4.40625 l -0.0625,0.25 -73.625,21.25 c -3.74728,-34.26517 4.32855,-67.57364 21.375,-95.34375 z m 258.15625,0.0312 c 8.5341,13.83256 14.99655,29.28214 18.84375,46.03125 3.80106,16.54828 4.75499,33.06697 3.1875,49.03125 l -74,-21.3125 -0.0625,-0.3125 c -6.6265,-1.81104 -10.69893,-8.55162 -9.15625,-15.3125 0.63203,-2.76962 2.10222,-5.11264 4.09375,-6.84375 l -0.0312,-0.15625 57.125,-51.125 z m -140.65625,55.3125 23.53125,0 14.625,18.28125 -5.25,22.8125 -21.125,10.15625 -21.1875,-10.1875 -5.25,-22.8125 z m 75.4375,62.5625 c 0.99997,-0.0505 1.99558,0.0396 2.96875,0.21875 l 0.125,-0.15625 76.15625,12.875 c -11.1455,31.3131 -32.47281,58.44018 -60.96875,76.59375 l -29.5625,-71.40625 0.0937,-0.125 c -2.71561,-6.30999 0.002,-13.70956 6.25,-16.71875 1.59965,-0.77041 3.27089,-1.19701 4.9375,-1.28125 z m -127.90625,0.3125 c 5.81174,0.0815 11.02462,4.11525 12.375,10.03125 0.63219,2.76958 0.3245,5.51375 -0.71875,7.9375 l 0.21875,0.28125 -29.25,70.6875 c -27.34716,-17.5486 -49.12927,-43.82403 -60.78125,-76.06245 l 75.5,-12.8125 0.125,0.15625 c 0.84451,-0.15541 1.701,-0.2304 2.53125,-0.21875 z m 63.78125,30.9688 c 2.02445,-0.0744 4.07865,0.34098 6.03125,1.28125 2.55951,1.23253 4.53673,3.17319 5.78125,5.5 l 0.28125,0 37.21875,67.25 c -4.83029,1.61923 -9.79609,3.00308 -14.875,4.15625 -28.46453,6.4629 -56.83862,4.50467 -82.53125,-4.25 l 37.125,-67.125 0.0625,0 c 2.22767,-4.16441 6.45247,-6.64887 10.90625,-6.8125 z"
       style="font-size:medium;font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;text-indent:0;text-align:start;text-decoration:none;line-height:normal;letter-spacing:normal;word-spacing:normal;text-transform:none;direction:ltr;block-progression:tb;writing-mode:lr-tb;text-anchor:start;baseline-shift:baseline;color:#000000;fill:#ffffff;fill-opacity:1;stroke:none;stroke-width:0;stroke-miterlimit:4;stroke-opacity:1;stroke-dasharray:none;marker:none;visibility:visible;display:inline;overflow:visible;enable-background:accumulate;font-family:Sans;-inkscape-font-specification:Sans"
       sodipodi:nodetypes="ccccccccsccccscssccsccccccccscccsccccccccccccccscccscsccsccccscscsccccccccscccscsccccsccccscscscccccccccccccccscccsccccccccccccscccccscccccccccccccccccccccccscccscccccccccscccscccc"
       inkscape:export-filename="./path3059.png"
       inkscape:export-xdpi="250.55"
       inkscape:export-ydpi="250.55" />
  </g>
</svg>
</file>

<file path="kube-scheduler-simulator/web/static/kubernetes-logo.svg">
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!-- Created with Inkscape (http://www.inkscape.org/) -->

<svg
   xmlns:dc="http://purl.org/dc/elements/1.1/"
   xmlns:cc="http://creativecommons.org/ns#"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:svg="http://www.w3.org/2000/svg"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   width="722.8457"
   height="701.96637"
   id="svg2"
   version="1.1"
   inkscape:version="0.48.4 r9939"
   sodipodi:docname="logo.svg"
   inkscape:export-filename="/home/thockin/src/kubernetes/new.png"
   inkscape:export-xdpi="460.95001"
   inkscape:export-ydpi="460.95001">
  <defs
     id="defs4" />
  <sodipodi:namedview
     id="base"
     pagecolor="#ffffff"
     bordercolor="#666666"
     borderopacity="1.0"
     inkscape:pageopacity="0.0"
     inkscape:pageshadow="2"
     inkscape:zoom="16.190509"
     inkscape:cx="277.56851"
     inkscape:cy="157.54494"
     inkscape:document-units="px"
     inkscape:current-layer="g3052"
     showgrid="false"
     inkscape:window-width="1519"
     inkscape:window-height="822"
     inkscape:window-x="51"
     inkscape:window-y="25"
     inkscape:window-maximized="0"
     inkscape:snap-global="false"
     fit-margin-top="10"
     fit-margin-left="10"
     fit-margin-right="10"
     fit-margin-bottom="10" />
  <metadata
     id="metadata7">
    <rdf:RDF>
      <cc:Work
         rdf:about="">
        <dc:format>image/svg+xml</dc:format>
        <dc:type
           rdf:resource="http://purl.org/dc/dcmitype/StillImage" />
        <dc:title />
      </cc:Work>
    </rdf:RDF>
  </metadata>
  <g
     inkscape:label="Layer 1"
     inkscape:groupmode="layer"
     id="layer1"
     transform="translate(-6.3260942,-174.7524)">
    <g
       id="g3052">
      <path
         style="fill:#326ce5;fill-opacity:1;stroke:#ffffff;stroke-width:0;stroke-miterlimit:4;stroke-opacity:1;stroke-dasharray:none"
         d="m 365.3125,184.8125 a 46.724621,46.342246 0 0 0 -17.90625,4.53125 l -244.34375,116.75 a 46.724621,46.342246 0 0 0 -25.28125,31.4375 L 17.5,599.78125 A 46.724621,46.342246 0 0 0 23.84375,635.3125 46.724621,46.342246 0 0 0 26.5,639 l 169.125,210.28125 a 46.724621,46.342246 0 0 0 36.53125,17.4375 L 503.375,866.65625 A 46.724621,46.342246 0 0 0 539.90625,849.25 L 708.96875,638.9375 A 46.724621,46.342246 0 0 0 718,599.71875 l -60.375,-262.25 a 46.724621,46.342246 0 0 0 -25.28125,-31.4375 l -244.375,-116.6875 A 46.724621,46.342246 0 0 0 365.3125,184.8125 z"
         id="path3055"
         inkscape:connector-curvature="0"
         inkscape:export-filename="new.png"
         inkscape:export-xdpi="250.55"
         inkscape:export-ydpi="250.55" />
      <path
         inkscape:connector-curvature="0"
         id="path3059"
         d="m 367.73366,274.05962 c -8.07696,8.2e-4 -14.62596,7.27591 -14.625,16.25 1e-5,0.13773 0.0282,0.26934 0.0312,0.40625 -0.0119,1.21936 -0.0708,2.68836 -0.0312,3.75 0.19262,5.176 1.3209,9.13749 2,13.90625 1.23028,10.20666 2.26117,18.66736 1.625,26.53125 -0.61869,2.9654 -2.80288,5.67741 -4.75,7.5625 l -0.34375,6.1875 c -8.77682,0.72717 -17.61235,2.05874 -26.4375,4.0625 -37.97461,8.62218 -70.67008,28.18307 -95.5625,54.59375 -1.61522,-1.10193 -4.44103,-3.12914 -5.2813,-3.75 -2.61117,0.35262 -5.25021,1.15829 -8.6875,-0.84375 -6.54491,-4.40563 -12.50587,-10.48693 -19.71875,-17.8125 -3.30498,-3.50419 -5.69832,-6.84101 -9.625,-10.21875 -0.89172,-0.76707 -2.25258,-1.80455 -3.25,-2.59375 -3.06988,-2.44757 -6.6907,-3.72402 -10.1875,-3.84375 -4.49589,-0.15394 -8.82394,1.60385 -11.65625,5.15625 -5.03521,6.31538 -3.42312,15.96805 3.59375,21.5625 0.0712,0.0567 0.14702,0.10078 0.21875,0.15625 0.96422,0.78162 2.14496,1.78313 3.03125,2.4375 4.16687,3.07655 7.9732,4.65145 12.125,7.09375 8.747,5.40181 15.99837,9.88086 21.75,15.28125 2.24602,2.39417 2.63858,6.61292 2.9375,8.4375 l 4.6875,4.1875 c -25.09342,37.76368 -36.70686,84.40946 -29.8437,131.9375 l -6.125,1.78125 c -1.6143,2.08461 -3.89541,5.36474 -6.2813,6.34375 -7.52513,2.37021 -15.99424,3.24059 -26.21875,4.3125 -4.80031,0.39915 -8.94218,0.16095 -14.03125,1.125 -1.12008,0.21218 -2.68072,0.61877 -3.90625,0.90625 -0.0426,0.009 -0.0824,0.0216 -0.125,0.0312 -0.0668,0.0155 -0.15456,0.0479 -0.21875,0.0625 -8.62014,2.08279 -14.15774,10.006 -12.375,17.8125 1.78316,7.80833 10.20314,12.55677 18.875,10.6875 0.0626,-0.0143 0.1535,-0.0167 0.21875,-0.0312 0.0979,-0.0224 0.18409,-0.0699 0.28125,-0.0937 1.20885,-0.26536 2.72377,-0.5606 3.78125,-0.84375 5.00334,-1.33963 8.62694,-3.30796 13.125,-5.03125 9.67694,-3.47077 17.69173,-6.37022 25.5,-7.5 3.26118,-0.25542 6.69711,2.01216 8.40625,2.96875 l 6.375,-1.09375 c 14.67018,45.48282 45.41416,82.24502 84.34375,105.3125 l -2.65625,6.375 c 0.95742,2.47542 2.01341,5.8247 1.30022,8.26932 -2.83868,7.3612 -7.70097,15.13097 -13.23772,23.79318 -2.68085,4.00192 -5.42453,7.10761 -7.84375,11.6875 -0.5789,1.09589 -1.31618,2.77932 -1.875,3.9375 -3.75884,8.04236 -1.00164,17.3052 6.21875,20.78125 7.26575,3.49788 16.28447,-0.19134 20.1875,-8.25 0.006,-0.0114 0.0257,-0.0198 0.0312,-0.0312 0.004,-0.009 -0.004,-0.0225 0,-0.0312 0.55593,-1.14255 1.34353,-2.64437 1.8125,-3.71875 2.07213,-4.74702 2.76161,-8.81506 4.21875,-13.40625 3.86962,-9.72014 5.99567,-19.91903 11.32258,-26.27411 1.45868,-1.74023 3.83681,-2.4095 6.30242,-3.06964 l 3.3125,-6 c 33.93824,13.0268 71.92666,16.52246 109.875,7.90625 8.65697,-1.96557 17.01444,-4.50945 25.09375,-7.5625 0.93098,1.65133 2.66113,4.8257 3.125,5.625 2.50559,0.81518 5.24044,1.23614 7.46875,4.53125 3.98539,6.80898 6.7109,14.86416 10.03125,24.59375 1.45738,4.59111 2.17762,8.65933 4.25,13.40625 0.47234,1.08195 1.256,2.60486 1.8125,3.75 3.89482,8.08484 12.94212,11.78667 20.21875,8.28125 7.2195,-3.4779 9.97974,-12.7399 6.21875,-20.78125 -0.55889,-1.15814 -1.3273,-2.84164 -1.90625,-3.9375 -2.41946,-4.57976 -5.1627,-7.65448 -7.84375,-11.65625 -5.53721,-8.66192 -10.12968,-15.8577 -12.96875,-23.21875 -1.18711,-3.79657 0.20028,-6.15774 1.125,-8.625 -0.55378,-0.63477 -1.73881,-4.22009 -2.4375,-5.90625 40.4574,-23.88816 70.29856,-62.02129 84.3125,-106.0625 1.8924,0.29742 5.18154,0.87936 6.25,1.09375 2.19954,-1.4507 4.22194,-3.34352 8.1875,-3.03125 7.80832,1.12937 15.82288,4.02973 25.5,7.5 4.49815,1.72306 8.1216,3.72313 13.125,5.0625 1.05749,0.28309 2.57238,0.5472 3.78125,0.8125 0.0972,0.0238 0.1833,0.0714 0.28125,0.0937 0.0653,0.0146 0.15615,0.0169 0.21875,0.0312 8.67236,1.86695 17.09384,-2.87871 18.875,-10.6875 1.78074,-7.80696 -3.7543,-15.73201 -12.375,-17.8125 -1.25393,-0.28513 -3.03225,-0.76938 -4.25,-1 -5.08912,-0.96378 -9.23092,-0.7261 -14.03125,-1.125 -10.22456,-1.07138 -18.6935,-1.94269 -26.21875,-4.3125 -3.06826,-1.19028 -5.25103,-4.84124 -6.31255,-6.34375 l -5.90625,-1.71875 c 3.06226,-22.15442 2.23655,-45.21134 -3.0625,-68.28125 -5.34839,-23.28471 -14.80037,-44.58084 -27.40625,-63.34375 1.51505,-1.37729 4.37619,-3.91091 5.1875,-4.65625 0.23716,-2.62417 0.0334,-5.37553 2.75,-8.28125 5.75134,-5.40069 13.00329,-9.87898 21.75,-15.28125 4.15167,-2.44252 7.98954,-4.01698 12.15625,-7.09375 0.94225,-0.69576 2.2289,-1.79759 3.21875,-2.59375 7.01538,-5.59633 8.63058,-15.24842 3.59375,-21.5625 -5.03683,-6.31408 -14.79712,-6.90883 -21.8125,-1.3125 -0.99856,0.79085 -2.35353,1.82252 -3.25,2.59375 -3.9265,3.37796 -6.35145,6.71439 -9.65625,10.21875 -7.21249,7.32595 -13.17407,13.43777 -19.71875,17.84375 -2.83601,1.65106 -6.98996,1.07978 -8.87505,0.96875 l -5.5625,3.96875 c -31.7188,-33.26057 -74.90466,-54.52546 -121.40605,-58.6563 -0.13006,-1.94872 -0.30045,-5.47117 -0.34375,-6.53125 -1.90371,-1.82165 -4.20342,-3.37686 -4.78125,-7.3125 -0.63617,-7.86389 0.42597,-16.32459 1.65625,-26.53125 0.6791,-4.76876 1.80738,-8.73025 2,-13.90625 0.0438,-1.17663 -0.0265,-2.88401 -0.0312,-4.15625 -9.6e-4,-8.97409 -6.54804,-16.25082 -14.625,-16.25 z m -18.3125,113.4375 -4.34375,76.71875 -0.3125,0.15625 c -0.29134,6.86335 -5.93996,12.34375 -12.875,12.34375 -2.84081,0 -5.46294,-0.91229 -7.59375,-2.46875 l -0.125,0.0625 -62.90625,-44.59375 c 19.33365,-19.01115 44.06291,-33.06039 72.5625,-39.53125 5.20599,-1.18203 10.40966,-2.0591 15.59375,-2.6875 z m 36.65625,0 c 33.27347,4.09232 64.04501,19.15882 87.625,42.25 l -62.5,44.3125 -0.21875,-0.0937 c -5.54745,4.05169 -13.36343,3.04639 -17.6875,-2.375 -1.77132,-2.22096 -2.70072,-4.83239 -2.8125,-7.46875 l -0.0625,-0.0312 z m -147.625,70.875 57.4375,51.375 -0.0625,0.3125 c 5.18437,4.50697 5.94888,12.32794 1.625,17.75 -1.7712,2.22105 -4.14208,3.71074 -6.6875,4.40625 l -0.0625,0.25 -73.625,21.25 c -3.74728,-34.26517 4.32855,-67.57364 21.375,-95.34375 z m 258.15625,0.0312 c 8.5341,13.83256 14.99655,29.28214 18.84375,46.03125 3.80106,16.54828 4.75499,33.06697 3.1875,49.03125 l -74,-21.3125 -0.0625,-0.3125 c -6.6265,-1.81104 -10.69893,-8.55162 -9.15625,-15.3125 0.63203,-2.76962 2.10222,-5.11264 4.09375,-6.84375 l -0.0312,-0.15625 57.125,-51.125 z m -140.65625,55.3125 23.53125,0 14.625,18.28125 -5.25,22.8125 -21.125,10.15625 -21.1875,-10.1875 -5.25,-22.8125 z m 75.4375,62.5625 c 0.99997,-0.0505 1.99558,0.0396 2.96875,0.21875 l 0.125,-0.15625 76.15625,12.875 c -11.1455,31.3131 -32.47281,58.44018 -60.96875,76.59375 l -29.5625,-71.40625 0.0937,-0.125 c -2.71561,-6.30999 0.002,-13.70956 6.25,-16.71875 1.59965,-0.77041 3.27089,-1.19701 4.9375,-1.28125 z m -127.90625,0.3125 c 5.81174,0.0815 11.02462,4.11525 12.375,10.03125 0.63219,2.76958 0.3245,5.51375 -0.71875,7.9375 l 0.21875,0.28125 -29.25,70.6875 c -27.34716,-17.5486 -49.12927,-43.82403 -60.78125,-76.06245 l 75.5,-12.8125 0.125,0.15625 c 0.84451,-0.15541 1.701,-0.2304 2.53125,-0.21875 z m 63.78125,30.9688 c 2.02445,-0.0744 4.07865,0.34098 6.03125,1.28125 2.55951,1.23253 4.53673,3.17319 5.78125,5.5 l 0.28125,0 37.21875,67.25 c -4.83029,1.61923 -9.79609,3.00308 -14.875,4.15625 -28.46453,6.4629 -56.83862,4.50467 -82.53125,-4.25 l 37.125,-67.125 0.0625,0 c 2.22767,-4.16441 6.45247,-6.64887 10.90625,-6.8125 z"
         style="font-size:medium;font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;text-indent:0;text-align:start;text-decoration:none;line-height:normal;letter-spacing:normal;word-spacing:normal;text-transform:none;direction:ltr;block-progression:tb;writing-mode:lr-tb;text-anchor:start;baseline-shift:baseline;color:#000000;fill:#ffffff;fill-opacity:1;stroke:#ffffff;stroke-width:0.25;stroke-miterlimit:4;stroke-opacity:1;stroke-dasharray:none;marker:none;visibility:visible;display:inline;overflow:visible;enable-background:accumulate;font-family:Sans;-inkscape-font-specification:Sans"
         sodipodi:nodetypes="ccccccccsccccscssccsccccccccscccsccccccccccccccscccscsccsccccscscsccccccccscccscsccccsccccscscscccccccccccccccscccsccccccccccccscccccscccccccccccccccccccccccscccscccccccccscccscccc"
         inkscape:export-filename="./path3059.png"
         inkscape:export-xdpi="250.55"
         inkscape:export-ydpi="250.55" />
    </g>
  </g>
</svg>
</file>

<file path="kube-scheduler-simulator/web/static/node.svg">
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!-- Created with Inkscape (http://www.inkscape.org/) -->

<svg
   xmlns:dc="http://purl.org/dc/elements/1.1/"
   xmlns:cc="http://creativecommons.org/ns#"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:svg="http://www.w3.org/2000/svg"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   width="18.035334mm"
   height="17.500378mm"
   viewBox="0 0 18.035334 17.500378"
   version="1.1"
   id="svg13826"
   inkscape:version="0.91 r13725"
   sodipodi:docname="node.svg">
  <defs
     id="defs13820" />
  <sodipodi:namedview
     id="base"
     pagecolor="#ffffff"
     bordercolor="#666666"
     borderopacity="1.0"
     inkscape:pageopacity="0.0"
     inkscape:pageshadow="2"
     inkscape:zoom="8"
     inkscape:cx="16.847496"
     inkscape:cy="33.752239"
     inkscape:document-units="mm"
     inkscape:current-layer="layer1"
     showgrid="false"
     inkscape:window-width="1440"
     inkscape:window-height="775"
     inkscape:window-x="0"
     inkscape:window-y="1"
     inkscape:window-maximized="1"
     fit-margin-top="0"
     fit-margin-left="0"
     fit-margin-right="0"
     fit-margin-bottom="0" />
  <metadata
     id="metadata13823">
    <rdf:RDF>
      <cc:Work
         rdf:about="">
        <dc:format>image/svg+xml</dc:format>
        <dc:type
           rdf:resource="http://purl.org/dc/dcmitype/StillImage" />
        <dc:title />
      </cc:Work>
    </rdf:RDF>
  </metadata>
  <g
     inkscape:label="Calque 1"
     inkscape:groupmode="layer"
     id="layer1"
     transform="translate(-0.99262638,-1.174181)">
    <g
       id="g70"
       transform="matrix(1.0148887,0,0,1.0148887,16.902146,-2.698726)">
      <path
         inkscape:export-ydpi="250.55"
         inkscape:export-xdpi="250.55"
         inkscape:export-filename="new.png"
         inkscape:connector-curvature="0"
         id="path3055"
         d="m -6.8492015,4.2724668 a 1.1191255,1.1099671 0 0 0 -0.4288818,0.1085303 l -5.8524037,2.7963394 a 1.1191255,1.1099671 0 0 0 -0.605524,0.7529759 l -1.443828,6.2812846 a 1.1191255,1.1099671 0 0 0 0.151943,0.851028 1.1191255,1.1099671 0 0 0 0.06362,0.08832 l 4.0508,5.036555 a 1.1191255,1.1099671 0 0 0 0.874979,0.417654 l 6.4961011,-0.0015 a 1.1191255,1.1099671 0 0 0 0.8749788,-0.416906 L 1.3818872,15.149453 A 1.1191255,1.1099671 0 0 0 1.5981986,14.210104 L 0.15212657,7.9288154 A 1.1191255,1.1099671 0 0 0 -0.45339794,7.1758396 L -6.3065496,4.3809971 A 1.1191255,1.1099671 0 0 0 -6.8492015,4.2724668 Z"
         style="fill:#326ce5;fill-opacity:1;stroke:none;stroke-width:0;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1" />
      <path
         id="path3054-2-9"
         d="M -6.8523435,3.8176372 A 1.1814304,1.171762 0 0 0 -7.3044284,3.932904 l -6.1787426,2.9512758 a 1.1814304,1.171762 0 0 0 -0.639206,0.794891 l -1.523915,6.6308282 a 1.1814304,1.171762 0 0 0 0.160175,0.89893 1.1814304,1.171762 0 0 0 0.06736,0.09281 l 4.276094,5.317236 a 1.1814304,1.171762 0 0 0 0.92363,0.440858 l 6.8576188,-0.0015 a 1.1814304,1.171762 0 0 0 0.9236308,-0.44011 l 4.2745966,-5.317985 a 1.1814304,1.171762 0 0 0 0.228288,-0.990993 L 0.53894439,7.6775738 A 1.1814304,1.171762 0 0 0 -0.10026101,6.8834313 L -6.2790037,3.9321555 A 1.1814304,1.171762 0 0 0 -6.8523435,3.8176372 Z m 0.00299,0.4550789 a 1.1191255,1.1099671 0 0 1 0.5426517,0.1085303 l 5.85315169,2.7948425 A 1.1191255,1.1099671 0 0 1 0.15197811,7.9290648 L 1.598051,14.21035 a 1.1191255,1.1099671 0 0 1 -0.2163123,0.939348 l -4.0493032,5.037304 a 1.1191255,1.1099671 0 0 1 -0.8749789,0.416906 l -6.4961006,0.0015 a 1.1191255,1.1099671 0 0 1 -0.874979,-0.417652 l -4.0508,-5.036554 a 1.1191255,1.1099671 0 0 1 -0.06362,-0.08832 1.1191255,1.1099671 0 0 1 -0.151942,-0.851028 l 1.443827,-6.2812853 a 1.1191255,1.1099671 0 0 1 0.605524,-0.7529758 l 5.8524036,-2.7963395 a 1.1191255,1.1099671 0 0 1 0.4288819,-0.1085303 z"
         style="color:#000000;font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:medium;line-height:normal;font-family:Sans;-inkscape-font-specification:Sans;text-indent:0;text-align:start;text-decoration:none;text-decoration-line:none;letter-spacing:normal;word-spacing:normal;text-transform:none;writing-mode:lr-tb;direction:ltr;baseline-shift:baseline;text-anchor:start;display:inline;overflow:visible;visibility:visible;fill:#ffffff;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0;stroke-miterlimit:4;stroke-dasharray:none;marker:none;enable-background:accumulate"
         inkscape:connector-curvature="0" />
    </g>
    <path
       sodipodi:nodetypes="cccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccscccccccccccccccc"
       id="path1994"
       d="M 9.9921803,5.0101759 C 9.8538505,5.0057759 5.9970805,6.902049 5.9466504,6.999295 c -0.12117,0.233682 -0.9989,4.281827 -0.94731,4.369074 0.03,0.05065 0.66219,0.851861 1.40458,1.780327 l 1.3498201,1.688014 2.2211901,9.31e-4 2.2216404,9.31e-4 1.41321,-1.765731 1.41365,-1.765228 -0.49479,-2.1685759 C 14.256441,7.946271 14.012981,6.950423 13.988111,6.926433 13.918611,6.859553 10.067151,5.0126389 9.9921803,5.0102179 Z m 0.1961407,0.947753 0.90893,0.2635771 -0.90893,0.263576 -0.9089209,-0.263576 z m -0.9089209,0.36452 0.8511209,0.2532261 -0.004,1.183289 -0.8468109,-0.469347 z m 1.8178509,0 0,0.9671681 -0.84679,0.469347 -0.004,-1.183289 z M 8.8997705,7.937127 9.8087101,8.2007027 8.8997705,8.4642797 7.9908504,8.2007027 Z m 2.2087005,0 0.90894,0.2635757 -0.90894,0.263577 -0.90893,-0.263577 z m -3.1176206,0.3645197 0.8511202,0.252792 -0.004,1.1832908 -0.8468199,-0.468915 z m 1.8178597,0 0,0.9671678 -0.8468098,0.468915 -0.004,-1.1832908 z m 0.3908309,0 0.85113,0.252792 -0.004,1.1832908 -0.84682,-0.468915 z m 1.81787,0 0,0.9671678 -0.84682,0.468915 -0.004,-1.1832908 z m -3.4496605,1.515028 c 0.2706299,0.0096 0.0611,0.2819093 0.3684101,0.4279353 0.3277495,0.155764 0.3953995,-0.235354 0.6013395,0.06341 0.20599,0.298766 -0.18339,0.223239 -0.15443,0.584957 0.029,0.361718 0.40165,0.224858 0.24589,0.552606 -0.15575,0.327746 -0.28532,-0.04764 -0.5840895,0.158317 -0.2987401,0.205957 0.006,0.460208 -0.35546,0.489192 -0.3617401,0.02898 -0.1015001,-0.270447 -0.42924,-0.426208 -0.32775,-0.155765 -0.3953801,0.234921 -0.6013402,-0.06385 -0.2059599,-0.298767 0.1838299,-0.22281 0.15485,-0.584528 -0.029,-0.361718 -0.4016499,-0.224859 -0.24587,-0.552604 0.1557501,-0.327749 0.2848801,0.04764 0.5836502,-0.15832 0.2987898,-0.205956 -0.006,-0.4602083 0.3559099,-0.4891903 0.022499,-0.0018 0.0424,-0.0023 0.0604,-0.0018 z m 2.3359605,0.3627943 c 0.48335,0.01358 0.0146,0.467218 0.45596,0.664766 0.44144,0.197548 0.46714,-0.454103 0.79937,-0.102669 0.33221,0.351432 -0.31997,0.34064 -0.14753,0.792455 0.17243,0.451813 0.65163,0.0092 0.63802,0.49264 -0.0137,0.483411 -0.46723,0.01456 -0.66477,0.455977 -0.19755,0.441412 0.4541,0.467143 0.10266,0.799357 -0.35141,0.332212 -0.34021,-0.319974 -0.79202,-0.147534 -0.45183,0.172437 -0.009,0.65161 -0.49265,0.638019 -0.48339,-0.01358 -0.0146,-0.467216 -0.45596,-0.664764 -0.4414105,-0.197551 -0.4675805,0.454102 -0.7997909,0.102669 -0.3322097,-0.351431 0.3199804,-0.340209 0.14754,-0.792025 -0.17245,-0.451815 -0.6516296,-0.0092 -0.6380295,-0.492642 0.013699,-0.483408 0.4672095,-0.01499 0.6647795,-0.456405 0.1975204,-0.441414 -0.45411,-0.467143 -0.10269,-0.799357 0.3514505,-0.332213 0.3406505,0.319971 0.7924609,0.147534 0.45184,-0.17244 0.009,-0.651611 0.49265,-0.638021 z m -2.3148207,0.253655 c -0.2936499,9e-5 -0.5316098,0.238249 -0.5314498,0.531898 7.99e-5,0.293481 0.2379701,0.531377 0.5314498,0.531467 0.2936602,1.59e-4 0.5318202,-0.23781 0.5319,-0.531467 1.601e-4,-0.293825 -0.2380699,-0.532057 -0.5319,-0.531898 z m 2.2643607,0.480564 c -0.58689,-1.96e-4 -1.0627109,0.475614 -1.0625209,1.062502 5e-5,0.586719 0.4758009,1.062267 1.0625209,1.062071 0.58654,-5.8e-5 1.06201,-0.475531 1.06206,-1.062071 1.9e-4,-0.586708 -0.47535,-1.062444 -1.06206,-1.062502 z"
       style="opacity:1;fill:#ffffff;fill-opacity:1;stroke:#eeeeee;stroke-width:0;stroke-miterlimit:10;stroke-dasharray:none;stroke-dashoffset:11.23642349;stroke-opacity:1"
       inkscape:connector-curvature="0" />
  </g>
</svg>
</file>

<file path="kube-scheduler-simulator/web/static/pod.svg">
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!-- Created with Inkscape (http://www.inkscape.org/) -->

<svg
   xmlns:dc="http://purl.org/dc/elements/1.1/"
   xmlns:cc="http://creativecommons.org/ns#"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:svg="http://www.w3.org/2000/svg"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   width="18.035334mm"
   height="17.500378mm"
   viewBox="0 0 18.035334 17.500378"
   version="1.1"
   id="svg13826"
   inkscape:version="0.91 r13725"
   sodipodi:docname="pod.svg">
  <defs
     id="defs13820" />
  <sodipodi:namedview
     id="base"
     pagecolor="#ffffff"
     bordercolor="#666666"
     borderopacity="1.0"
     inkscape:pageopacity="0.0"
     inkscape:pageshadow="2"
     inkscape:zoom="8"
     inkscape:cx="-2.090004"
     inkscape:cy="33.752239"
     inkscape:document-units="mm"
     inkscape:current-layer="layer1"
     showgrid="false"
     inkscape:window-width="1440"
     inkscape:window-height="775"
     inkscape:window-x="0"
     inkscape:window-y="1"
     inkscape:window-maximized="1"
     fit-margin-top="0"
     fit-margin-left="0"
     fit-margin-right="0"
     fit-margin-bottom="0" />
  <metadata
     id="metadata13823">
    <rdf:RDF>
      <cc:Work
         rdf:about="">
        <dc:format>image/svg+xml</dc:format>
        <dc:type
           rdf:resource="http://purl.org/dc/dcmitype/StillImage" />
        <dc:title />
      </cc:Work>
    </rdf:RDF>
  </metadata>
  <g
     inkscape:label="Calque 1"
     inkscape:groupmode="layer"
     id="layer1"
     transform="translate(-0.99262638,-1.174181)">
    <g
       id="g70"
       transform="matrix(1.0148887,0,0,1.0148887,16.902146,-2.698726)">
      <path
         inkscape:export-ydpi="250.55"
         inkscape:export-xdpi="250.55"
         inkscape:export-filename="new.png"
         inkscape:connector-curvature="0"
         id="path3055"
         d="m -6.8492015,4.2724668 a 1.1191255,1.1099671 0 0 0 -0.4288818,0.1085303 l -5.8524037,2.7963394 a 1.1191255,1.1099671 0 0 0 -0.605524,0.7529759 l -1.443828,6.2812846 a 1.1191255,1.1099671 0 0 0 0.151943,0.851028 1.1191255,1.1099671 0 0 0 0.06362,0.08832 l 4.0508,5.036555 a 1.1191255,1.1099671 0 0 0 0.874979,0.417654 l 6.4961011,-0.0015 a 1.1191255,1.1099671 0 0 0 0.8749788,-0.416906 L 1.3818872,15.149453 A 1.1191255,1.1099671 0 0 0 1.5981986,14.210104 L 0.15212657,7.9288154 A 1.1191255,1.1099671 0 0 0 -0.45339794,7.1758396 L -6.3065496,4.3809971 A 1.1191255,1.1099671 0 0 0 -6.8492015,4.2724668 Z"
         style="fill:#326ce5;fill-opacity:1;stroke:none;stroke-width:0;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1" />
      <path
         id="path3054-2-9"
         d="M -6.8523435,3.8176372 A 1.1814304,1.171762 0 0 0 -7.3044284,3.932904 l -6.1787426,2.9512758 a 1.1814304,1.171762 0 0 0 -0.639206,0.794891 l -1.523915,6.6308282 a 1.1814304,1.171762 0 0 0 0.160175,0.89893 1.1814304,1.171762 0 0 0 0.06736,0.09281 l 4.276094,5.317236 a 1.1814304,1.171762 0 0 0 0.92363,0.440858 l 6.8576188,-0.0015 a 1.1814304,1.171762 0 0 0 0.9236308,-0.44011 l 4.2745966,-5.317985 a 1.1814304,1.171762 0 0 0 0.228288,-0.990993 L 0.53894439,7.6775738 A 1.1814304,1.171762 0 0 0 -0.10026101,6.8834313 L -6.2790037,3.9321555 A 1.1814304,1.171762 0 0 0 -6.8523435,3.8176372 Z m 0.00299,0.4550789 a 1.1191255,1.1099671 0 0 1 0.5426517,0.1085303 l 5.85315169,2.7948425 A 1.1191255,1.1099671 0 0 1 0.15197811,7.9290648 L 1.598051,14.21035 a 1.1191255,1.1099671 0 0 1 -0.2163123,0.939348 l -4.0493032,5.037304 a 1.1191255,1.1099671 0 0 1 -0.8749789,0.416906 l -6.4961006,0.0015 a 1.1191255,1.1099671 0 0 1 -0.874979,-0.417652 l -4.0508,-5.036554 a 1.1191255,1.1099671 0 0 1 -0.06362,-0.08832 1.1191255,1.1099671 0 0 1 -0.151942,-0.851028 l 1.443827,-6.2812853 a 1.1191255,1.1099671 0 0 1 0.605524,-0.7529758 l 5.8524036,-2.7963395 a 1.1191255,1.1099671 0 0 1 0.4288819,-0.1085303 z"
         style="color:#000000;font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:medium;line-height:normal;font-family:Sans;-inkscape-font-specification:Sans;text-indent:0;text-align:start;text-decoration:none;text-decoration-line:none;letter-spacing:normal;word-spacing:normal;text-transform:none;writing-mode:lr-tb;direction:ltr;baseline-shift:baseline;text-anchor:start;display:inline;overflow:visible;visibility:visible;fill:#ffffff;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0;stroke-miterlimit:4;stroke-dasharray:none;marker:none;enable-background:accumulate"
         inkscape:connector-curvature="0" />
    </g>
    <g
       id="g3341"
       transform="translate(0.12766661,0.35147801)">
      <path
         inkscape:export-ydpi="376.57999"
         inkscape:export-xdpi="376.57999"
         style="fill:#ffffff;fill-rule:evenodd;stroke:none;stroke-width:0.26458332;stroke-linecap:square;stroke-miterlimit:10"
         inkscape:connector-curvature="0"
         d="M 6.2617914,7.036086 9.8826317,5.986087 13.503462,7.036086 9.8826317,8.086087 Z"
         id="path910" />
      <path
         inkscape:export-ydpi="376.57999"
         inkscape:export-xdpi="376.57999"
         style="fill:#ffffff;fill-rule:evenodd;stroke:none;stroke-width:0.26458332;stroke-linecap:square;stroke-miterlimit:10"
         inkscape:connector-curvature="0"
         d="m 6.2617914,7.43817 0,3.852778 3.3736103,1.868749 0.0167,-4.713193 z"
         id="path912" />
      <path
         inkscape:export-ydpi="376.57999"
         inkscape:export-xdpi="376.57999"
         style="fill:#ffffff;fill-rule:evenodd;stroke:none;stroke-width:0.26458332;stroke-linecap:square;stroke-miterlimit:10"
         inkscape:connector-curvature="0"
         d="m 13.503462,7.43817 0,3.852778 -3.37361,1.868749 -0.0167,-4.713193 z"
         id="path914" />
    </g>
  </g>
</svg>
</file>

<file path="kube-scheduler-simulator/web/static/pv.svg">
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!-- Created with Inkscape (http://www.inkscape.org/) -->

<svg
   xmlns:dc="http://purl.org/dc/elements/1.1/"
   xmlns:cc="http://creativecommons.org/ns#"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:svg="http://www.w3.org/2000/svg"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   width="18.035334mm"
   height="17.500378mm"
   viewBox="0 0 18.035334 17.500378"
   version="1.1"
   id="svg13826"
   inkscape:version="0.91 r13725"
   sodipodi:docname="pv.svg">
  <defs
     id="defs13820" />
  <sodipodi:namedview
     id="base"
     pagecolor="#ffffff"
     bordercolor="#666666"
     borderopacity="1.0"
     inkscape:pageopacity="0.0"
     inkscape:pageshadow="2"
     inkscape:zoom="8"
     inkscape:cx="-12.090004"
     inkscape:cy="28.752239"
     inkscape:document-units="mm"
     inkscape:current-layer="layer1"
     showgrid="false"
     inkscape:window-width="1440"
     inkscape:window-height="775"
     inkscape:window-x="0"
     inkscape:window-y="1"
     inkscape:window-maximized="1"
     fit-margin-top="0"
     fit-margin-left="0"
     fit-margin-right="0"
     fit-margin-bottom="0" />
  <metadata
     id="metadata13823">
    <rdf:RDF>
      <cc:Work
         rdf:about="">
        <dc:format>image/svg+xml</dc:format>
        <dc:type
           rdf:resource="http://purl.org/dc/dcmitype/StillImage" />
        <dc:title />
      </cc:Work>
    </rdf:RDF>
  </metadata>
  <g
     inkscape:label="Calque 1"
     inkscape:groupmode="layer"
     id="layer1"
     transform="translate(-0.99262638,-1.174181)">
    <g
       id="g70"
       transform="matrix(1.0148887,0,0,1.0148887,16.902146,-2.698726)">
      <path
         inkscape:export-ydpi="250.55"
         inkscape:export-xdpi="250.55"
         inkscape:export-filename="new.png"
         inkscape:connector-curvature="0"
         id="path3055"
         d="m -6.8492015,4.2724668 a 1.1191255,1.1099671 0 0 0 -0.4288818,0.1085303 l -5.8524037,2.7963394 a 1.1191255,1.1099671 0 0 0 -0.605524,0.7529759 l -1.443828,6.2812846 a 1.1191255,1.1099671 0 0 0 0.151943,0.851028 1.1191255,1.1099671 0 0 0 0.06362,0.08832 l 4.0508,5.036555 a 1.1191255,1.1099671 0 0 0 0.874979,0.417654 l 6.4961011,-0.0015 a 1.1191255,1.1099671 0 0 0 0.8749788,-0.416906 L 1.3818872,15.149453 A 1.1191255,1.1099671 0 0 0 1.5981986,14.210104 L 0.15212657,7.9288154 A 1.1191255,1.1099671 0 0 0 -0.45339794,7.1758396 L -6.3065496,4.3809971 A 1.1191255,1.1099671 0 0 0 -6.8492015,4.2724668 Z"
         style="fill:#326ce5;fill-opacity:1;stroke:none;stroke-width:0;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1" />
      <path
         id="path3054-2-9"
         d="M -6.8523435,3.8176372 A 1.1814304,1.171762 0 0 0 -7.3044284,3.932904 l -6.1787426,2.9512758 a 1.1814304,1.171762 0 0 0 -0.639206,0.794891 l -1.523915,6.6308282 a 1.1814304,1.171762 0 0 0 0.160175,0.89893 1.1814304,1.171762 0 0 0 0.06736,0.09281 l 4.276094,5.317236 a 1.1814304,1.171762 0 0 0 0.92363,0.440858 l 6.8576188,-0.0015 a 1.1814304,1.171762 0 0 0 0.9236308,-0.44011 l 4.2745966,-5.317985 a 1.1814304,1.171762 0 0 0 0.228288,-0.990993 L 0.53894439,7.6775738 A 1.1814304,1.171762 0 0 0 -0.10026101,6.8834313 L -6.2790037,3.9321555 A 1.1814304,1.171762 0 0 0 -6.8523435,3.8176372 Z m 0.00299,0.4550789 a 1.1191255,1.1099671 0 0 1 0.5426517,0.1085303 l 5.85315169,2.7948425 A 1.1191255,1.1099671 0 0 1 0.15197811,7.9290648 L 1.598051,14.21035 a 1.1191255,1.1099671 0 0 1 -0.2163123,0.939348 l -4.0493032,5.037304 a 1.1191255,1.1099671 0 0 1 -0.8749789,0.416906 l -6.4961006,0.0015 a 1.1191255,1.1099671 0 0 1 -0.874979,-0.417652 l -4.0508,-5.036554 a 1.1191255,1.1099671 0 0 1 -0.06362,-0.08832 1.1191255,1.1099671 0 0 1 -0.151942,-0.851028 l 1.443827,-6.2812853 a 1.1191255,1.1099671 0 0 1 0.605524,-0.7529758 l 5.8524036,-2.7963395 a 1.1191255,1.1099671 0 0 1 0.4288819,-0.1085303 z"
         style="color:#000000;font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:medium;line-height:normal;font-family:Sans;-inkscape-font-specification:Sans;text-indent:0;text-align:start;text-decoration:none;text-decoration-line:none;letter-spacing:normal;word-spacing:normal;text-transform:none;writing-mode:lr-tb;direction:ltr;baseline-shift:baseline;text-anchor:start;display:inline;overflow:visible;visibility:visible;fill:#ffffff;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0;stroke-miterlimit:4;stroke-dasharray:none;marker:none;enable-background:accumulate"
         inkscape:connector-curvature="0" />
    </g>
    <g
       id="g3341"
       transform="translate(-0.18983289,0.49258906)">
      <path
         style="fill:#ffffff;fill-rule:evenodd;stroke:none;stroke-width:0.26458332;stroke-linecap:square;stroke-miterlimit:10"
         inkscape:connector-curvature="0"
         d="m 5.5709614,7.9105849 0,0 c 0,0.621121 2.0725401,1.124639 4.6291706,1.124639 2.556609,0 4.629159,-0.503518 4.629159,-1.124639 l 0,3.0423911 c 0,0.62112 -2.07255,1.124638 -4.629159,1.124638 -2.5566305,0 -4.6291706,-0.503518 -4.6291706,-1.124638 z"
         id="path1114" />
      <path
         style="fill:#ffffff;fill-opacity:1;fill-rule:evenodd;stroke:none;stroke-width:0.26458332;stroke-linecap:square;stroke-miterlimit:10"
         inkscape:connector-curvature="0"
         d="m 5.5709614,7.9105849 0,0 c 0,-0.621119 2.0725401,-1.124637 4.6291706,-1.124637 2.556609,0 4.629159,0.503518 4.629159,1.124637 l 0,0 c 0,0.621121 -2.07255,1.124639 -4.629159,1.124639 -2.5566305,0 -4.6291706,-0.503518 -4.6291706,-1.124639 z"
         id="path1116" />
      <path
         style="fill:none;fill-rule:evenodd;stroke:#326ce5;stroke-width:0.26458332;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:10;stroke-opacity:1"
         inkscape:connector-curvature="0"
         d="m 14.829291,7.9105849 0,0 c 0,0.621121 -2.07255,1.124639 -4.629159,1.124639 -2.5566205,0 -4.6291706,-0.503518 -4.6291706,-1.124639 l 0,0 c 0,-0.621119 2.0725501,-1.124637 4.6291706,-1.124637 2.556609,0 4.629159,0.503518 4.629159,1.124637 l 0,3.0423911 c 0,0.62112 -2.07255,1.124638 -4.629159,1.124638 -2.5566205,0 -4.6291706,-0.503518 -4.6291706,-1.124638 l 0,-3.0423911"
         id="path1120" />
    </g>
  </g>
</svg>
</file>

<file path="kube-scheduler-simulator/web/static/pvc.svg">
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!-- Created with Inkscape (http://www.inkscape.org/) -->

<svg
   xmlns:dc="http://purl.org/dc/elements/1.1/"
   xmlns:cc="http://creativecommons.org/ns#"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:svg="http://www.w3.org/2000/svg"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   width="18.035334mm"
   height="17.500378mm"
   viewBox="0 0 18.035334 17.500378"
   version="1.1"
   id="svg13826"
   inkscape:version="0.91 r13725"
   sodipodi:docname="pvc.svg">
  <defs
     id="defs13820" />
  <sodipodi:namedview
     id="base"
     pagecolor="#ffffff"
     bordercolor="#666666"
     borderopacity="1.0"
     inkscape:pageopacity="0.0"
     inkscape:pageshadow="2"
     inkscape:zoom="11.313708"
     inkscape:cx="7.8215027"
     inkscape:cy="25.692001"
     inkscape:document-units="mm"
     inkscape:current-layer="layer1"
     showgrid="false"
     inkscape:window-width="1440"
     inkscape:window-height="775"
     inkscape:window-x="0"
     inkscape:window-y="1"
     inkscape:window-maximized="1"
     fit-margin-top="0"
     fit-margin-left="0"
     fit-margin-right="0"
     fit-margin-bottom="0" />
  <metadata
     id="metadata13823">
    <rdf:RDF>
      <cc:Work
         rdf:about="">
        <dc:format>image/svg+xml</dc:format>
        <dc:type
           rdf:resource="http://purl.org/dc/dcmitype/StillImage" />
        <dc:title />
      </cc:Work>
    </rdf:RDF>
  </metadata>
  <g
     inkscape:label="Calque 1"
     inkscape:groupmode="layer"
     id="layer1"
     transform="translate(-0.99262638,-1.174181)">
    <g
       id="g70"
       transform="matrix(1.0148887,0,0,1.0148887,16.902146,-2.698726)">
      <path
         inkscape:export-ydpi="250.55"
         inkscape:export-xdpi="250.55"
         inkscape:export-filename="new.png"
         inkscape:connector-curvature="0"
         id="path3055"
         d="m -6.8492015,4.2724668 a 1.1191255,1.1099671 0 0 0 -0.4288818,0.1085303 l -5.8524037,2.7963394 a 1.1191255,1.1099671 0 0 0 -0.605524,0.7529759 l -1.443828,6.2812846 a 1.1191255,1.1099671 0 0 0 0.151943,0.851028 1.1191255,1.1099671 0 0 0 0.06362,0.08832 l 4.0508,5.036555 a 1.1191255,1.1099671 0 0 0 0.874979,0.417654 l 6.4961011,-0.0015 a 1.1191255,1.1099671 0 0 0 0.8749788,-0.416906 L 1.3818872,15.149453 A 1.1191255,1.1099671 0 0 0 1.5981986,14.210104 L 0.15212657,7.9288154 A 1.1191255,1.1099671 0 0 0 -0.45339794,7.1758396 L -6.3065496,4.3809971 A 1.1191255,1.1099671 0 0 0 -6.8492015,4.2724668 Z"
         style="fill:#326ce5;fill-opacity:1;stroke:none;stroke-width:0;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1" />
      <path
         id="path3054-2-9"
         d="M -6.8523435,3.8176372 A 1.1814304,1.171762 0 0 0 -7.3044284,3.932904 l -6.1787426,2.9512758 a 1.1814304,1.171762 0 0 0 -0.639206,0.794891 l -1.523915,6.6308282 a 1.1814304,1.171762 0 0 0 0.160175,0.89893 1.1814304,1.171762 0 0 0 0.06736,0.09281 l 4.276094,5.317236 a 1.1814304,1.171762 0 0 0 0.92363,0.440858 l 6.8576188,-0.0015 a 1.1814304,1.171762 0 0 0 0.9236308,-0.44011 l 4.2745966,-5.317985 a 1.1814304,1.171762 0 0 0 0.228288,-0.990993 L 0.53894439,7.6775738 A 1.1814304,1.171762 0 0 0 -0.10026101,6.8834313 L -6.2790037,3.9321555 A 1.1814304,1.171762 0 0 0 -6.8523435,3.8176372 Z m 0.00299,0.4550789 a 1.1191255,1.1099671 0 0 1 0.5426517,0.1085303 l 5.85315169,2.7948425 A 1.1191255,1.1099671 0 0 1 0.15197811,7.9290648 L 1.598051,14.21035 a 1.1191255,1.1099671 0 0 1 -0.2163123,0.939348 l -4.0493032,5.037304 a 1.1191255,1.1099671 0 0 1 -0.8749789,0.416906 l -6.4961006,0.0015 a 1.1191255,1.1099671 0 0 1 -0.874979,-0.417652 l -4.0508,-5.036554 a 1.1191255,1.1099671 0 0 1 -0.06362,-0.08832 1.1191255,1.1099671 0 0 1 -0.151942,-0.851028 l 1.443827,-6.2812853 a 1.1191255,1.1099671 0 0 1 0.605524,-0.7529758 l 5.8524036,-2.7963395 a 1.1191255,1.1099671 0 0 1 0.4288819,-0.1085303 z"
         style="color:#000000;font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:medium;line-height:normal;font-family:Sans;-inkscape-font-specification:Sans;text-indent:0;text-align:start;text-decoration:none;text-decoration-line:none;letter-spacing:normal;word-spacing:normal;text-transform:none;writing-mode:lr-tb;direction:ltr;baseline-shift:baseline;text-anchor:start;display:inline;overflow:visible;visibility:visible;fill:#ffffff;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0;stroke-miterlimit:4;stroke-dasharray:none;marker:none;enable-background:accumulate"
         inkscape:connector-curvature="0" />
    </g>
    <path
       id="path1134"
       d="m 14.63946,8.4031737 0,0 c 0,0.6211207 -2.07255,1.1246391 -4.62916,1.1246391 -2.5566224,0 -4.629173,-0.5035184 -4.629173,-1.1246391 l 0,0 c 0,-0.621119 2.0725506,-1.124637 4.629173,-1.124637 2.55661,0 4.62916,0.503518 4.62916,1.124637 l 0,3.0423913 c 0,0.62112 -2.07255,1.124638 -4.62916,1.124638 -2.5566224,0 -4.629173,-0.503518 -4.629173,-1.124638 l 0,-3.0423913"
       inkscape:connector-curvature="0"
       style="fill:none;fill-rule:evenodd;stroke:#ffffff;stroke-width:0.52916664;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:10;stroke-dasharray:2.11666669, 0.52916668;stroke-dashoffset:0;stroke-opacity:1" />
  </g>
</svg>
</file>

<file path="kube-scheduler-simulator/web/static/README.md">
# STATIC

**This directory is not required, you can delete it if you don't want to use it.**

This directory contains your static files.
Each file inside this directory is mapped to `/`.
Thus you'd want to delete this README.md before deploying to production.

Example: `/static/robots.txt` is mapped as `/robots.txt`.

More information about the usage of this directory in [the documentation](https://nuxtjs.org/guide/assets#static).
</file>

<file path="kube-scheduler-simulator/web/static/sc.svg">
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!-- Created with Inkscape (http://www.inkscape.org/) -->

<svg
   xmlns:dc="http://purl.org/dc/elements/1.1/"
   xmlns:cc="http://creativecommons.org/ns#"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:svg="http://www.w3.org/2000/svg"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   width="18.035334mm"
   height="17.500378mm"
   viewBox="0 0 18.035334 17.500378"
   version="1.1"
   id="svg13826"
   inkscape:version="0.91 r13725"
   sodipodi:docname="sc.svg">
  <defs
     id="defs13820" />
  <sodipodi:namedview
     id="base"
     pagecolor="#ffffff"
     bordercolor="#666666"
     borderopacity="1.0"
     inkscape:pageopacity="0.0"
     inkscape:pageshadow="2"
     inkscape:zoom="8.01"
     inkscape:cx="3.0086534"
     inkscape:cy="26.225672"
     inkscape:document-units="mm"
     inkscape:current-layer="g3347"
     showgrid="false"
     inkscape:window-width="1440"
     inkscape:window-height="775"
     inkscape:window-x="0"
     inkscape:window-y="1"
     inkscape:window-maximized="1"
     fit-margin-top="0"
     fit-margin-left="0"
     fit-margin-right="0"
     fit-margin-bottom="0" />
  <metadata
     id="metadata13823">
    <rdf:RDF>
      <cc:Work
         rdf:about="">
        <dc:format>image/svg+xml</dc:format>
        <dc:type
           rdf:resource="http://purl.org/dc/dcmitype/StillImage" />
        <dc:title />
      </cc:Work>
    </rdf:RDF>
  </metadata>
  <g
     inkscape:label="Calque 1"
     inkscape:groupmode="layer"
     id="layer1"
     transform="translate(-0.99262638,-1.174181)">
    <g
       id="g70"
       transform="matrix(1.0148887,0,0,1.0148887,16.902146,-2.698726)">
      <path
         inkscape:export-ydpi="250.55"
         inkscape:export-xdpi="250.55"
         inkscape:export-filename="new.png"
         inkscape:connector-curvature="0"
         id="path3055"
         d="m -6.8492015,4.2724668 a 1.1191255,1.1099671 0 0 0 -0.4288818,0.1085303 l -5.8524037,2.7963394 a 1.1191255,1.1099671 0 0 0 -0.605524,0.7529759 l -1.443828,6.2812846 a 1.1191255,1.1099671 0 0 0 0.151943,0.851028 1.1191255,1.1099671 0 0 0 0.06362,0.08832 l 4.0508,5.036555 a 1.1191255,1.1099671 0 0 0 0.874979,0.417654 l 6.4961011,-0.0015 a 1.1191255,1.1099671 0 0 0 0.8749788,-0.416906 L 1.3818872,15.149453 A 1.1191255,1.1099671 0 0 0 1.5981986,14.210104 L 0.15212657,7.9288154 A 1.1191255,1.1099671 0 0 0 -0.45339794,7.1758396 L -6.3065496,4.3809971 A 1.1191255,1.1099671 0 0 0 -6.8492015,4.2724668 Z"
         style="fill:#326ce5;fill-opacity:1;stroke:none;stroke-width:0;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1" />
      <path
         id="path3054-2-9"
         d="M -6.8523435,3.8176372 A 1.1814304,1.171762 0 0 0 -7.3044284,3.932904 l -6.1787426,2.9512758 a 1.1814304,1.171762 0 0 0 -0.639206,0.794891 l -1.523915,6.6308282 a 1.1814304,1.171762 0 0 0 0.160175,0.89893 1.1814304,1.171762 0 0 0 0.06736,0.09281 l 4.276094,5.317236 a 1.1814304,1.171762 0 0 0 0.92363,0.440858 l 6.8576188,-0.0015 a 1.1814304,1.171762 0 0 0 0.9236308,-0.44011 l 4.2745966,-5.317985 a 1.1814304,1.171762 0 0 0 0.228288,-0.990993 L 0.53894439,7.6775738 A 1.1814304,1.171762 0 0 0 -0.10026101,6.8834313 L -6.2790037,3.9321555 A 1.1814304,1.171762 0 0 0 -6.8523435,3.8176372 Z m 0.00299,0.4550789 a 1.1191255,1.1099671 0 0 1 0.5426517,0.1085303 l 5.85315169,2.7948425 A 1.1191255,1.1099671 0 0 1 0.15197811,7.9290648 L 1.598051,14.21035 a 1.1191255,1.1099671 0 0 1 -0.2163123,0.939348 l -4.0493032,5.037304 a 1.1191255,1.1099671 0 0 1 -0.8749789,0.416906 l -6.4961006,0.0015 a 1.1191255,1.1099671 0 0 1 -0.874979,-0.417652 l -4.0508,-5.036554 a 1.1191255,1.1099671 0 0 1 -0.06362,-0.08832 1.1191255,1.1099671 0 0 1 -0.151942,-0.851028 l 1.443827,-6.2812853 a 1.1191255,1.1099671 0 0 1 0.605524,-0.7529758 l 5.8524036,-2.7963395 a 1.1191255,1.1099671 0 0 1 0.4288819,-0.1085303 z"
         style="color:#000000;font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:medium;line-height:normal;font-family:Sans;-inkscape-font-specification:Sans;text-indent:0;text-align:start;text-decoration:none;text-decoration-line:none;letter-spacing:normal;word-spacing:normal;text-transform:none;writing-mode:lr-tb;direction:ltr;baseline-shift:baseline;text-anchor:start;display:inline;overflow:visible;visibility:visible;fill:#ffffff;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0;stroke-miterlimit:4;stroke-dasharray:none;marker:none;enable-background:accumulate"
         inkscape:connector-curvature="0" />
    </g>
    <path
       style="fill:#326ce5;fill-opacity:1;fill-rule:evenodd;stroke:none;stroke-width:0.20757428;stroke-linecap:square;stroke-miterlimit:10"
       inkscape:connector-curvature="0"
       d="m 5.4198595,10.526751 0,0 c 0,0.376676 2.10346,0.682033 4.6982195,0.682033 2.594751,0 4.69821,-0.305357 4.69821,-0.682033 l 0,1.845045 c 0,0.376674 -2.103459,0.68203 -4.69821,0.68203 -2.5947595,0 -4.6982195,-0.305356 -4.6982195,-0.68203 z"
       id="path6473" />
    <path
       style="fill:#326ce5;fill-opacity:1;fill-rule:evenodd;stroke:none;stroke-width:0.20757428;stroke-linecap:square;stroke-miterlimit:10"
       inkscape:connector-curvature="0"
       d="m 5.4198595,10.526751 0,0 c 0,-0.376674 2.10346,-0.6820334 4.6982195,-0.6820334 2.594751,0 4.69821,0.3053594 4.69821,0.6820334 l 0,0 c 0,0.376676 -2.103459,0.682033 -4.69821,0.682033 -2.5947595,0 -4.6982195,-0.305357 -4.6982195,-0.682033 z"
       id="path6475" />
    <path
       style="fill:#ffffff;fill-opacity:1;fill-rule:evenodd;stroke:#eeeeee;stroke-width:0.20757429;stroke-linecap:square;stroke-miterlimit:10;stroke-opacity:1"
       inkscape:connector-curvature="0"
       d="m 5.4198595,8.6089656 0,0 c 0,0.376676 2.10346,0.6820319 4.6982195,0.6820319 2.594751,0 4.69821,-0.3053559 4.69821,-0.6820319 l 0,1.8450454 c 0,0.376674 -2.103459,0.68203 -4.69821,0.68203 -2.5947595,0 -4.6982195,-0.305356 -4.6982195,-0.68203 z"
       id="path6479" />
    <path
       style="fill:#326ce5;fill-opacity:1;fill-rule:evenodd;stroke:none;stroke-width:0.20757428;stroke-linecap:square;stroke-miterlimit:10"
       inkscape:connector-curvature="0"
       d="m 5.4198595,8.2170433 0,0 c 0,-0.376675 2.10346,-0.6820325 4.6982195,-0.6820325 2.594751,0 4.69821,0.3053575 4.69821,0.6820325 l 0,0 c 0,0.376676 -2.103459,0.6820319 -4.69821,0.6820319 -2.5947595,0 -4.6982195,-0.3053559 -4.6982195,-0.6820319 z"
       id="path6481" />
    <path
       id="path6487"
       d="m 5.4199585,5.6616598 0,0 c 0,-0.376674 2.103456,-0.682032 4.6982235,-0.682032 2.594746,0 4.698212,0.305358 4.698212,0.682032 l 0,0 c 0,0.376676 -2.103466,0.682032 -4.698212,0.682032 -2.5947675,0 -4.6982235,-0.305356 -4.6982235,-0.682032 z"
       inkscape:connector-curvature="0"
       style="fill:#326ce5;fill-opacity:1;fill-rule:evenodd;stroke:none;stroke-width:0.20757428;stroke-linecap:square;stroke-miterlimit:10" />
    <g
       id="g3347"
       transform="translate(-0.10787059,0.73050426)">
      <path
         style="fill:#326ce5;fill-opacity:1;fill-rule:evenodd;stroke:none;stroke-width:0.20757428;stroke-linecap:square;stroke-miterlimit:10"
         inkscape:connector-curvature="0"
         d="m 5.4199585,5.6616598 0,0 c 0,0.376676 2.103456,0.682032 4.6982235,0.682032 2.594746,0 4.698212,-0.305356 4.698212,-0.682032 l 0,1.845045 c 0,0.3766745 -2.103466,0.6820305 -4.698212,0.6820305 -2.5947675,0 -4.6982235,-0.305356 -4.6982235,-0.6820305 z"
         id="path6485" />
      <g
         id="g3348">
        <path
           style="fill:none;fill-rule:evenodd;stroke:#ffffff;stroke-width:0.41499999;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:10;stroke-dasharray:1.66000002, 0.415;stroke-dashoffset:0;stroke-opacity:1"
           inkscape:connector-curvature="0"
           d="m 14.816289,10.881103 0,0 c 0,0.376676 -2.103459,0.682033 -4.69821,0.682033 -2.5947495,0 -4.6982195,-0.305357 -4.6982195,-0.682033 l 0,0 c 0,-0.376675 2.10347,-0.682031 4.6982195,-0.682031 2.594751,0 4.69821,0.305356 4.69821,0.682031 l 0,1.845046 c 0,0.376673 -2.103459,0.682029 -4.69821,0.682029 -2.5947495,0 -4.6982195,-0.305356 -4.6982195,-0.682029 l 0,-1.845046"
           id="path6477" />
        <path
           style="fill:none;fill-rule:evenodd;stroke:#ffffff;stroke-width:0.41499999;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:10;stroke-dasharray:none;stroke-opacity:1"
           inkscape:connector-curvature="0"
           d="m 14.816289,7.8640013 0,0 c 0,0.376676 -2.103459,0.6820319 -4.69821,0.6820319 -2.5947495,0 -4.6982195,-0.3053559 -4.6982195,-0.6820319 l 0,0 c 0,-0.376675 2.10347,-0.6820325 4.6982195,-0.6820325 2.594751,0 4.69821,0.3053575 4.69821,0.6820325 l 0,1.8450457 c 0,0.376674 -2.103459,0.68203 -4.69821,0.68203 -2.5947495,0 -4.6982195,-0.305356 -4.6982195,-0.68203 l 0,-1.8450457"
           id="path6483" />
        <path
           id="path6489"
           d="m 14.816394,5.6616598 0,0 c 0,0.376676 -2.103466,0.682032 -4.698212,0.682032 -2.5947575,0 -4.6982235,-0.305356 -4.6982235,-0.682032 l 0,0 c 0,-0.376674 2.103466,-0.682032 4.6982235,-0.682032 2.594746,0 4.698212,0.305358 4.698212,0.682032 l 0,1.845045 c 0,0.3766745 -2.103466,0.6820305 -4.698212,0.6820305 -2.5947575,0 -4.6982235,-0.305356 -4.6982235,-0.6820305 l 0,-1.845045"
           inkscape:connector-curvature="0"
           style="fill:none;fill-rule:evenodd;stroke:#ffffff;stroke-width:0.41514859;stroke-linecap:butt;stroke-linejoin:round;stroke-miterlimit:10;stroke-dasharray:1.66059433, 0.41514858;stroke-dashoffset:0;stroke-opacity:1" />
      </g>
    </g>
  </g>
</svg>
</file>

<file path="kube-scheduler-simulator/web/static/sched.svg">
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!-- Created with Inkscape (http://www.inkscape.org/) -->

<svg
   xmlns:dc="http://purl.org/dc/elements/1.1/"
   xmlns:cc="http://creativecommons.org/ns#"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:svg="http://www.w3.org/2000/svg"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   width="18.035334mm"
   height="17.500378mm"
   viewBox="0 0 18.035334 17.500378"
   version="1.1"
   id="svg11531"
   inkscape:version="0.91 r13725"
   sodipodi:docname="sched.svg">
  <defs
     id="defs11525" />
  <sodipodi:namedview
     id="base"
     pagecolor="#ffffff"
     bordercolor="#666666"
     borderopacity="1.0"
     inkscape:pageopacity="0.0"
     inkscape:pageshadow="2"
     inkscape:zoom="8"
     inkscape:cx="16.968621"
     inkscape:cy="33.509081"
     inkscape:document-units="mm"
     inkscape:current-layer="layer1"
     showgrid="false"
     inkscape:window-width="1440"
     inkscape:window-height="771"
     inkscape:window-x="55"
     inkscape:window-y="0"
     inkscape:window-maximized="0"
     fit-margin-top="0"
     fit-margin-left="0"
     fit-margin-right="0"
     fit-margin-bottom="0" />
  <metadata
     id="metadata11528">
    <rdf:RDF>
      <cc:Work
         rdf:about="">
        <dc:format>image/svg+xml</dc:format>
        <dc:type
           rdf:resource="http://purl.org/dc/dcmitype/StillImage" />
        <dc:title />
      </cc:Work>
    </rdf:RDF>
  </metadata>
  <g
     inkscape:label="Calque 1"
     inkscape:groupmode="layer"
     id="layer1"
     transform="translate(-0.99262638,-1.174181)">
    <g
       id="g70"
       transform="matrix(1.0148887,0,0,1.0148887,16.902146,-2.698726)">
      <path
         inkscape:export-ydpi="250.55"
         inkscape:export-xdpi="250.55"
         inkscape:export-filename="new.png"
         inkscape:connector-curvature="0"
         id="path3055"
         d="m -6.8492015,4.2724668 a 1.1191255,1.1099671 0 0 0 -0.4288818,0.1085303 l -5.8524037,2.7963394 a 1.1191255,1.1099671 0 0 0 -0.605524,0.7529759 l -1.443828,6.2812846 a 1.1191255,1.1099671 0 0 0 0.151943,0.851028 1.1191255,1.1099671 0 0 0 0.06362,0.08832 l 4.0508,5.036555 a 1.1191255,1.1099671 0 0 0 0.874979,0.417654 l 6.4961011,-0.0015 a 1.1191255,1.1099671 0 0 0 0.8749788,-0.416906 L 1.3818872,15.149453 A 1.1191255,1.1099671 0 0 0 1.5981986,14.210104 L 0.15212657,7.9288154 A 1.1191255,1.1099671 0 0 0 -0.45339794,7.1758396 L -6.3065496,4.3809971 A 1.1191255,1.1099671 0 0 0 -6.8492015,4.2724668 Z"
         style="fill:#326ce5;fill-opacity:1;stroke:none;stroke-width:0;stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1" />
      <path
         id="path3054-2-9"
         d="M -6.8523435,3.8176372 A 1.1814304,1.171762 0 0 0 -7.3044284,3.932904 l -6.1787426,2.9512758 a 1.1814304,1.171762 0 0 0 -0.639206,0.794891 l -1.523915,6.6308282 a 1.1814304,1.171762 0 0 0 0.160175,0.89893 1.1814304,1.171762 0 0 0 0.06736,0.09281 l 4.276094,5.317236 a 1.1814304,1.171762 0 0 0 0.92363,0.440858 l 6.8576188,-0.0015 a 1.1814304,1.171762 0 0 0 0.9236308,-0.44011 l 4.2745966,-5.317985 a 1.1814304,1.171762 0 0 0 0.228288,-0.990993 L 0.53894439,7.6775738 A 1.1814304,1.171762 0 0 0 -0.10026101,6.8834313 L -6.2790037,3.9321555 A 1.1814304,1.171762 0 0 0 -6.8523435,3.8176372 Z m 0.00299,0.4550789 a 1.1191255,1.1099671 0 0 1 0.5426517,0.1085303 l 5.85315169,2.7948425 A 1.1191255,1.1099671 0 0 1 0.15197811,7.9290648 L 1.598051,14.21035 a 1.1191255,1.1099671 0 0 1 -0.2163123,0.939348 l -4.0493032,5.037304 a 1.1191255,1.1099671 0 0 1 -0.8749789,0.416906 l -6.4961006,0.0015 a 1.1191255,1.1099671 0 0 1 -0.874979,-0.417652 l -4.0508,-5.036554 a 1.1191255,1.1099671 0 0 1 -0.06362,-0.08832 1.1191255,1.1099671 0 0 1 -0.151942,-0.851028 l 1.443827,-6.2812853 a 1.1191255,1.1099671 0 0 1 0.605524,-0.7529758 l 5.8524036,-2.7963395 a 1.1191255,1.1099671 0 0 1 0.4288819,-0.1085303 z"
         style="color:#000000;font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:medium;line-height:normal;font-family:Sans;-inkscape-font-specification:Sans;text-indent:0;text-align:start;text-decoration:none;text-decoration-line:none;letter-spacing:normal;word-spacing:normal;text-transform:none;writing-mode:lr-tb;direction:ltr;baseline-shift:baseline;text-anchor:start;display:inline;overflow:visible;visibility:visible;fill:#ffffff;fill-opacity:1;fill-rule:nonzero;stroke:none;stroke-width:0;stroke-miterlimit:4;stroke-dasharray:none;marker:none;enable-background:accumulate"
         inkscape:connector-curvature="0" />
    </g>
    <text
       id="text2066"
       y="16.811775"
       x="10.066792"
       style="font-style:normal;font-weight:normal;font-size:10.58333302px;line-height:6.61458349px;font-family:Sans;letter-spacing:0px;word-spacing:0px;fill:#ffffff;fill-opacity:1;stroke:none;stroke-width:0.26458332px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1"
       xml:space="preserve"><tspan
         style="font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:2.82222223px;font-family:Arial;-inkscape-font-specification:'Arial, Normal';text-align:center;writing-mode:lr-tb;text-anchor:middle;fill:#ffffff;fill-opacity:1;stroke-width:0.26458332px"
         y="16.811775"
         x="10.066792"
         id="tspan2064"
         sodipodi:role="line">sched</tspan></text>
    <path
       id="path1984"
       d="m 9.9922574,4.4469925 c -0.1383234,-0.0044 -3.9953139,1.8917791 -4.0457429,1.9890241 -0.121153,0.233682 -0.998802,4.2814784 -0.947235,4.3687244 0.03,0.05065 0.662199,0.852305 1.404567,1.780771 l 1.349798,1.687754 2.2215743,0.0011 2.2215722,0.0011 1.412819,-1.765784 1.41388,-1.765265 -0.49456,-2.1688607 C 14.25673,7.3828155 14.01329,6.3872756 13.98842,6.3632856 13.91892,6.2964056 10.06724,4.4495465 9.9922574,4.4471255 Z m 0.2361666,1.9828241 c 0.186637,0.008 0.205792,0.291571 0.605128,0.321427 0.5216,0.039 0.465508,-0.446651 0.864553,-0.108522 0.399044,0.338125 -0.0889,0.362641 0.03514,0.870749 0.12409,0.5081041 0.568801,0.3050991 0.370523,0.7890987 -0.198279,0.484002 -0.372613,0.02728 -0.81751,0.302308 -0.444897,0.275023 -0.114564,0.635346 -0.636138,0.596347 -0.521652,-0.039 -0.140467,-0.346065 -0.53951,-0.684197 C 9.711566,8.1788955 9.4703718,8.6044873 9.3463088,8.0963797 9.2222188,7.5882716 9.632244,7.8553625 9.830523,7.3713607 10.029224,6.8873136 9.5494288,6.7901076 9.9943524,6.5150816 c 0.05564,-0.03434 0.1018906,-0.05688 0.1421056,-0.07028 0.03508,-0.01167 0.06533,-0.01614 0.09197,-0.01497 z m 0.521493,0.572059 c -0.07578,-5.306e-4 -0.151209,0.01021 -0.223758,0.03204 -0.400393,0.120732 -0.626954,0.5433219 -0.505909,0.943613 0.12094,0.3999197 0.54311,0.6261487 0.943107,0.5053937 0.399838,-0.120746 0.626268,-0.5426107 0.505909,-0.9425788 C 11.373516,7.2225236 11.081837,7.0041896 10.749917,7.0018776 Z M 7.7572695,8.3036033 c 0.03969,7.9e-4 0.08832,0.009 0.1482991,0.02739 0.4799279,0.147138 -0.0077,0.399503 0.334354,0.766879 0.3420531,0.367377 0.6285182,-0.101086 0.7410452,0.388091 0.112527,0.4891777 -0.3499913,0.192921 -0.4971261,0.6728277 -0.1471612,0.479904 0.4014791,0.494083 0.03411,0.836123 -0.367374,0.342041 -0.3418151,-0.20606 -0.8309501,-0.09353 -0.489188,0.112527 -0.227039,0.594654 -0.70694,0.447516 -0.479928,-0.147137 0.0077,-0.3995 -0.334354,-0.766876 -0.342054,-0.367379 -0.628518,0.100571 -0.741045,-0.388607 -0.112528,-0.48918 0.349991,-0.192924 0.497125,-0.6728267 0.147162,-0.479907 -0.401981,-0.49357 -0.03461,-0.83561 0.367374,-0.342043 0.342291,0.206065 0.831453,0.09354 0.428043,-0.09846 0.280643,-0.479607 0.558641,-0.474906 z m 3.5672165,0.524516 c 0.04551,5.36e-4 0.100807,0.01243 0.170022,0.03927 0.55327,0.214739 -0.175234,0.545894 0.253735,0.9560137 0.428969,0.410118 0.727472,-0.3327417 0.966867,0.210323 0.239369,0.543065 -0.510434,0.262432 -0.497126,0.855763 0.01323,0.593331 0.749591,0.27872 0.534829,0.83199 -0.214709,0.553268 -0.545888,-0.175247 -0.955992,0.25373 -0.410131,0.428977 0.33221,0.726972 -0.210847,0.966348 -0.543057,0.23938 -0.261382,-0.509939 -0.854736,-0.496609 -0.593328,0.01333 -0.279215,0.749072 -0.83251,0.534334 C 9.3454848,12.764543 10.073987,12.433388 9.644993,12.023268 9.2160238,11.613148 8.9180238,12.356008 8.6786546,11.812945 8.4392867,11.26988 9.1885598,11.551032 9.1752778,10.957701 9.1617878,10.36437 8.4256867,10.678462 8.6404225,10.125192 8.8551588,9.5719223 9.1868138,10.300955 9.596945,9.871978 10.007048,9.4430013 9.2641778,9.1444903 9.807262,8.9051143 c 0.543055,-0.239377 0.26191,0.509939 0.855237,0.496609 0.519192,-0.01167 0.343429,-0.576408 0.661987,-0.573608 z m -3.8814095,0.25528 c -0.417988,-1.58e-4 -0.756946,0.338564 -0.757078,0.7565447 -1.59e-4,0.418182 0.338905,0.757219 0.757078,0.75706 0.4179631,-1.48e-4 0.756682,-0.339095 0.7565239,-0.75706 C 8.1994675,9.4221803 7.8608276,9.0835473 7.4430765,9.0833993 Z M 10.69663,9.808421 c -0.615684,4.2e-5 -1.114846,0.498983 -1.11519,1.114661 -2.38e-4,0.616082 0.499109,1.11565 1.11519,1.115692 0.616268,2.22e-4 1.115907,-0.499425 1.115669,-1.115692 0,-0.615866 -0.499798,-1.114883 -1.115669,-1.114661 z"
       style="opacity:1;fill:#ffffff;fill-opacity:1;stroke:#eeeeee;stroke-width:0;stroke-miterlimit:10;stroke-dasharray:none;stroke-dashoffset:11.23642349;stroke-opacity:1"
       inkscape:connector-curvature="0"
       sodipodi:nodetypes="cccccccccccccscccccsccccccccccccccscscccccccccccccccccccccscccccccccccccc" />
  </g>
</svg>
</file>

<file path="kube-scheduler-simulator/web/store/helpers/storeHelper.ts">
import { V1ObjectMeta } from "@kubernetes/client-node";

// resourceObject is an interface of stored resource.
interface resourceObject {
  metadata?: V1ObjectMeta;
}

// ResourceState represents a type of each resource's state.
type ResourceState<T extends resourceObject> = Array<T>;

// createResourceState returns a new resource store's state.
export function createResourceState<T extends resourceObject>(
  resource: T[]
): ResourceState<T> {
  let result: ResourceState<T> = [];
  resource.forEach((r) => {
    result = addResourceToState(result, r);
  });
  return result;
}

// addResourceToState adds the resource to the state.
export function addResourceToState<T extends resourceObject>(
  state: ResourceState<T>,
  r: T
): ResourceState<T> {
  state.push(r);
  return state;
}

// addResourceToState updates the specified resource in the state.
export function modifyResourceInState<T extends resourceObject>(
  state: ResourceState<T>,
  r: T
): ResourceState<T> {
  const i = state.findIndex((res) => res.metadata?.uid === r.metadata?.uid);
  // the resource doesn't exist in the state
  if (i === -1) {
    console.warn("resource doesn't exist in the state");
    return addResourceToState(state, r);
  }
  state.splice(i, 1);
  return addResourceToState(state, r);
}

// deleteResourceInState deletes the specified resouce from the state.
export function deleteResourceInState<T extends resourceObject>(
  state: ResourceState<T>,
  r: T
): ResourceState<T> {
  const i = state.findIndex((res) => res.metadata?.uid === r.metadata?.uid);
  if (i === -1) {
    console.warn("resource doesn't exist in the state");
    return state;
  }
  state.splice(i, 1);
  return state;
}
</file>

<file path="kube-scheduler-simulator/web/store/namespace.ts">
import { reactive, inject } from "@nuxtjs/composition-api";
import { V1Namespace } from "@kubernetes/client-node";
import { NamespaceAPIKey } from "~/api/APIProviderKeys";
import {
  createResourceState,
  addResourceToState,
  modifyResourceInState,
  deleteResourceInState,
} from "./helpers/storeHelper";
import { WatchEventType } from "@/types/resources";

type selectedNamespace = {
  isNew: boolean;
  item: V1Namespace;
  resourceKind: string;
  isDeletable: boolean;
}

type stateType = {
  selectedNamespace: selectedNamespace | null;
  namespaces: V1Namespace[];
  lastResourceVersion: string,
}

export default function namespaceStore() {
  const state: stateType = reactive({
    selectedNamespace: null,
    namespaces: [],
    lastResourceVersion: "",
  });

  const namespaceAPI = inject(NamespaceAPIKey);
  if (!namespaceAPI) {
    throw new Error(`${NamespaceAPIKey.description} is not provided`);
  }

  return {
    get namespaces() {
      return state.namespaces;
    },
    get count(): number {
      return state.namespaces.length;
    },
    get selected() {
      return state.selectedNamespace;
    },
    get lastResourceVersion() {
      return state.lastResourceVersion;
    },
    async setLastResourceVersion(pv: V1Namespace) {
      state.lastResourceVersion =
        pv.metadata!.resourceVersion || state.lastResourceVersion;
    },
    select(ns: V1Namespace | null, isNew: boolean) {
      if (ns !== null) {
        state.selectedNamespace = {
          isNew: isNew,
          item: ns,
          resourceKind: "namespace",
          isDeletable: true,
        }
      }
    },
    resetSelected() {
      state.selectedNamespace = null;
    },
    async fetchSelected() {
      if (
        state.selectedNamespace?.item.metadata?.name &&
        !this.selected?.isNew
      ) {
        const ns = await namespaceAPI.getNamespace(
          state.selectedNamespace.item.metadata.name
        );
        this.select(ns, false);
      }
    },
    async apply(ns: V1Namespace) {
      if (ns.metadata?.name) {
        await namespaceAPI.applyNamespace(ns);
      } else if (ns.metadata?.generateName) {
        await namespaceAPI.createNamespace(ns);
      } else {
        throw new Error(
          "failed to apply namespace: namespace should have metadata.name or metadata.generateName"
        );
      }
    },
    async delete(ns: V1Namespace) {
      if (!ns.metadata?.name) {
        throw new Error(
          "failed to delete namespace: node should have metadata.name"
        )
      }
      namespaceAPI.deleteNamespace(ns.metadata.name).then((res: V1Namespace)=>{
        // When deleting a namespace then it still exists, there is the possibility that any finalizers are specified.
        // We expect that this condition would be almost true.
        if (res.status?.phase === "Terminating" ) {
          res.spec!.finalizers = []
          namespaceAPI.finalizeNamespace(res)
        }
      }).catch((e)=> {
        throw new Error(`failed during the delete process`)
      })

    },
    // initList calls list API, and stores current resource data and the lastResourceVersion.
    async initList() {
      const listns = await namespaceAPI.listNamespace();
      state.namespaces = createResourceState<V1Namespace>(listns.items);
      state.lastResourceVersion = listns.metadata?.resourceVersion!;
    },
    // watchEventHandler handles each notified event.
    async watchEventHandler(eventType: WatchEventType, ns: V1Namespace) {
      switch (eventType) {
        case WatchEventType.ADDED: {
          state.namespaces = addResourceToState(state.namespaces, ns);
          break;
        }
        case WatchEventType.MODIFIED: {
          state.namespaces = modifyResourceInState(state.namespaces, ns);
          break;
        }
        case WatchEventType.DELETED: {
          state.namespaces = deleteResourceInState(state.namespaces, ns);
          break;
        }
        default:
          break;
      }
    },
  }
}

export type NamespaceStore = ReturnType<typeof namespaceStore>;
</file>

<file path="kube-scheduler-simulator/web/store/node.ts">
import { reactive, inject } from "@nuxtjs/composition-api";
import { V1Node } from "@kubernetes/client-node";
import { NodeAPIKey } from "~/api/APIProviderKeys";
import {
  createResourceState,
  addResourceToState,
  modifyResourceInState,
  deleteResourceInState,
} from "./helpers/storeHelper";
import { WatchEventType } from "@/types/resources";

type stateType = {
  selectedNode: selectedNode | null;
  nodes: V1Node[];
  lastResourceVersion: string;
};

type selectedNode = {
  // isNew represents whether this Node is a new one or not.
  isNew: boolean;
  item: V1Node;
  resourceKind: string;
  isDeletable: boolean;
};

export default function nodeStore() {
  const state: stateType = reactive({
    selectedNode: null,
    nodes: [],
    lastResourceVersion: "",
  });
  const nodeAPI = inject(NodeAPIKey);
  if (!nodeAPI) {
    throw new Error(`${NodeAPIKey.description} is not provided`);
  }
  return {
    get nodes() {
      return state.nodes;
    },

    get count(): number {
      return state.nodes.length;
    },

    get selected() {
      return state.selectedNode;
    },

    select(n: V1Node | null, isNew: boolean) {
      if (n !== null) {
        state.selectedNode = {
          isNew: isNew,
          item: n,
          resourceKind: "Node",
          isDeletable: true,
        };
      }
    },

    resetSelected() {
      state.selectedNode = null;
    },

    async fetchSelected() {
      if (state.selectedNode?.item.metadata?.name && !this.selected?.isNew) {
        const n = await nodeAPI.getNode(state.selectedNode.item.metadata.name);
        this.select(n, false);
      }
    },

    async apply(n: V1Node) {
      if (n.metadata?.name) {
        await nodeAPI.applyNode(n);
      } else if (n.metadata?.generateName) {
        // This Node can be expected to be a newly created Node. So, use `createNode` instead.
        await nodeAPI.createNode(n);
      } else {
        throw new Error(
          "failed to apply node: node should have metadata.name or metadata.generateName"
        );
      }
    },

    async delete(n: V1Node) {
      if (n.metadata?.name) {
        await nodeAPI.deleteNode(n.metadata.name);
      } else {
        throw new Error(
          "failed to delete node: node should have metadata.name"
        )
      }
    },

    // initList calls list API, and stores current resource data and lastResourceVersion.
    async initList() {
      const listnodes = await nodeAPI.listNode();
      state.nodes = createResourceState<V1Node>(listnodes.items);
      state.lastResourceVersion = listnodes.metadata?.resourceVersion!;
    },

    // watchEventHandler handles each notified event.
    async watchEventHandler(eventType: WatchEventType, node: V1Node) {
      switch (eventType) {
        case WatchEventType.ADDED: {
          state.nodes = addResourceToState(state.nodes, node);
          break;
        }
        case WatchEventType.MODIFIED: {
          state.nodes = modifyResourceInState(state.nodes, node);
          break;
        }
        case WatchEventType.DELETED: {
          state.nodes = deleteResourceInState(state.nodes, node);
          break;
        }
        default:
          break;
      }
    },

    get lastResourceVersion() {
      return state.lastResourceVersion;
    },

    async setLastResourceVersion(node: V1Node) {
      state.lastResourceVersion =
        node.metadata!.resourceVersion || state.lastResourceVersion;
    },
  };
}

export type NodeStore = ReturnType<typeof nodeStore>;
</file>

<file path="kube-scheduler-simulator/web/store/pod.ts">
import { reactive, inject } from "@nuxtjs/composition-api";
import { V1Pod } from "@kubernetes/client-node";
import { PodAPIKey } from "~/api/APIProviderKeys";
import { WatchEventType } from "@/types/resources";

type stateType = {
  selectedPod: SelectedPod | null;
  pods: StatePods;
  lastResourceVersion: string;
};

type StatePods = {
  // key is node name or "unscheduled"
  [key: string]: Array<V1Pod>;
};
type StatePodsKeyIndexTuple = { key: string; index: number };

export type SelectedPod = {
  // isNew represents whether this is a new one or not.
  isNew: boolean;
  item: V1Pod;
  resourceKind: string;
  isDeletable: boolean;
};

export default function podStore() {
  const state: stateType = reactive({
    selectedPod: null,
    pods: {},
    lastResourceVersion: "",
  });

  const podAPI = inject(PodAPIKey);
  if (!podAPI) {
    throw new Error(`${PodAPIKey.description} is not provided`);
  }

  function createPodState(pods: V1Pod[]): void {
    pods.forEach((p) => {
      addPodToState(p);
    });
  }
  function addPodToState(p: V1Pod): void {
    if (!p.spec?.nodeName) {
      // unscheduled pod
      if (!state.pods["unscheduled"]) {
        state.pods = Object.assign({}, state.pods, { unscheduled: [p] });
      } else {
        state.pods["unscheduled"].push(p);
      }
    } else if (!state.pods[p.spec?.nodeName as string]) {
      // first pod on the node
      state.pods = Object.assign({}, state.pods, { [p.spec?.nodeName]: [p] });
    } else {
      state.pods[p.spec?.nodeName as string].push(p);
    }
  }
  function modifyPodInState(p: V1Pod): void {
    const targetInfo = findStatePodsKeyByUID(state.pods, p.metadata?.uid!);
    // the pod doesn't exist in the state
    if (targetInfo.index === -1) {
      console.warn("pod doesn't exist in the state");
      addPodToState(p);
    }
    state.pods[targetInfo.key].splice(targetInfo.index, 1);
    addPodToState(p);
  }
  function deletePodInState(p: V1Pod): void {
    const targetInfo = findStatePodsKeyByUID(state.pods, p.metadata?.uid!);
    // the pod doesn't exist in the state
    if (targetInfo.index === -1) {
      console.warn("pod doesn't exist in the state");
    }
    state.pods[targetInfo.key].splice(targetInfo.index, 1);
  }

  return {
    get pods() {
      return state.pods;
    },

    get count(): number {
      let num = 0;
      Object.keys(state.pods).forEach((key) => {
        num += state.pods[key].length;
      });
      return num;
    },

    get selected() {
      return state.selectedPod;
    },

    select(p: V1Pod | null, isNew: boolean) {
      if (p !== null) {
        state.selectedPod = {
          isNew: isNew,
          item: p,
          resourceKind: "Pod",
          isDeletable: true,
        };
      }
    },

    resetSelected() {
      state.selectedPod = null;
    },

    async fetchSelected() {
      if (this.selected?.item.metadata?.name && this.selected?.item.metadata?.namespace && !this.selected?.isNew) {
        const p = await podAPI.getPod(this.selected.item.metadata.namespace, this.selected.item.metadata.name);
        this.select(p, false);
      }
    },

    async apply(p: V1Pod) {
      if (p.metadata?.name) {
        await podAPI.applyPod(p);
      } else if (p.metadata?.generateName) {
        // This Pod can be expected to be a newly created Pod. So, use `createPod` instead.
        await podAPI.createPod(p);
      } else {
        throw new Error(
          "failed to apply pod: pod should have metadata.name or metadata.generateName"
        );
      }
    },

    async delete(p: V1Pod) {
      if (p.metadata?.name && p.metadata.namespace) {
        await podAPI.deletePod(p.metadata.namespace, p.metadata.name);
      } else {
        throw new Error(
          "failed to delete pod: pod should have metadata.name"
        );
      }
    },

    // initList calls list API, and stores current resource data and lastResourceVersion.
    async initList() {
      const listpods = await podAPI.listPod();
      createPodState(listpods.items);
      state.lastResourceVersion = listpods.metadata?.resourceVersion!;
    },

    // watchEventHandler handles each notified event.
    async watchEventHandler(eventType: WatchEventType, pod: V1Pod) {
      switch (eventType) {
        case WatchEventType.ADDED: {
          addPodToState(pod);
          break;
        }
        case WatchEventType.MODIFIED: {
          modifyPodInState(pod);
          break;
        }
        case WatchEventType.DELETED: {
          deletePodInState(pod);
          break;
        }
        default:
          break;
      }
    },

    get lastResourceVersion() {
      return state.lastResourceVersion;
    },

    async setLastResourceVersion(pod: V1Pod) {
      state.lastResourceVersion =
        pod.metadata!.resourceVersion || state.lastResourceVersion;
    },
  };

  // findStatePodsKeyByUID searches the pods in the state by uid and returns the key and index of the pods.
  function findStatePodsKeyByUID(
    statePods: StatePods,
    uid: string
  ): StatePodsKeyIndexTuple {
    for (const k of Object.keys(statePods)) {
      const i = statePods[k].findIndex((pod) => pod.metadata?.uid === uid);
      // found the pod
      if (i !== -1) {
        return { key: k, index: i } as StatePodsKeyIndexTuple;
      }
    }
    // not found.
    return { key: "", index: -1 } as StatePodsKeyIndexTuple;
  }
}

export type PodStore = ReturnType<typeof podStore>;
</file>

<file path="kube-scheduler-simulator/web/store/priorityclass.ts">
import { reactive, inject } from "@nuxtjs/composition-api";
import { V1PriorityClass } from "@kubernetes/client-node";
import { PriorityClassAPIKey } from "~/api/APIProviderKeys";
import {
  createResourceState,
  addResourceToState,
  modifyResourceInState,
  deleteResourceInState,
} from "./helpers/storeHelper";
import { WatchEventType } from "@/types/resources";

type stateType = {
  selectedPriorityClass: selectedPriorityClass | null;
  priorityclasses: V1PriorityClass[];
  lastResourceVersion: string;
};

type selectedPriorityClass = {
  // isNew represents whether this is a new PriorityClass or not.
  isNew: boolean;
  item: V1PriorityClass;
  resourceKind: string;
  isDeletable: boolean;
};

export default function priorityclassStore() {
  const state: stateType = reactive({
    selectedPriorityClass: null,
    priorityclasses: [],
    lastResourceVersion: "",
  });

  const priorityClassAPI = inject(PriorityClassAPIKey);
  if (!priorityClassAPI) {
    throw new Error(`${PriorityClassAPIKey.description} is not provided`);
  }

  // `CheckIsDeletable` returns whether the given PriorityClass can be deleted or not.
  // The PriorityClasses that have the name prefixed with `system-` are reserved by the system so can't be deleted.
  const checkIsDeletable = (p: V1PriorityClass) => {
    return !!p.metadata?.name && !p.metadata?.name?.startsWith("system-");
  };

  return {
    get priorityclasses() {
      return state.priorityclasses;
    },

    get count(): number {
      return state.priorityclasses.length;
    },

    get selected() {
      return state.selectedPriorityClass;
    },

    select(pc: V1PriorityClass | null, isNew: boolean) {
      if (pc !== null) {
        state.selectedPriorityClass = {
          isNew: isNew,
          item: pc,
          resourceKind: "PC",
          isDeletable: checkIsDeletable(pc),
        };
      }
    },

    resetSelected() {
      state.selectedPriorityClass = null;
    },

    async apply(pc: V1PriorityClass) {
      if (pc.metadata?.name) {
        await priorityClassAPI.applyPriorityClass(pc);
      } else if (pc.metadata?.generateName) {
        // This PriorityClass can be expected to be a newly created PriorityClass. So, use `createPriorityClass` instead.
        await priorityClassAPI.createPriorityClass(pc);
      } else {
        throw new Error(
          "failed to apply priorityclass: priorityclass should have metadata.name or metadata.generateName"
        );
      }
    },

    async fetchSelected() {
      if (
        state.selectedPriorityClass?.item.metadata?.name &&
        !this.selected?.isNew
      ) {
        const s = await priorityClassAPI.getPriorityClass(
          state.selectedPriorityClass.item.metadata.name
        );
        this.select(s, false);
      }
    },

    async delete(p: V1PriorityClass) {
      if (p.metadata?.name) {
        await priorityClassAPI.deletePriorityClass(p.metadata.name);
      } else {
        throw new Error(
          "failed to delete priorityclass: priorityclass should have metadata.name"
        );
      }

    },

    // initList calls list API, and stores current resource data and lastResourceVersion.
    async initList() {
      const listpriorityclasses = await priorityClassAPI.listPriorityClass();
      state.priorityclasses = createResourceState<V1PriorityClass>(
        listpriorityclasses.items
      );
      state.lastResourceVersion =
        listpriorityclasses.metadata?.resourceVersion!;
    },

    // watchEventHandler handles each notified event.
    async watchEventHandler(eventType: WatchEventType, pc: V1PriorityClass) {
      switch (eventType) {
        case WatchEventType.ADDED: {
          state.priorityclasses = addResourceToState(state.priorityclasses, pc);
          break;
        }
        case WatchEventType.MODIFIED: {
          state.priorityclasses = modifyResourceInState(
            state.priorityclasses,
            pc
          );
          break;
        }
        case WatchEventType.DELETED: {
          state.priorityclasses = deleteResourceInState(
            state.priorityclasses,
            pc
          );
          break;
        }
        default:
          break;
      }
    },

    get lastResourceVersion() {
      return state.lastResourceVersion;
    },

    async setLastResourceVersion(pc: V1PriorityClass) {
      state.lastResourceVersion =
        pc.metadata!.resourceVersion || state.lastResourceVersion;
    },
  };
}

export type PriorityClassStore = ReturnType<typeof priorityclassStore>;
</file>

<file path="kube-scheduler-simulator/web/store/pv.ts">
import { reactive, inject } from "@nuxtjs/composition-api";
import { V1PersistentVolume } from "@kubernetes/client-node";
import { PVAPIKey } from "~/api/APIProviderKeys";
import {
  createResourceState,
  addResourceToState,
  modifyResourceInState,
  deleteResourceInState,
} from "./helpers/storeHelper";
import { WatchEventType } from "@/types/resources";

type stateType = {
  selectedPersistentVolume: selectedPersistentVolume | null;
  pvs: V1PersistentVolume[];
  lastResourceVersion: string;
};

type selectedPersistentVolume = {
  // isNew represents whether this is a new PersistentVolume or not.
  isNew: boolean;
  item: V1PersistentVolume;
  resourceKind: string;
  isDeletable: boolean;
};

export default function pvStore() {
  const state: stateType = reactive({
    selectedPersistentVolume: null,
    pvs: [],
    lastResourceVersion: "",
  });

  const pvAPI = inject(PVAPIKey);
  if (!pvAPI) {
    throw new Error(`${PVAPIKey.description} is not provided`);
  }

  return {
    get pvs() {
      return state.pvs;
    },

    get count(): number {
      return state.pvs.length;
    },

    get selected() {
      return state.selectedPersistentVolume;
    },

    select(p: V1PersistentVolume | null, isNew: boolean) {
      if (p !== null) {
        state.selectedPersistentVolume = {
          isNew: isNew,
          item: p,
          resourceKind: "PV",
          isDeletable: true,
        };
      }
    },

    resetSelected() {
      state.selectedPersistentVolume = null;
    },

    async fetchSelected() {
      if (
        state.selectedPersistentVolume?.item.metadata?.name &&
        !this.selected?.isNew
      ) {
        const p = await pvAPI.getPersistentVolume(
          state.selectedPersistentVolume.item.metadata.name
        );
        this.select(p, false);
      }
    },

    async apply(n: V1PersistentVolume) {
      if (n.metadata?.name) {
        await pvAPI.applyPersistentVolume(n);
      } else if (n.metadata?.generateName) {
        // This PersistentVolume can be expected to be a newly created PersistentVolume. So, use `createPersistentVolume` instead.
        await pvAPI.createPersistentVolume(n);
      } else {
        throw new Error(
          "failed to apply persistentvolume: persistentvolume should have metadata.name or metadata.generateName"
        );
      }
    },

    async delete(pv: V1PersistentVolume) {
      if (pv.metadata?.name) {
        await pvAPI.deletePersistentVolume(pv.metadata.name);
      } else {
        throw new Error(
          "failed to delete persistentvolume: persistentvolume should have metadata.name"
        );
      }
    },

    // initList calls list API, and stores current resource data and lastResourceVersion.
    async initList() {
      const listpvs = await pvAPI.listPersistentVolume();
      state.pvs = createResourceState<V1PersistentVolume>(listpvs.items);
      state.lastResourceVersion = listpvs.metadata?.resourceVersion!;
    },

    // watchEventHandler handles each notified event.
    async watchEventHandler(eventType: WatchEventType, pv: V1PersistentVolume) {
      switch (eventType) {
        case WatchEventType.ADDED: {
          state.pvs = addResourceToState(state.pvs, pv);
          break;
        }
        case WatchEventType.MODIFIED: {
          state.pvs = modifyResourceInState(state.pvs, pv);
          break;
        }
        case WatchEventType.DELETED: {
          state.pvs = deleteResourceInState(state.pvs, pv);
          break;
        }
        default:
          break;
      }
    },

    get lastResourceVersion() {
      return state.lastResourceVersion;
    },

    async setLastResourceVersion(pv: V1PersistentVolume) {
      state.lastResourceVersion =
        pv.metadata!.resourceVersion || state.lastResourceVersion;
    },
  };
}

export type PersistentVolumeStore = ReturnType<typeof pvStore>;
</file>

<file path="kube-scheduler-simulator/web/store/pvc.ts">
import { reactive, inject } from "@nuxtjs/composition-api";
import { V1PersistentVolumeClaim } from "@kubernetes/client-node";
import { PVCAPIKey } from "~/api/APIProviderKeys";
import {
  createResourceState,
  addResourceToState,
  modifyResourceInState,
  deleteResourceInState,
} from "./helpers/storeHelper";
import { WatchEventType } from "@/types/resources";

type stateType = {
  selectedPersistentVolumeClaim: selectedPersistentVolumeClaim | null;
  pvcs: V1PersistentVolumeClaim[];
  lastResourceVersion: string;
};

type selectedPersistentVolumeClaim = {
  // isNew represents whether this is a new PersistentVolumeClaim or not.
  isNew: boolean;
  item: V1PersistentVolumeClaim;
  resourceKind: string;
  isDeletable: boolean;
};

export default function pvcStore() {
  const state: stateType = reactive({
    selectedPersistentVolumeClaim: null,
    pvcs: [],
    lastResourceVersion: "",
  });

  const pvcAPI = inject(PVCAPIKey);
  if (!pvcAPI) {
    throw new Error(`${PVCAPIKey.description} is not provided`);
  }

  return {
    get pvcs() {
      return state.pvcs;
    },

    get count(): number {
      return state.pvcs.length;
    },

    get selected() {
      return state.selectedPersistentVolumeClaim;
    },

    select(pvc: V1PersistentVolumeClaim | null, isNew: boolean) {
      if (pvc !== null) {
        state.selectedPersistentVolumeClaim = {
          isNew: isNew,
          item: pvc,
          resourceKind: "PVC",
          isDeletable: true,
        };
      }
    },

    resetSelected() {
      state.selectedPersistentVolumeClaim = null;
    },

    async apply(pvc: V1PersistentVolumeClaim) {
      if (pvc.metadata?.name) {
        await pvcAPI.applyPersistentVolumeClaim(pvc);
      } else if (pvc.metadata?.generateName) {
        // This PersistentVolumeClaim can be expected to be a newly created PersistentVolumeClaim. So, use `createPersistentVolumeClaim` instead.
        await pvcAPI.createPersistentVolumeClaim(pvc);
      } else {
        throw new Error(
          "failed to apply persistentvolumeclaim: persistentvolumeclaim should have metadata.name or metadata.generateName"
        );
      }
    },

    async fetchSelected() {
      if (
        state.selectedPersistentVolumeClaim?.item.metadata?.namespace &&
        state.selectedPersistentVolumeClaim?.item.metadata?.name &&
        !this.selected?.isNew
      ) {
        const p = await pvcAPI.getPersistentVolumeClaim(
          state.selectedPersistentVolumeClaim.item.metadata.namespace,
          state.selectedPersistentVolumeClaim.item.metadata.name,
        );
        this.select(p, false);
      }
    },

    async delete(pvc: V1PersistentVolumeClaim) {
      if (pvc.metadata?.name && pvc.metadata?.namespace) {
        await pvcAPI.deletePersistentVolumeClaim(pvc.metadata.namespace, pvc.metadata.name);
      } else {
        throw new Error(
          "failed to delete persistentvolumeclaim: persistentvolumeclaim should have metadata.name"
        );
      }
    },

    // initList calls list API, and stores current resource data and lastResourceVersion.
    async initList() {
      const listpvcs = await pvcAPI.listPersistentVolumeClaim();
      state.pvcs = createResourceState<V1PersistentVolumeClaim>(listpvcs.items);
      state.lastResourceVersion = listpvcs.metadata?.resourceVersion!;
    },

    // watchEventHandler handles each notified event.
    async watchEventHandler(
      eventType: WatchEventType,
      pvc: V1PersistentVolumeClaim
    ) {
      switch (eventType) {
        case WatchEventType.ADDED: {
          state.pvcs = addResourceToState(state.pvcs, pvc);
          break;
        }
        case WatchEventType.MODIFIED: {
          state.pvcs = modifyResourceInState(state.pvcs, pvc);
          break;
        }
        case WatchEventType.DELETED: {
          state.pvcs = deleteResourceInState(state.pvcs, pvc);
          break;
        }
        default:
          break;
      }
    },

    get lastResourceVersion() {
      return state.lastResourceVersion;
    },

    async setLastResourceVersion(pvc: V1PersistentVolumeClaim) {
      state.lastResourceVersion =
        pvc.metadata!.resourceVersion || state.lastResourceVersion;
    },
  };
}

export type PersistentVolumeClaimStore = ReturnType<typeof pvcStore>;
</file>

<file path="kube-scheduler-simulator/web/store/schedulerconfiguration.ts">
import { reactive, inject } from "@nuxtjs/composition-api";
import { SchedulerConfiguration } from "~/api/v1/types";
import { SchedulerconfigurationAPIKey } from "~/api/APIProviderKeys";

type stateType = {
  // when users use an external scheduler, we disable it.
  disabled: boolean;
  selectedConfig: selectedConfig | null;
};

type selectedConfig = {
  // isNew represents whether this Config is a new one or not.
  isNew: boolean;
  item: SchedulerConfiguration;
  resourceKind: string;
  isDeletable: boolean;
};

export default function schedulerconfigurationStore() {
  const state: stateType = reactive({
    selectedConfig: null,
    disabled: false,
    schedulerconfigurations: [],
  });

  const schedconfAPI = inject(SchedulerconfigurationAPIKey);
  if (!schedconfAPI) {
    throw new Error(`${SchedulerconfigurationAPIKey.description} is not provided`);
  }

  return {
    get disabled() {
      return state.disabled;
    },

    get selected() {
      return state.selectedConfig;
    },

    resetSelected() {
      state.selectedConfig = null;
    },

    select() {
      this.fetchSelected();
    },

    async initialize() {
      await schedconfAPI.getSchedulerConfiguration().catch((e) => {
        if (e.response.status == 400) {
          // users use an external scheduler on backend.
          state.disabled = true;
          return;
        }
        throw new Error(`failed to apply scheduler configration: ${e}`);
      });
    },

    async fetchSelected() {
      const c = await schedconfAPI.getSchedulerConfiguration().catch((e) => {
        if (e.response.status == 400) {
          // users use an external scheduler on backend.
          state.disabled = true;
          return;
        }
        throw new Error(`failed to apply scheduler configration: ${e}`);
      });
      if (c) {
        state.selectedConfig = {
          isNew: true,
          item: c,
          resourceKind: "SchedulerConfiguration",
          isDeletable: true,
        };
      }
    },

    async apply(cfg: SchedulerConfiguration) {
      await schedconfAPI.applySchedulerConfiguration(cfg);
    },

    async delete(_: SchedulerConfiguration) {
      // This function do nothing, but exist to satisfy interface on ResourceBar.vue.
    },
  };
}

export type SchedulerConfigurationStore = ReturnType<
  typeof schedulerconfigurationStore
>;
</file>

<file path="kube-scheduler-simulator/web/store/snackbar.ts">
import { reactive } from "@nuxtjs/composition-api";

export interface stateType {
  isOpen: boolean;
  message: string;
  messageType: MessageType;
}

export interface SnackbarPayload {
  message: string;
  messageType: MessageType;
}

export type MessageType = "info" | "error";

export default function snackbarStore() {
  const state: stateType = reactive({
    message: "",
    messageType: "error",
    isOpen: false,
  } as stateType);

  return {
    get message() {
      return state.message;
    },
    get isOpen() {
      return state.isOpen;
    },
    get messageType() {
      return state.messageType;
    },
    open() {
      state.isOpen = true;
    },
    close() {
      state.isOpen = false;
    },
    setIsOpen(isOpen: boolean) {
      if (isOpen) {
        this.open();
      } else {
        this.close();
      }
    },
    setServerErrorMessage(error: string) {
      const servererrormsg: string = "Server error occurred: ";

      state.message = servererrormsg + error;
      this.setMessageType("error");
      this.open();
    },
    setServerInfoMessage(message: string) {
      state.message = message;
      this.setMessageType("info");
      this.open();
    },
    setMessageType(messageType: MessageType) {
      state.messageType = messageType;
    },
  };
}

export type SnackBarStore = ReturnType<typeof snackbarStore>;
</file>

<file path="kube-scheduler-simulator/web/store/storageclass.ts">
import { reactive, inject } from "@nuxtjs/composition-api";
import { V1StorageClass } from "@kubernetes/client-node";
import { StorageClassAPIKey } from "~/api/APIProviderKeys";
import {
  createResourceState,
  addResourceToState,
  modifyResourceInState,
  deleteResourceInState,
} from "./helpers/storeHelper";
import { WatchEventType } from "@/types/resources";

type stateType = {
  selectedStorageClass: selectedStorageClass | null;
  storageclasses: V1StorageClass[];
  lastResourceVersion: string;
};

type selectedStorageClass = {
  // isNew represents whether this is a new StorageClass or not.
  isNew: boolean;
  item: V1StorageClass;
  resourceKind: string;
  isDeletable: boolean;
};

export default function storageclassStore() {
  const state: stateType = reactive({
    selectedStorageClass: null,
    storageclasses: [],
    lastResourceVersion: "",
  });

  const storageClassAPI = inject(StorageClassAPIKey);
  if (!storageClassAPI) {
    throw new Error(`${StorageClassAPIKey.description} is not provided`);
  }

  return {
    get storageclasses() {
      return state.storageclasses;
    },

    get count(): number {
      return state.storageclasses.length;
    },

    get selected() {
      return state.selectedStorageClass;
    },

    select(sc: V1StorageClass | null, isNew: boolean) {
      if (sc !== null) {
        state.selectedStorageClass = {
          isNew: isNew,
          item: sc,
          resourceKind: "SC",
          isDeletable: true,
        };
      }
    },

    resetSelected() {
      state.selectedStorageClass = null;
    },

    async apply(sc: V1StorageClass) {
      if (sc.metadata?.name) {
        await storageClassAPI.applyStorageClass(sc);
      } else if (sc.metadata?.generateName) {
        // This StorageClass can be expected to be a newly created StorageClass. So, use `createStorageClass` instead.
        await storageClassAPI.createStorageClass(sc);
      } else {
        throw new Error(
          "failed to apply storageclass: storageclass should have metadata.name or metadata.generateName"
        );
      }
    },

    async fetchSelected() {
      if (
        state.selectedStorageClass?.item.metadata?.name &&
        !this.selected?.isNew
      ) {
        const s = await storageClassAPI.getStorageClass(
          state.selectedStorageClass.item.metadata.name
        );
        this.select(s, false);
      }
    },

    async delete(sc: V1StorageClass) {
      if (sc.metadata?.name) {
        await storageClassAPI.deleteStorageClass(sc.metadata.name);
      } else {
        throw new Error(
          "failed to delete storageclass: storageclass should have metadata.name"
        );
      }
    },

    // initList calls list API, and stores current resource data and lastResourceVersion.
    async initList() {
      const liststorageclasses = await storageClassAPI.listStorageClass();
      state.storageclasses = createResourceState<V1StorageClass>(
        liststorageclasses.items
      );
      state.lastResourceVersion = liststorageclasses.metadata?.resourceVersion!;
    },

    // watchEventHandler handles each notified event.
    async watchEventHandler(eventType: WatchEventType, sc: V1StorageClass) {
      switch (eventType) {
        case WatchEventType.ADDED: {
          state.storageclasses = addResourceToState(state.storageclasses, sc);
          break;
        }
        case WatchEventType.MODIFIED: {
          state.storageclasses = modifyResourceInState(
            state.storageclasses,
            sc
          );
          break;
        }
        case WatchEventType.DELETED: {
          state.storageclasses = deleteResourceInState(
            state.storageclasses,
            sc
          );
          break;
        }
        default:
          break;
      }
    },

    get lastResourceVersion() {
      return state.lastResourceVersion;
    },

    async setLastResourceVersion(sc: V1StorageClass) {
      state.lastResourceVersion =
        sc.metadata!.resourceVersion || state.lastResourceVersion;
    },
  };
}

export type StorageClassStore = ReturnType<typeof storageclassStore>;
</file>

<file path="kube-scheduler-simulator/web/types/api/v1.ts">
// LastResourceVersions is used to pass each lastResourceVersion to the server.
export type LastResourceVersions = {
  pods: string;
  nodes: string;
  pvs: string;
  pvcs: string;
  storageClasses: string;
  priorityClasses: string;
  namespaces: string;
};
</file>

<file path="kube-scheduler-simulator/web/types/resources.ts">
// WatchEventType represents the event type of the resource being watched.
export enum WatchEventType {
  ADDED = "ADDED",
  DELETED = "DELETED",
  MODIFIED = "MODIFIED",
}
</file>

<file path="kube-scheduler-simulator/web/.dockerignore">
node_modules
npm-debug.log
yarn-error.log
dist
</file>

<file path="kube-scheduler-simulator/web/.eslintrc.js">
module.exports = {
  extends: [
    "eslint:recommended",
    "plugin:vue/vue3-recommended",
    "@vue/typescript",
    "plugin:prettier/recommended",
    "prettier",
  ],
  env: {
    node: true,
  },
  rules: {
    "no-unused-vars": "off",
    "@typescript-eslint/no-unused-vars": ["error", { argsIgnorePattern: "^_" }],
  },
};
</file>

<file path="kube-scheduler-simulator/web/.gitignore">
# Created by .ignore support plugin (hsz.mobi)
### Node template
# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*

# Runtime data
pids
*.pid
*.seed
*.pid.lock

# Directory for instrumented libs generated by jscoverage/JSCover
lib-cov

# Coverage directory used by tools like istanbul
coverage

# nyc test coverage
.nyc_output

# Grunt intermediate storage (http://gruntjs.com/creating-plugins#storing-task-files)
.grunt

# Bower dependency directory (https://bower.io/)
bower_components

# node-waf configuration
.lock-wscript

# Compiled binary addons (https://nodejs.org/api/addons.html)
build/Release

# Dependency directories
node_modules/
jspm_packages/

# TypeScript v1 declaration files
typings/

# Optional npm cache directory
.npm

# Optional eslint cache
.eslintcache

# Optional REPL history
.node_repl_history

# Output of 'npm pack'
*.tgz

# Yarn Integrity file
.yarn-integrity

# dotenv environment variables file
.env

# parcel-bundler cache (https://parceljs.org/)
.cache

# next.js build output
.next

# nuxt.js build output
.nuxt

# Nuxt generate
dist

# vuepress build output
.vuepress/dist

# Serverless directories
.serverless

# IDE
.idea

# Service worker
sw.*

# macOS
.DS_Store
</file>

<file path="kube-scheduler-simulator/web/.prettierignore">
.nuxt/
node_modules/
coverage/
</file>

<file path="kube-scheduler-simulator/web/.stylelintignore">
coverage/
</file>

<file path="kube-scheduler-simulator/web/.stylelintrc.json">
{
  "extends": "stylelint-config-standard"
}
</file>

<file path="kube-scheduler-simulator/web/Dockerfile">
FROM node:16-alpine AS deps

RUN apk update && \
    apk upgrade && \
    apk add --no-cache make gcc g++ py-pip

WORKDIR /app
COPY package.json yarn.lock ./
RUN yarn install --frozen-lockfile

FROM node:16-alpine AS builder
WORKDIR /app
COPY . .
COPY --from=deps /app/node_modules ./node_modules
RUN yarn build && yarn install --production --ignore-scripts --prefer-offline

FROM node:16-alpine AS runner
WORKDIR /app

ENV NODE_ENV=production
ENV NUXT_TELEMETRY_DISABLED=1
# Currnently, HOST_ENV is used to decide whether to import the TEMPLATE files of the k8s resources.
# ./components/lib/templates/*.yaml
ENV HOST_ENV=production
ENV BASE_URL=http://localhost:1212
ENV KUBE_API_SERVER_URL=http://localhost:3131

RUN addgroup -g 1001 -S nodejs
RUN adduser -S nuxtjs -u 1001

COPY --from=builder /app/nuxt.config.js ./
#COPY --from=builder ./app/server ./server/
COPY --from=builder ./app/package.json ./
COPY --from=builder ./app/node_modules ./node_modules/
COPY --from=builder ./app/.nuxt ./.nuxt/
COPY --from=builder ./app/static ./static/

USER nuxtjs

EXPOSE 3000

CMD ["yarn", "start"]
</file>

<file path="kube-scheduler-simulator/web/jest.config.js">
module.exports = {
  moduleNameMapper: {
    "^@/(.*)$": "<rootDir>/$1",
    "^~/(.*)$": "<rootDir>/$1",
    "^vue$": "vue/dist/vue.common.js",
  },
  moduleFileExtensions: ["ts", "js", "vue", "json"],
  transform: {
    "^.+\\.ts$": "ts-jest",
    "^.+\\.js$": "babel-jest",
    ".*\\.(vue)$": "vue-jest",
  },
  collectCoverage: true,
  collectCoverageFrom: [
    "<rootDir>/components/**/*.vue",
    "<rootDir>/pages/**/*.vue",
  ],
};
</file>

<file path="kube-scheduler-simulator/web/nuxt.config.js">
import fs from "fs";

const getTemplateEnv = (hostEnv) => {
  return hostEnv !== "production"
    ? {
        // Build Configuration: https://go.nuxtjs.dev/config-build
        POD_TEMPLATE: fs.readFileSync(
          "./components/lib/templates/pod.yaml",
          "utf8"
        ),
        NODE_TEMPLATE: fs.readFileSync(
          "./components/lib/templates/node.yaml",
          "utf8"
        ),
        PV_TEMPLATE: fs.readFileSync(
          "./components/lib/templates/pv.yaml",
          "utf8"
        ),
        PVC_TEMPLATE: fs.readFileSync(
          "./components/lib/templates/pvc.yaml",
          "utf8"
        ),
        SC_TEMPLATE: fs.readFileSync(
          "./components/lib/templates/storageclass.yaml",
          "utf8"
        ),
        PC_TEMPLATE: fs.readFileSync(
          "./components/lib/templates/priorityclass.yaml",
          "utf8"
        ),
        NAMESPACE_TEMPLATE: fs.readFileSync(
          "./components/lib/templates/namespace.yaml",
          "utf8"
        ),
      }
    : true;
};

export default {
  // Global page headers: https://go.nuxtjs.dev/config-head
  head: {
    titleTemplate: "%s - scheduler-simulator",
    title: "scheduler-simulator",
    htmlAttrs: {
      lang: "en",
    },
    meta: [
      { charset: "utf-8" },
      { name: "viewport", content: "width=device-width, initial-scale=1" },
      { hid: "description", name: "description", content: "" },
    ],
    link: [{ rel: "icon", type: "image/x-icon", href: "/favicon.ico" }],
  },

  // Global CSS: https://go.nuxtjs.dev/config-css
  css: [],

  // Plugins to run before rendering page: https://go.nuxtjs.dev/config-plugins
  plugins: [
    // ApiRuntimeConfigPlugin is a custom plugin that is used to create and inject axios instance for API using the value of publicRuntimeConfig.
    { src: "~/plugins/ApiRuntimeConfigPlugin.ts" },
  ],

  // Auto import components: https://go.nuxtjs.dev/config-components
  components: true,

  // Modules for dev and build (recommended): https://go.nuxtjs.dev/config-modules
  buildModules: [
    // https://go.nuxtjs.dev/typescript
    "@nuxt/typescript-build",
    // https://go.nuxtjs.dev/vuetify
    "@nuxtjs/vuetify",
    // for nuxtjs/composition-api
    "@nuxtjs/composition-api/module",
  ],

  // Modules: https://go.nuxtjs.dev/config-modules
  modules: [
    // https://go.nuxtjs.dev/axios
    "@nuxtjs/axios",
  ],

  // Axios module configuration: https://go.nuxtjs.dev/config-axios
  axios: {},

  // Vuetify module configuration: https://go.nuxtjs.dev/config-vuetify
  vuetify: {
    customVariables: ["~/assets/variables.scss"],
    theme: {
      themes: {
        light: {
          primary: "#326ce5",
          background: "#f5f5f5",
        },
      },
    },
  },
  // publicRuntimeConfig should hold all env variables that are public as these will be exposed on the frontend.
  publicRuntimeConfig: {
    baseURL: process.env.BASE_URL || "http://localhost:1212",
    kubeApiServerURL:
      process.env.KUBE_API_SERVER_URL || "http://localhost:3131",
    // alphaTableViews is a optional parameter for the datatable view. This is an alpha feature.
    // If this value is set to "1", the datatable view will be enabled.
    alphaTableViews: process.env.ALPHA_TABLE_VIEWS || "0",
  },

  env: getTemplateEnv(process.env.HOST_ENV),
};
</file>

<file path="kube-scheduler-simulator/web/package.json">
{
  "name": "scheduler-simulator",
  "version": "1.0.0",
  "private": true,
  "scripts": {
    "dev": "nuxt",
    "build": "nuxt build",
    "start": "nuxt start",
    "generate": "nuxt generate",
    "eslint": "eslint --ext \\\".js,.vue,.ts\\\" ",
    "eslint:fix": "eslint --ext \\\".js,.vue,.ts\\\" --fix ",
    "stylelint": "stylelint \"**/**.{css,scss,sass}\"",
    "lint": "yarn eslint && yarn stylelint",
    "prettier:fix": "prettier --write .",
    "format": "yarn eslint:fix && yarn prettier:fix",
    "test": "jest"
  },
  "dependencies": {
    "@kubernetes/client-node": "^0.14.3",
    "@nuxtjs/axios": "^5.13.1",
    "@nuxtjs/composition-api": "^0.24.4",
    "@types/node": "^16.4.0",
    "core-js": "^3.9.1",
    "file-saver": "^2.0.5",
    "js-yaml": "^4.1.0",
    "nuxt": "^2.15.3",
    "vue-monaco": "^1.2.2"
  },
  "devDependencies": {
    "@nuxt/types": "^2.15.3",
    "@nuxt/typescript-build": "^2.1.0",
    "@nuxtjs/eslint-config-typescript": "^6.0.0",
    "@nuxtjs/eslint-module": "^3.0.2",
    "@nuxtjs/vuetify": "^1.11.3",
    "@types/file-saver": "^2.0.5",
    "@typescript-eslint/eslint-plugin": "^4.29.3",
    "@typescript-eslint/parser": "^4.29.3",
    "@vue/eslint-config-typescript": "^7.0.0",
    "@vue/test-utils": "^1.1.3",
    "babel-core": "7.0.0-bridge.0",
    "babel-eslint": "^10.1.0",
    "babel-jest": "^26.6.3",
    "eslint": "^7.32.0",
    "eslint-config-prettier": "^8.3.0",
    "eslint-plugin-nuxt": "^2.0.0",
    "eslint-plugin-prettier": "^3.4.1",
    "eslint-plugin-vue": "^7.0.0-beta.4",
    "jest": "^26.6.3",
    "prettier": "^2.3.2",
    "stylelint": "^13.13.1",
    "stylelint-config-standard": "^22.0.0",
    "ts-jest": "^26.5.4",
    "vue-jest": "^3.0.4"
  }
}
</file>

<file path="kube-scheduler-simulator/web/README.md">
# Scheduler-Simulator Web

This is the frontend of Kubernetes scheduler simulator.

## Run frontend

You have to install node.js and yarn.

- for yarn, see: [Installation | Yarn](https://classic.yarnpkg.com/en/docs/install/#mac-stable)
- for node.js, see: [Downloads | Node.js](https://nodejs.org/en/download/)
  Note: Nodejs 16 is suggested, other version may cause problems.

### Build Setup

```bash
# install dependencies
$ yarn install

# build for production and launch server
$ yarn build
$ yarn start
```

For detailed explanation on how things work, check out [Nuxt.js docs](https://nuxtjs.org).

### Environment Variables
Please see [environment-variables.md](./docs/environment-variables.md)

## For developer

```bash
# serve with hot reload at localhost:3000
$ yarn dev
# format the code
$ yarn format
# lint the code
$ yarn lint
```
</file>

<file path="kube-scheduler-simulator/web/tsconfig.eslint.json">
{
  "extends": "./tsconfig.json",
  "include": ["src/**/*.ts", ".eslintrc.js"],
  "exclude": ["node_modules", "dist"]
}
</file>

<file path="kube-scheduler-simulator/web/tsconfig.json">
{
  "compilerOptions": {
    "target": "ES2018",
    "module": "ESNext",
    "moduleResolution": "Node",
    "lib": ["ESNext", "ESNext.AsyncIterable", "DOM"],
    "esModuleInterop": true,
    "resolveJsonModule": true,
    "allowJs": true,
    "sourceMap": true,
    "strict": true,
    "noEmit": true,
    "experimentalDecorators": true,
    "baseUrl": "./",
    "paths": {
      "~/*": ["./*"],
      "@/*": ["./*"]
    },
    "types": ["@nuxt/types", "@nuxtjs/axios", "@types/node"]
  },
  "exclude": ["node_modules", ".nuxt", "dist"]
}
</file>

<file path="kube-scheduler-simulator/.gitignore">
bin/
</file>

<file path="kube-scheduler-simulator/cloudbuild.yaml">
# See https://cloud.google.com/cloud-build/docs/build-config

# this must be specified in seconds. If omitted, defaults to 600s (10 mins)
timeout: 3000s
steps:
  - name: gcr.io/cloud-builders/docker
    args:
    - buildx
    - build
    - --tag=gcr.io/$PROJECT_ID/simulator-backend:$_GIT_TAG
    - --tag=gcr.io/$PROJECT_ID/simulator-backend:latest
    - -f=./simulator/cmd/simulator/Dockerfile
    - ./simulator/
  - name: gcr.io/cloud-builders/docker
    args:
    - buildx
    - build
    - --tag=gcr.io/$PROJECT_ID/debuggable-scheduler:$_GIT_TAG
    - --tag=gcr.io/$PROJECT_ID/debuggable-scheduler:latest
    - -f=./simulator/cmd/scheduler/Dockerfile
    - ./simulator/
  - name: gcr.io/cloud-builders/docker
    args:
    - buildx
    - build
    - --tag=gcr.io/$PROJECT_ID/simulator-frontend:$_GIT_TAG
    - --tag=gcr.io/$PROJECT_ID/simulator-frontend:latest
    - -f=./web/Dockerfile
    - ./web/
substitutions:
  _GIT_TAG: '12345'
  _PULL_BASE_REF: 'master'
# this prevents errors if you don't use both _GIT_TAG and _PULL_BASE_REF,
# or any new substitutions added in the future.
options:
  substitution_option: ALLOW_LOOSE
# this will push these images, or cause the build to fail if they weren't built.
images:
  - 'gcr.io/$PROJECT_ID/simulator-frontend:$_GIT_TAG'
  - 'gcr.io/$PROJECT_ID/simulator-frontend:latest'
  - 'gcr.io/$PROJECT_ID/simulator-backend:$_GIT_TAG'
  - 'gcr.io/$PROJECT_ID/simulator-backend:latest'
  - 'gcr.io/$PROJECT_ID/debuggable-scheduler:$_GIT_TAG'
  - 'gcr.io/$PROJECT_ID/debuggable-scheduler:latest'
</file>

<file path="kube-scheduler-simulator/code-of-conduct.md">
# Kubernetes Community Code of Conduct

Please refer to our [Kubernetes Community Code of Conduct](https://git.k8s.io/community/code-of-conduct.md)
</file>

<file path="kube-scheduler-simulator/compose.yml">
services:
  # This container copies the data,
  # so any changes made to the configuration files within the Pod will not affect the original files.
  init-container:
      image: busybox
      volumes:
        - conf:/config
        - ${PWD}/simulator/cmd/scheduler:/host-config:ro
      command: sh -c "cp -rf /host-config/* /config/"
  simulator-scheduler:
    image: registry.k8s.io/scheduler-simulator/debuggable-scheduler:v0.4.0
    container_name: simulator-scheduler
    environment:
      - KUBECONFIG=/config/kubeconfig.yaml
    volumes:
      - conf:/config
    depends_on:
      - init-container
      - simulator-cluster
    restart: always
    tty: true
    networks:
      - simulator-internal-network
  simulator-server:
    image: registry.k8s.io/scheduler-simulator/simulator-backend:v0.4.0
    container_name: simulator-server
    volumes:
      - ./simulator/config.yaml:/config.yaml
      - ./simulator/kubeconfig.yaml:/kubeconfig.yaml
      - /var/run/docker.sock:/var/run/docker.sock
      - conf:/config
    environment:
      - PORT=1212
      - KUBE_SCHEDULER_SIMULATOR_ETCD_URL=http://simulator-cluster:2379
      - KUBE_APISERVER_URL=http://simulator-cluster:3131
    ports:
      - "1212:1212"
    restart: always
    tty: true
    networks:
      - simulator-internal-network
  simulator-frontend:
    image: registry.k8s.io/scheduler-simulator/simulator-frontend:v0.4.0
    restart: always
    container_name: simulator-frontend
    environment:
      - HOST=0.0.0.0
      - BASE_URL=http://${SIMULATOR_EXTERNAL_IP:-localhost}:1212
      - KUBE_API_SERVER_URL=http://${SIMULATOR_EXTERNAL_IP:-localhost}:3131
    ports:
    - "3000:3000"
    tty: true
  simulator-cluster:
    image: registry.k8s.io/kwok/cluster:v0.6.0-k8s.v1.30.2
    container_name: simulator-cluster
    restart: always
    ports:
      - "3131:3131"
    volumes:
      - simulator-etcd-data:/var/lib/etcd
      - ./kwok.yaml:/root/.kwok/kwok.yaml
    environment:
      - KWOK_KUBE_APISERVER_PORT=3131
    networks:
      - simulator-internal-network
networks:
  simulator-internal-network:
    driver: bridge
volumes:
  simulator-etcd-data:
  conf:
</file>

<file path="kube-scheduler-simulator/CONTRIBUTING.md">
# Contributing Guidelines

Welcome to Kubernetes. We are excited about the prospect of you joining our [community](https://git.k8s.io/community)! The Kubernetes community abides by the CNCF [code of conduct](code-of-conduct.md). Here is an excerpt:

_As contributors and maintainers of this project, and in the interest of fostering an open and welcoming community, we pledge to respect all people who contribute through reporting issues, posting feature requests, updating documentation, submitting pull requests or patches, and other activities._

- [Contributor License Agreement](https://git.k8s.io/community/CLA.md) - Kubernetes projects require that you sign a Contributor License Agreement (CLA) before we can accept your pull requests
- [Kubernetes Contributor Guide](https://git.k8s.io/community/contributors/guide) - Main contributor documentation, or you can just jump directly to the [contributing section](https://git.k8s.io/community/contributors/guide#contributing)
- [Contributor Cheat Sheet](https://git.k8s.io/community/contributors/guide/contributor-cheatsheet) - Common resources for existing developers

## Getting Started

For the frontend, please see [README.md](web/README.md) on ./web dir.

**prerequisite**: Go 1.17 or more

You have to prepare tools with make.

```bash
make tools
```

Also, you can run lint, format and test with make.

```bash
# test
make test
# lint
make lint
# format
make format
```

See [Makefile](Makefile) for more details.

## Internal KEP

We may discuss new features in internal KEP.
See [keps/README.md](keps/README.md) for more details.
</file>

<file path="kube-scheduler-simulator/kwok.yaml">
kind: KwokctlConfiguration
apiVersion: config.kwok.x-k8s.io/v1alpha1
options:
  etcdPort: 2379
  etcdPrefix: /kube-scheduler-simulator
  disableKubeScheduler: true
  enableMetricsServer: true
componentsPatches:
- name: kube-apiserver
  extraArgs:
  - key: cors-allowed-origins
    value: ^*$
</file>

<file path="kube-scheduler-simulator/LICENSE">
Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "{}"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright {yyyy} {name of copyright owner}

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
</file>

<file path="kube-scheduler-simulator/Makefile">
BUILD      ?= build

USE_BUILDX ?=
HOSTARCH := $(shell uname -m)
ifeq ($(HOSTARCH),x86_64)
HOSTARCH := amd64
endif
BUILDX_PLATFORM ?= linux/$(HOSTARCH)

# Setup buildx flags
ifneq ("$(USE_BUILDX)","")
BUILD = buildx build --platform=$(BUILDX_PLATFORM) -o type=docker
endif

.PHONY: tools
tools:
	curl -sSfL https://raw.githubusercontent.com/golangci/golangci-lint/master/install.sh | sh -s -- -b $(shell go env GOPATH)/bin v1.64.5
	cd ./tools; \
	cat tools.go | grep "_" | awk -F'"' '{print $$2}' | xargs -tI % go install %

.PHONY: lint
# run golangci-lint on all modules
lint:
	cd simulator/ && make lint

.PHONY: format
# format codes on all modules
format:
	cd simulator/ && make format

.PHONY: test
# run test on all modules
test:
	cd simulator/ && make test

.PHONY: mod-download
mod-download:
	cd simulator/ && go mod download -x

.PHONY: build
# build all modules
build:
	cd simulator/ && make build

.PHONY: docker_build
docker_build: docker_build_server docker_build_scheduler docker_build_front

.PHONY: docker_build_server
docker_build_server:
	docker $(BUILD) -f simulator/cmd/simulator/Dockerfile -t simulator-server simulator

.PHONY: docker_build_scheduler
docker_build_scheduler:
	docker $(BUILD) -f simulator/cmd/scheduler/Dockerfile -t simulator-scheduler simulator

.PHONY: docker_build_front
docker_build_front:
	docker $(BUILD) -t simulator-frontend ./web/

.PHONY: docker_up
docker_up:
	docker compose up -d

.PHONY: docker_up_local
docker_up_local:
	docker compose -f compose.yml -f compose.local.yml up -d

.PHONY: docker_build_and_up
docker_build_and_up: docker_build docker_up_local

.PHONY: docker_down
docker_down:
	docker compose down --volumes

.PHONY: docker_down_local
docker_down_local:
	docker compose -f compose.yml -f compose.local.yml down --volumes
</file>

<file path="kube-scheduler-simulator/OWNERS">
# See the OWNERS docs at https://go.k8s.io/owners

approvers:
- sanposhiho
- 196Ikuchil

# emeritus:
# - adtac

reviewers:
- sanposhiho
- 196Ikuchil

# emeritus:
# - adtac

labels:
- sig/scheduling
</file>

<file path="kube-scheduler-simulator/README.md">
# Kubernetes scheduler simulator

Hello world. Here is Kubernetes scheduler simulator.

Nowadays, the scheduler is configurable/extendable in the multiple ways:
- configure with [KubeSchedulerConfiguration](https://kubernetes.io/docs/reference/scheduling/config/)
- add Plugins of [Scheduling Framework](https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/)
- add [Extenders](https://github.com/kubernetes/enhancements/tree/5320deb4834c05ad9fb491dcd361f952727ece3e/keps/sig-scheduling/1819-scheduler-extender)
- etc...

But, unfortunately, not all configurations/expansions yield good results.
Those who customize the scheduler need to make sure their scheduler is working as expected,
and doesn't have an unacceptably negative impact on the scheduling.

In real Kubernetes, we cannot know the results of scheduling in detail without reading the logs,
which usually requires privileged access to the control plane.

That's why we are developing a simulator for kube-scheduler
-- you can try out the behavior of the scheduler while checking which plugin made what decision for which Node.

## Getting started 

```shell
git clone git@github.com:kubernetes-sigs/kube-scheduler-simulator.git
cd kube-scheduler-simulator
# Pull the latest stable images for simulator from the registry, 
# and run up all components.
make docker_up 
# All things up! 
```

You can access the simulator in http://localhost:3000.

Please see [this doc](./simulator/docs/running-simulator.md) to know other ways to run up the simulator.

## Simulator's architecture

We have several components:
- Simulator (in `/simulator`): the core implementation of the simulator
- Web UI (in `/web`): the Web client of the simulator
- Coming soon... :)  (see [./keps](./keps) to see some nice ideas we're working on)

### Simulator

Simulator is composed of debuggable scheduler + the HTTP server which mainly for the web UI.

There are several ways to integrate your scheduler into the simulator.
See [integrate-your-scheduler.md](simulator/docs/integrate-your-scheduler.md).

Simulator runs with a fake cluster powered by [KWOK](https://github.com/kubernetes-sigs/kwok)
You can create any resources in KWOK cluster via any clients (e.g. kubectl, k8s client library, or web UI described next).
And when you create Pods, 
Pods will be scheduled by the [debuggable scheduler](./simulator/docs/debuggable-scheduler.md),
and they'll get the annotations that explain how each Pod was evaluated by each scheduler plugin.

```yaml
kind: Pod
apiVersion: v1
metadata:
  name: hoge-pod
  annotations:
    kube-scheduler-simulator.sigs.k8s.io/bind-result: '{"DefaultBinder":"success"}'
    kube-scheduler-simulator.sigs.k8s.io/filter-result: >-
      {"node-282x7":{"AzureDiskLimits":"passed","EBSLimits":"passed","GCEPDLimits":"passed","InterPodAffinity":"passed","NodeAffinity":"passed","NodeName":"passed","NodePorts":"passed","NodeResourcesFit":"passed","NodeUnschedulable":"passed","NodeVolumeLimits":"passed","PodTopologySpread":"passed","TaintToleration":"passed","VolumeBinding":"passed","VolumeRestrictions":"passed","VolumeZone":"passed"},"node-gp9t4":{"AzureDiskLimits":"passed","EBSLimits":"passed","GCEPDLimits":"passed","InterPodAffinity":"passed","NodeAffinity":"passed","NodeName":"passed","NodePorts":"passed","NodeResourcesFit":"passed","NodeUnschedulable":"passed","NodeVolumeLimits":"passed","PodTopologySpread":"passed","TaintToleration":"passed","VolumeBinding":"passed","VolumeRestrictions":"passed","VolumeZone":"passed"}}
    kube-scheduler-simulator.sigs.k8s.io/finalscore-result: >-
      {"node-282x7":{"ImageLocality":"0","InterPodAffinity":"0","NodeAffinity":"0","NodeNumber":"0","NodeResourcesBalancedAllocation":"76","NodeResourcesFit":"73","PodTopologySpread":"200","TaintToleration":"300","VolumeBinding":"0"},"node-gp9t4":{"ImageLocality":"0","InterPodAffinity":"0","NodeAffinity":"0","NodeNumber":"0","NodeResourcesBalancedAllocation":"76","NodeResourcesFit":"73","PodTopologySpread":"200","TaintToleration":"300","VolumeBinding":"0"}}
    kube-scheduler-simulator.sigs.k8s.io/permit-result: '{}'
    kube-scheduler-simulator.sigs.k8s.io/permit-result-timeout: '{}'
    kube-scheduler-simulator.sigs.k8s.io/postfilter-result: '{}'
    kube-scheduler-simulator.sigs.k8s.io/prebind-result: '{"VolumeBinding":"success"}'
    kube-scheduler-simulator.sigs.k8s.io/prefilter-result: '{}'
    kube-scheduler-simulator.sigs.k8s.io/prefilter-result-status: >-
      {"InterPodAffinity":"success","NodeAffinity":"success","NodePorts":"success","NodeResourcesFit":"success","PodTopologySpread":"success","VolumeBinding":"success","VolumeRestrictions":"success"}
    kube-scheduler-simulator.sigs.k8s.io/prescore-result: >-
      {"InterPodAffinity":"success","NodeAffinity":"success","NodeNumber":"success","PodTopologySpread":"success","TaintToleration":"success"}
    kube-scheduler-simulator.sigs.k8s.io/reserve-result: '{"VolumeBinding":"success"}'
    kube-scheduler-simulator.sigs.k8s.io/result-history: >-
      [{"noderesourcefit-prefilter-data":"{\"MilliCPU\":100,\"Memory\":17179869184,\"EphemeralStorage\":0,\"AllowedPodNumber\":0,\"ScalarResources\":null}","kube-scheduler-simulator.sigs.k8s.io/bind-result":"{\"DefaultBinder\":\"success\"}","kube-scheduler-simulator.sigs.k8s.io/filter-result":"{\"node-282x7\":{\"AzureDiskLimits\":\"passed\",\"EBSLimits\":\"passed\",\"GCEPDLimits\":\"passed\",\"InterPodAffinity\":\"passed\",\"NodeAffinity\":\"passed\",\"NodeName\":\"passed\",\"NodePorts\":\"passed\",\"NodeResourcesFit\":\"passed\",\"NodeUnschedulable\":\"passed\",\"NodeVolumeLimits\":\"passed\",\"PodTopologySpread\":\"passed\",\"TaintToleration\":\"passed\",\"VolumeBinding\":\"passed\",\"VolumeRestrictions\":\"passed\",\"VolumeZone\":\"passed\"},\"node-gp9t4\":{\"AzureDiskLimits\":\"passed\",\"EBSLimits\":\"passed\",\"GCEPDLimits\":\"passed\",\"InterPodAffinity\":\"passed\",\"NodeAffinity\":\"passed\",\"NodeName\":\"passed\",\"NodePorts\":\"passed\",\"NodeResourcesFit\":\"passed\",\"NodeUnschedulable\":\"passed\",\"NodeVolumeLimits\":\"passed\",\"PodTopologySpread\":\"passed\",\"TaintToleration\":\"passed\",\"VolumeBinding\":\"passed\",\"VolumeRestrictions\":\"passed\",\"VolumeZone\":\"passed\"}}","kube-scheduler-simulator.sigs.k8s.io/finalscore-result":"{\"node-282x7\":{\"ImageLocality\":\"0\",\"InterPodAffinity\":\"0\",\"NodeAffinity\":\"0\",\"NodeNumber\":\"0\",\"NodeResourcesBalancedAllocation\":\"76\",\"NodeResourcesFit\":\"73\",\"PodTopologySpread\":\"200\",\"TaintToleration\":\"300\",\"VolumeBinding\":\"0\"},\"node-gp9t4\":{\"ImageLocality\":\"0\",\"InterPodAffinity\":\"0\",\"NodeAffinity\":\"0\",\"NodeNumber\":\"0\",\"NodeResourcesBalancedAllocation\":\"76\",\"NodeResourcesFit\":\"73\",\"PodTopologySpread\":\"200\",\"TaintToleration\":\"300\",\"VolumeBinding\":\"0\"}}","kube-scheduler-simulator.sigs.k8s.io/permit-result":"{}","kube-scheduler-simulator.sigs.k8s.io/permit-result-timeout":"{}","kube-scheduler-simulator.sigs.k8s.io/postfilter-result":"{}","kube-scheduler-simulator.sigs.k8s.io/prebind-result":"{\"VolumeBinding\":\"success\"}","kube-scheduler-simulator.sigs.k8s.io/prefilter-result":"{}","kube-scheduler-simulator.sigs.k8s.io/prefilter-result-status":"{\"InterPodAffinity\":\"success\",\"NodeAffinity\":\"success\",\"NodePorts\":\"success\",\"NodeResourcesFit\":\"success\",\"PodTopologySpread\":\"success\",\"VolumeBinding\":\"success\",\"VolumeRestrictions\":\"success\"}","kube-scheduler-simulator.sigs.k8s.io/prescore-result":"{\"InterPodAffinity\":\"success\",\"NodeAffinity\":\"success\",\"NodeNumber\":\"success\",\"PodTopologySpread\":\"success\",\"TaintToleration\":\"success\"}","kube-scheduler-simulator.sigs.k8s.io/reserve-result":"{\"VolumeBinding\":\"success\"}","kube-scheduler-simulator.sigs.k8s.io/score-result":"{\"node-282x7\":{\"ImageLocality\":\"0\",\"InterPodAffinity\":\"0\",\"NodeAffinity\":\"0\",\"NodeNumber\":\"0\",\"NodeResourcesBalancedAllocation\":\"76\",\"NodeResourcesFit\":\"73\",\"PodTopologySpread\":\"0\",\"TaintToleration\":\"0\",\"VolumeBinding\":\"0\"},\"node-gp9t4\":{\"ImageLocality\":\"0\",\"InterPodAffinity\":\"0\",\"NodeAffinity\":\"0\",\"NodeNumber\":\"0\",\"NodeResourcesBalancedAllocation\":\"76\",\"NodeResourcesFit\":\"73\",\"PodTopologySpread\":\"0\",\"TaintToleration\":\"0\",\"VolumeBinding\":\"0\"}}","kube-scheduler-simulator.sigs.k8s.io/selected-node":"node-282x7"}]
    kube-scheduler-simulator.sigs.k8s.io/score-result: >-
      {"node-282x7":{"ImageLocality":"0","InterPodAffinity":"0","NodeAffinity":"0","NodeNumber":"0","NodeResourcesBalancedAllocation":"76","NodeResourcesFit":"73","PodTopologySpread":"0","TaintToleration":"0","VolumeBinding":"0"},"node-gp9t4":{"ImageLocality":"0","InterPodAffinity":"0","NodeAffinity":"0","NodeNumber":"0","NodeResourcesBalancedAllocation":"76","NodeResourcesFit":"73","PodTopologySpread":"0","TaintToleration":"0","VolumeBinding":"0"}}
    kube-scheduler-simulator.sigs.k8s.io/selected-node: node-282x7
```

You can utilize these results to understand your scheduler, check/test your configurations or customized scheduler's behavior.

Further expansion, you can export internal state more, or change specific behaviours on plugins 
by implementing [PluginExtender](./simulator/docs/plugin-extender.md).

The simulator has its own configuration,
you can refer to the [documentation](./simulator/docs/simulator-server-config.md).

See the following docs to know more about simulator:
- [import-cluster-resources.md](./simulator/docs/import-cluster-resources.md): describes how you can import resources in your cluster to the simulator so that you can simulate scheduling based on your cluster's situation.
- [how-it-works.md](simulator/docs/how-it-works.md): describes about how the simulator works.
- [kube-apiserver.md](simulator/docs/kube-apiserver.md): describe about kube-apiserver in simulator. (how you can configure and access)
- [api.md](simulator/docs/api.md): describes about HTTP server the simulator has. (mainly for the webUI)

### Web UI

Web UI is the easiest way to check the scheduler's behavior.

Nice table view for the scheduling result, the scheduler configuration reload feature; 
you can access every core features of the simulator with human-friendly ways!

![list resources](simulator/docs/images/resources.png)

It has a cool yaml editor to create/edit resources.

![create node](simulator/docs/images/create-node.png)

And, after pods are scheduled, you can see the results of

- Each Filter plugins
- Each Score plugins
- Final score (normalized and applied Plugin Weight)

![result](simulator/docs/images/result.jpg)

Also, You can change the configuration of the scheduler through [KubeSchedulerConfiguration](https://kubernetes.io/docs/reference/scheduling/config/) in Web UI.

(Note: changes to any fields other than `.profiles` are disabled on simulator, 
since they do not affect the results of the scheduling.)

![configure scheduler](simulator/docs/images/schedulerconfiguration.png)

Also, you can take a snapshot of the resources so that you can share the situation or load them later.

![snapshot button](simulator/docs/images/webUI-snapshot.png)

## Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md)

## Community, discussion, contribution, and support

Learn how to engage with the Kubernetes community on the [community page](http://kubernetes.io/community/).

You can reach the maintainers of this project at:

- [Slack](http://slack.k8s.io/)
- [Mailing List](https://groups.google.com/forum/#!forum/kubernetes-dev)

### Code of conduct

Participation in the Kubernetes community is governed by the [Kubernetes Code of Conduct](code-of-conduct.md).

[owners]: https://git.k8s.io/community/contributors/guide/owners.md
[creative commons 4.0]: https://git.k8s.io/website/LICENSE
</file>

<file path="kube-scheduler-simulator/RELEASE.md">
# Release Guide

1. An issue is proposing a new release with a changelog since the last release.
2. Make sure your repo is clean by git's standards.
3. Tag the repository from the `master` branch (from the `release-1.19` branch for a patch release) and push the tag `VERSION=v0.19.0 git tag -m $VERSION $VERSION; git push origin $VERSION`.
4. An [OWNER](OWNERS) creates a release branch `git checkout -b release-1.19`. (not required for patch releases)
5. Add the prow-job settings for the new release branch [here](https://github.com/kubernetes/test-infra/tree/master/config/jobs/kubernetes-sigs/kube-scheduler-simulator).
6. Push the release branch to the kube-scheduler-simulator repo and ensure branch protection is enabled. (not required for patch releases)
7. Publish a draft release using the tag you created in 3.
8. Perform the [image promotion process](https://github.com/kubernetes/k8s.io/blob/main/registry.k8s.io/images/k8s-staging-sched-simulator/images.yaml).
9. Publish release.
10. Make sure the new version's image can be pulled and we can run the simulator correctly.
11. Create a PR to update [compose.yml](./compose.yml) specifies the new release.
12. Email `kubernetes-sig-scheduling@googlegroups.com` to announce the release.

## Notes
See [post-kube-scheduler-simulator-push-images dashboard](https://testgrid.k8s.io/sig-scheduling#post-kube-scheduler-simulator-push-images) for staging registry image build job status.

View the kube-scheduler-simulator staging registry using [this URL](https://console.cloud.google.com/gcr/images/k8s-staging-sched-simulator/GLOBAL) in a web browser
or use the below `gcloud` commands.

List images in staging registry.
```shell
gcloud container images list --repository gcr.io/k8s-staging-sched-simulator
```

List simulator-backend and simulator-frontend image tags in the staging registry.
```shell
gcloud container images list-tags gcr.io/k8s-staging-sched-simulator/simulator-backend
gcloud container images list-tags gcr.io/k8s-staging-sched-simulator/simulator-frontend
```
</file>

<file path="kube-scheduler-simulator/SECURITY_CONTACTS">
# Defined below are the security contacts for this repo.
#
# They are the contact point for the Product Security Committee to reach out
# to for triaging and handling of incoming issues.
#
# The below names agree to abide by the
# [Embargo Policy](https://git.k8s.io/security/private-distributors-list.md#embargo-policy)
# and will be removed and replaced if they violate that agreement.
#
# DO NOT REPORT SECURITY VULNERABILITIES DIRECTLY TO THESE NAMES, FOLLOW THE
# INSTRUCTIONS AT https://kubernetes.io/security/

adtac
sanposhiho
</file>

<file path="kube-scheduler-simulator/SECURITY.md">
# Security Policy

## Security Announcements

Join the [kubernetes-security-announce] group for security and vulnerability announcements.

You can also subscribe to an RSS feed of the above using [this link][kubernetes-security-announce-rss].

## Reporting a Vulnerability

Instructions for reporting a vulnerability can be found on the
[Kubernetes Security and Disclosure Information] page.

## Supported Versions

Information about supported Kubernetes versions can be found on the
[Kubernetes version and version skew support policy] page on the Kubernetes website.

[kubernetes-security-announce]: https://groups.google.com/forum/#!forum/kubernetes-security-announce
[kubernetes-security-announce-rss]: https://groups.google.com/forum/feed/kubernetes-security-announce/msgs/rss_v2_0.xml?num=50
[Kubernetes version and version skew support policy]: https://kubernetes.io/docs/setup/release/version-skew-policy/#supported-versions
[Kubernetes Security and Disclosure Information]: https://kubernetes.io/docs/reference/issues-security/security/#report-a-vulnerability
</file>

<file path=".gitignore">
# Go specific ignores
# Binaries for programs and plugins
*.exe
*.exe~
*.dll
*.so
*.dylib

# Test binary, built with `go test -c`
*.test

# Output of the go coverage tool
*.out

# Dependency directories
vendor/
go.sum
go.mod

# Go workspace file
go.work

# Python specific ignores
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions (from Python)
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
.hypothesis/
.pytest_cache/

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/
pythonenv*

# Jupyter Notebook
.ipynb_checkpoints

# IDE specific files
# VS Code
.vscode/*
!.vscode/settings.json
!.vscode/tasks.json
!.vscode/launch.json
!.vscode/extensions.json
*.code-workspace

# JetBrains IDEs
.idea/
*.iml
*.iws
*.ipr
.idea_modules/
out/

# Sublime Text
*.tmlanguage.cache
*.tmPreferences.cache
*.stTheme.cache
*.sublime-workspace
*.sublime-project

# General
.DS_Store
.AppleDouble
.LSOverride
Thumbs.db
ehthumbs.db
Desktop.ini

# Project specific
# Logs
logs/
*.log

# Local configs
config.local.yaml
*.local.yaml
*.local.yml
*_local.yaml
*_local.yml
*_local.json

# Dependency subdirectories (if using a custom approach)
ABC/.git/
</file>

<file path="pod-tmp.yml">
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fake-pod
  namespace: default
  kwok.x-k8s.io/usage-cpu: 748m
  kwok.x-k8s.io/usage-memory: 7332Mi
spec:
  replicas: 10
  selector:
    matchLabels:
      app: fake-pod
  template:
    metadata:
      labels:
        app: fake-pod
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: type
                operator: In
                values:
                - kwok
      # A taints was added to an automatically created Node.
      # You can remove taints of Node or add this tolerations.
      tolerations:
      - key: "kwok.x-k8s.io/node"
        operator: "Exists"
        effect: "NoSchedule"
      containers:
      - name: fake-container
        image: fake-image
</file>

</files>
