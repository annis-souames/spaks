
# SPAKS: Smart Power-Aware Kubernetes Scheduler

**SPAKS** (Smart Power-Aware Kubernetes Scheduler) is a research project that introduces a novel scheduling technique for Kubernetes to optimize energy consumption across the cluster. This is achieved by leveraging a Deep Reinforcement Learning (DRL) model to make intelligent scheduling decisions that reduce energy consumption across the k8s cluster. The environment is simulated using [kube-scheduler-simulator](https://github.com/kubernetes-sigs/kube-scheduler-simulator) for scheduler behavior  and [Kwok](https://kwok.sigs.k8s.io/) for simulating several nodes and pods efficiently.

This project is developed by Mohamed Annis Souames as part of a submitted research paper in collaboration with Dr. Satish Kumar, Pr. Ah Lian Kor, Dr. Thalita Vergilio and Dr. Nawar Jawad from Leeds Beckett University (Leeds, UK).

## Goal

The primary goal of SPAKS is to reduce the overall energy footprint of a Kubernetes cluster without compromising performance. It employs a deep reinforcement learning model to achieve such reduction.

## Project Structure

The repository is organized into several key directories, each responsible for a specific component of the simulation.

```
/
├── drl/                # DRL model, training notebooks, and inference server
├── energy/             # Energy consumption models
├── infra/              # Infrastructure setup for the simulated environment (Kwok)
├── kube-scheduler-simulator/ # Fork of the official K8s scheduler simulator
├── logs/               # Data logs from scheduler runs
├── results/            # Analysis of simulation results
└── readme.md           # Readme - guide
```

### `kube-scheduler-simulator/` - The Simulation Engine

This is a fork of the official **[Kubernetes Scheduler Simulator](https://github.com/kubernetes-sigs/kube-scheduler-simulator)**. It provides a complete, isolated environment for running and debugging the Kubernetes scheduler.

The primary change in this fork is the integration of the custom DRL scheduler as a plugin. This allows the simulator to run the DRL agent's logic in place of the default scoring mechanism. The relevant code for this integration can be found in the `simulator/drlScheduler/` directory.

**`Makefile` & `compose.yml`**: These files are used to build and run the simulator environment using Docker.

### `logs/` & `results/` 

These directories are for data collection and evaluation.

- **`logs/`**: Captures the raw state and action data from scheduling simulations (e.g., `states.csv`, `actions.csv`). This data is necessary for offline training of the DRL model.
- **`results/`**: Contains the final outputs and analysis. `default.txt` and `drl.txt` store aggregate metrics from runs with the default and DRL schedulers, respectively.

## How to Run the Project

Running the simulation involves setting up the containerized environment provided by the `kube-scheduler-simulator`.

1.  **Prerequisites**:
    - [Docker](https://docs.docker.com/get-docker/) and [Docker Compose](https://docs.docker.com/compose/install/)
    - `git`
    - `make`

2.  **Clone the Repository**:
    ```bash
    git clone <repository-url>
    cd spaks
    ```

3.  **Start the Simulator**:
    The core of the project is the scheduler simulator. Navigate to its directory and use the provided Makefile to bring up the environment.
    ```bash
    cd kube-scheduler-simulator
    make docker_up
    ```
    This command will pull the necessary Docker images, build the custom scheduler, and start all components, including the simulator backend and the web UI.

4.  **Access the Web UI**:
    Once the services are running, you can access the simulator's web interface by navigating to **[http://localhost:3000](http://localhost:3000)** in your browser.

5.  **Running a Simulation**:
    From the web UI, you can:
    - Apply node and workload configurations (generated by the scripts in `infra/`).
    - Select which scheduler to use (the default Kubernetes scheduler or the custom DRL scheduler).
    - Run the simulation and observe how pods are placed on the simulated nodes.
    - View the detailed scoring results for each pod, which provides insight into the scheduler's decision-making process.

By following these steps, you can replicate the experiments, test new models, or extend the functionality of the SPAKS scheduler.

## Licensing
This project is licensed under the Apache 2.0 license. 

## Citation
If you use or refer to this project in your work, please cite the original paper and the authors.
